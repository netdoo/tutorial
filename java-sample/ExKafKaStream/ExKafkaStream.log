2017-06-12 17:02:32 INFO  StreamsConfig:180 - StreamsConfig values: 
	application.id = Word Count Sample
	application.server = 
	bootstrap.servers = [localhost:9092]
	buffered.records.per.partition = 1000
	cache.max.bytes.buffering = 10485760
	client.id = 
	commit.interval.ms = 30000
	connections.max.idle.ms = 540000
	key.serde = class org.apache.kafka.common.serialization.Serdes$StringSerde
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	num.standby.replicas = 0
	num.stream.threads = 1
	partition.grouper = class org.apache.kafka.streams.processor.DefaultPartitionGrouper
	poll.ms = 100
	receive.buffer.bytes = 32768
	reconnect.backoff.ms = 50
	replication.factor = 1
	request.timeout.ms = 40000
	retry.backoff.ms = 100
	rocksdb.config.setter = null
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	state.cleanup.delay.ms = 60000
	state.dir = /tmp/kafka-streams
	timestamp.extractor = class org.apache.kafka.streams.processor.FailOnInvalidTimestamp
	value.serde = class org.apache.kafka.common.serialization.Serdes$StringSerde
	windowstore.changelog.additional.retention.ms = 86400000
	zookeeper.connect = 

2017-06-12 17:02:33 DEBUG Metrics:335 - Added sensor with name thread.Word Count Sample-a215918e-7abe-41f7-9de1-96afbab2d581-StreamThread-1.commit-latency
2017-06-12 17:02:33 DEBUG Metrics:335 - Added sensor with name thread.Word Count Sample-a215918e-7abe-41f7-9de1-96afbab2d581-StreamThread-1.poll-latency
2017-06-12 17:02:33 DEBUG Metrics:335 - Added sensor with name thread.Word Count Sample-a215918e-7abe-41f7-9de1-96afbab2d581-StreamThread-1.process-latency
2017-06-12 17:02:33 DEBUG Metrics:335 - Added sensor with name thread.Word Count Sample-a215918e-7abe-41f7-9de1-96afbab2d581-StreamThread-1.punctuate-latency
2017-06-12 17:02:33 DEBUG Metrics:335 - Added sensor with name thread.Word Count Sample-a215918e-7abe-41f7-9de1-96afbab2d581-StreamThread-1.task-created
2017-06-12 17:02:33 DEBUG Metrics:335 - Added sensor with name thread.Word Count Sample-a215918e-7abe-41f7-9de1-96afbab2d581-StreamThread-1.task-closed
2017-06-12 17:02:33 DEBUG Metrics:335 - Added sensor with name thread.Word Count Sample-a215918e-7abe-41f7-9de1-96afbab2d581-StreamThread-1.skipped-records
2017-06-12 17:02:33 INFO  StreamThread:304 - stream-thread [StreamThread-1] Creating producer client
2017-06-12 17:02:33 INFO  ProducerConfig:180 - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	block.on.buffer.full = false
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = Word Count Sample-a215918e-7abe-41f7-9de1-96afbab2d581-StreamThread-1-producer
	compression.type = none
	connections.max.idle.ms = 540000
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 100
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.fetch.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	timeout.ms = 30000
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2017-06-12 17:02:33 DEBUG Metrics:335 - Added sensor with name bufferpool-wait-time
2017-06-12 17:02:33 DEBUG Metrics:335 - Added sensor with name buffer-exhausted-records
2017-06-12 17:02:33 DEBUG Metadata:249 - Updated cluster metadata version 1 to Cluster(id = null, nodes = [localhost:9092 (id: -1 rack: null)], partitions = [])
2017-06-12 17:02:33 DEBUG Metrics:335 - Added sensor with name connections-closed:
2017-06-12 17:02:33 DEBUG Metrics:335 - Added sensor with name connections-created:
2017-06-12 17:02:33 DEBUG Metrics:335 - Added sensor with name bytes-sent-received:
2017-06-12 17:02:33 DEBUG Metrics:335 - Added sensor with name bytes-sent:
2017-06-12 17:02:33 DEBUG Metrics:335 - Added sensor with name bytes-received:
2017-06-12 17:02:33 DEBUG Metrics:335 - Added sensor with name select-time:
2017-06-12 17:02:33 DEBUG Metrics:335 - Added sensor with name io-time:
2017-06-12 17:02:34 DEBUG Metrics:335 - Added sensor with name batch-size
2017-06-12 17:02:34 DEBUG Metrics:335 - Added sensor with name compression-rate
2017-06-12 17:02:34 DEBUG Metrics:335 - Added sensor with name queue-time
2017-06-12 17:02:34 DEBUG Metrics:335 - Added sensor with name request-time
2017-06-12 17:02:34 DEBUG Metrics:335 - Added sensor with name produce-throttle-time
2017-06-12 17:02:34 DEBUG Metrics:335 - Added sensor with name records-per-request
2017-06-12 17:02:34 DEBUG Metrics:335 - Added sensor with name record-retries
2017-06-12 17:02:34 DEBUG Metrics:335 - Added sensor with name errors
2017-06-12 17:02:34 DEBUG Metrics:335 - Added sensor with name record-size-max
2017-06-12 17:02:34 DEBUG Sender:121 - Starting Kafka producer I/O thread.
2017-06-12 17:02:34 INFO  AppInfoParser:83 - Kafka version : 0.10.2.1
2017-06-12 17:02:34 INFO  AppInfoParser:84 - Kafka commitId : e89bffd6b2eff799
2017-06-12 17:02:34 DEBUG KafkaProducer:336 - Kafka producer started
2017-06-12 17:02:34 INFO  StreamThread:306 - stream-thread [StreamThread-1] Creating consumer client
2017-06-12 17:02:34 INFO  ConsumerConfig:180 - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = Word Count Sample-a215918e-7abe-41f7-9de1-96afbab2d581-StreamThread-1-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = Word Count Sample
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 2147483647
	max.poll.records = 1000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamPartitionAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2017-06-12 17:02:34 DEBUG KafkaConsumer:607 - Starting the Kafka consumer
2017-06-12 17:02:34 DEBUG Metadata:249 - Updated cluster metadata version 1 to Cluster(id = null, nodes = [localhost:9092 (id: -1 rack: null)], partitions = [])
2017-06-12 17:02:34 DEBUG Metrics:335 - Added sensor with name connections-closed:
2017-06-12 17:02:34 DEBUG Metrics:335 - Added sensor with name connections-created:
2017-06-12 17:02:34 DEBUG Metrics:335 - Added sensor with name bytes-sent-received:
2017-06-12 17:02:34 DEBUG Metrics:335 - Added sensor with name bytes-sent:
2017-06-12 17:02:34 DEBUG Metrics:335 - Added sensor with name bytes-received:
2017-06-12 17:02:34 DEBUG Metrics:335 - Added sensor with name select-time:
2017-06-12 17:02:34 DEBUG Metrics:335 - Added sensor with name io-time:
2017-06-12 17:02:34 DEBUG Metadata:249 - Updated cluster metadata version 1 to Cluster(id = null, nodes = [localhost:9092 (id: -1 rack: null)], partitions = [])
2017-06-12 17:02:34 DEBUG Metrics:335 - Added sensor with name connections-closed:
2017-06-12 17:02:34 DEBUG Metrics:335 - Added sensor with name connections-created:
2017-06-12 17:02:34 DEBUG Metrics:335 - Added sensor with name bytes-sent-received:
2017-06-12 17:02:34 DEBUG Metrics:335 - Added sensor with name bytes-sent:
2017-06-12 17:02:34 DEBUG Metrics:335 - Added sensor with name bytes-received:
2017-06-12 17:02:34 DEBUG Metrics:335 - Added sensor with name select-time:
2017-06-12 17:02:34 DEBUG Metrics:335 - Added sensor with name io-time:
2017-06-12 17:02:34 DEBUG Metrics:335 - Added sensor with name heartbeat-latency
2017-06-12 17:02:34 DEBUG Metrics:335 - Added sensor with name join-latency
2017-06-12 17:02:34 DEBUG Metrics:335 - Added sensor with name sync-latency
2017-06-12 17:02:34 DEBUG Metrics:335 - Added sensor with name commit-latency
2017-06-12 17:02:34 DEBUG Metrics:335 - Added sensor with name bytes-fetched
2017-06-12 17:02:34 DEBUG Metrics:335 - Added sensor with name records-fetched
2017-06-12 17:02:34 DEBUG Metrics:335 - Added sensor with name fetch-latency
2017-06-12 17:02:34 DEBUG Metrics:335 - Added sensor with name records-lag
2017-06-12 17:02:34 DEBUG Metrics:335 - Added sensor with name fetch-throttle-time
2017-06-12 17:02:34 INFO  AppInfoParser:83 - Kafka version : 0.10.2.1
2017-06-12 17:02:34 INFO  AppInfoParser:84 - Kafka commitId : e89bffd6b2eff799
2017-06-12 17:02:34 DEBUG KafkaConsumer:711 - Kafka consumer created
2017-06-12 17:02:34 INFO  StreamThread:317 - stream-thread [StreamThread-1] Creating restore consumer client
2017-06-12 17:02:34 INFO  ConsumerConfig:180 - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = Word Count Sample-a215918e-7abe-41f7-9de1-96afbab2d581-StreamThread-1-restore-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 2147483647
	max.poll.records = 1000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2017-06-12 17:02:34 DEBUG KafkaConsumer:607 - Starting the Kafka consumer
2017-06-12 17:02:34 DEBUG Metadata:249 - Updated cluster metadata version 1 to Cluster(id = null, nodes = [localhost:9092 (id: -1 rack: null)], partitions = [])
2017-06-12 17:02:34 DEBUG Metrics:335 - Added sensor with name connections-closed:
2017-06-12 17:02:34 DEBUG Metrics:335 - Added sensor with name connections-created:
2017-06-12 17:02:34 DEBUG Metrics:335 - Added sensor with name bytes-sent-received:
2017-06-12 17:02:34 DEBUG Metrics:335 - Added sensor with name bytes-sent:
2017-06-12 17:02:34 DEBUG Metrics:335 - Added sensor with name bytes-received:
2017-06-12 17:02:34 DEBUG Metrics:335 - Added sensor with name select-time:
2017-06-12 17:02:34 DEBUG Metrics:335 - Added sensor with name io-time:
2017-06-12 17:02:34 DEBUG Metrics:335 - Added sensor with name heartbeat-latency
2017-06-12 17:02:34 DEBUG Metrics:335 - Added sensor with name join-latency
2017-06-12 17:02:34 DEBUG Metrics:335 - Added sensor with name sync-latency
2017-06-12 17:02:34 DEBUG Metrics:335 - Added sensor with name commit-latency
2017-06-12 17:02:34 DEBUG Metrics:335 - Added sensor with name bytes-fetched
2017-06-12 17:02:34 DEBUG Metrics:335 - Added sensor with name records-fetched
2017-06-12 17:02:34 DEBUG Metrics:335 - Added sensor with name fetch-latency
2017-06-12 17:02:34 DEBUG Metrics:335 - Added sensor with name records-lag
2017-06-12 17:02:34 DEBUG Metrics:335 - Added sensor with name fetch-throttle-time
2017-06-12 17:02:34 INFO  AppInfoParser:83 - Kafka version : 0.10.2.1
2017-06-12 17:02:34 INFO  AppInfoParser:84 - Kafka commitId : e89bffd6b2eff799
2017-06-12 17:02:34 DEBUG KafkaConsumer:711 - Kafka consumer created
2017-06-12 17:02:34 INFO  StreamThread:163 - stream-thread [StreamThread-1] State transition from NOT_RUNNING to RUNNING.
2017-06-12 17:02:34 DEBUG KafkaStreams:422 - stream-client [Word Count Sample-a215918e-7abe-41f7-9de1-96afbab2d581] Starting Kafka Stream process.
2017-06-12 17:02:34 DEBUG Metadata:249 - Updated cluster metadata version 1 to Cluster(id = null, nodes = [localhost:9092 (id: -1 rack: null)], partitions = [])
2017-06-12 17:02:34 DEBUG Metrics:335 - Added sensor with name connections-closed:
2017-06-12 17:02:34 DEBUG Metrics:335 - Added sensor with name connections-created:
2017-06-12 17:02:34 DEBUG Metrics:335 - Added sensor with name bytes-sent-received:
2017-06-12 17:02:34 DEBUG Metrics:335 - Added sensor with name bytes-sent:
2017-06-12 17:02:34 DEBUG Metrics:335 - Added sensor with name bytes-received:
2017-06-12 17:02:34 DEBUG Metrics:335 - Added sensor with name select-time:
2017-06-12 17:02:34 DEBUG Metrics:335 - Added sensor with name io-time:
2017-06-12 17:02:34 DEBUG Metadata:249 - Updated cluster metadata version 1 to Cluster(id = null, nodes = [localhost:9092 (id: -1 rack: null)], partitions = [])
2017-06-12 17:02:34 DEBUG NetworkClient:627 - Initiating connection to node -1 at localhost:9092.
2017-06-12 17:02:34 DEBUG Metrics:335 - Added sensor with name node--1.bytes-sent
2017-06-12 17:02:34 DEBUG Metrics:335 - Added sensor with name node--1.bytes-received
2017-06-12 17:02:34 DEBUG Metrics:335 - Added sensor with name node--1.latency
2017-06-12 17:02:34 DEBUG Selector:339 - Created socket with SO_RCVBUF = 32768, SO_SNDBUF = 131072, SO_TIMEOUT = 0 to node -1
2017-06-12 17:02:34 DEBUG NetworkClient:590 - Completed connection to node -1.  Fetching API versions.
2017-06-12 17:02:34 DEBUG NetworkClient:603 - Initiating API versions fetch from node -1.
2017-06-12 17:02:34 DEBUG NetworkClient:558 - Recorded API versions for node -1: (Produce(0): 0 to 2 [usable: 2], Fetch(1): 0 to 3 [usable: 3], Offsets(2): 0 to 1 [usable: 1], Metadata(3): 0 to 2 [usable: 2], LeaderAndIsr(4): 0 [usable: 0], StopReplica(5): 0 [usable: 0], UpdateMetadata(6): 0 to 3 [usable: 3], ControlledShutdown(7): 1 [usable: 1], OffsetCommit(8): 0 to 2 [usable: 2], OffsetFetch(9): 0 to 2 [usable: 2], GroupCoordinator(10): 0 [usable: 0], JoinGroup(11): 0 to 1 [usable: 1], Heartbeat(12): 0 [usable: 0], LeaveGroup(13): 0 [usable: 0], SyncGroup(14): 0 [usable: 0], DescribeGroups(15): 0 [usable: 0], ListGroups(16): 0 [usable: 0], SaslHandshake(17): 0 [usable: 0], ApiVersions(18): 0 [usable: 0], CreateTopics(19): 0 to 1 [usable: 1], DeleteTopics(20): 0 [usable: 0])
2017-06-12 17:02:34 INFO  KafkaStreams:224 - stream-client [Word Count Sample-a215918e-7abe-41f7-9de1-96afbab2d581] State transition from CREATED to RUNNING.
2017-06-12 17:02:34 INFO  KafkaStreams:436 - stream-client [Word Count Sample-a215918e-7abe-41f7-9de1-96afbab2d581] Started Kafka Stream process
2017-06-12 17:02:34 INFO  StreamThread:358 - stream-thread [StreamThread-1] Starting
2017-06-12 17:02:34 DEBUG KafkaConsumer:882 - Subscribed to pattern: TextLinesTopic|Word Count Sample-Counts-repartition
2017-06-12 17:02:34 DEBUG AbstractCoordinator:561 - Sending GroupCoordinator request for group Word Count Sample to broker localhost:9092 (id: -1 rack: null)
2017-06-12 17:02:34 DEBUG NetworkClient:627 - Initiating connection to node -1 at localhost:9092.
2017-06-12 17:02:34 DEBUG Metrics:335 - Added sensor with name node--1.bytes-sent
2017-06-12 17:02:34 DEBUG Metrics:335 - Added sensor with name node--1.bytes-received
2017-06-12 17:02:34 DEBUG Metrics:335 - Added sensor with name node--1.latency
2017-06-12 17:02:34 DEBUG Selector:339 - Created socket with SO_RCVBUF = 65536, SO_SNDBUF = 131072, SO_TIMEOUT = 0 to node -1
2017-06-12 17:02:34 DEBUG NetworkClient:590 - Completed connection to node -1.  Fetching API versions.
2017-06-12 17:02:34 DEBUG NetworkClient:603 - Initiating API versions fetch from node -1.
2017-06-12 17:02:34 DEBUG NetworkClient:558 - Recorded API versions for node -1: (Produce(0): 0 to 2 [usable: 2], Fetch(1): 0 to 3 [usable: 3], Offsets(2): 0 to 1 [usable: 1], Metadata(3): 0 to 2 [usable: 2], LeaderAndIsr(4): 0 [usable: 0], StopReplica(5): 0 [usable: 0], UpdateMetadata(6): 0 to 3 [usable: 3], ControlledShutdown(7): 1 [usable: 1], OffsetCommit(8): 0 to 2 [usable: 2], OffsetFetch(9): 0 to 2 [usable: 2], GroupCoordinator(10): 0 [usable: 0], JoinGroup(11): 0 to 1 [usable: 1], Heartbeat(12): 0 [usable: 0], LeaveGroup(13): 0 [usable: 0], SyncGroup(14): 0 [usable: 0], DescribeGroups(15): 0 [usable: 0], ListGroups(16): 0 [usable: 0], SaslHandshake(17): 0 [usable: 0], ApiVersions(18): 0 [usable: 0], CreateTopics(19): 0 to 1 [usable: 1], DeleteTopics(20): 0 [usable: 0])
2017-06-12 17:02:34 DEBUG NetworkClient:751 - Sending metadata request (type=MetadataRequest, topics=<ALL>) to node -1
2017-06-12 17:02:34 DEBUG Metadata:249 - Updated cluster metadata version 2 to Cluster(id = kbd_yf8GQXCf13cNvyCN1w, nodes = [tmon-jhkwon78-n.tmoncorp.com:9092 (id: 0 rack: null)], partitions = [Partition(topic = TextLinesTopic, partition = 0, leader = 0, replicas = [0], isr = [0])])
2017-06-12 17:02:34 DEBUG AbstractCoordinator:572 - Received GroupCoordinator response ClientResponse(receivedTimeMs=1497254554451, latencyMs=30, disconnected=false, requestHeader={api_key=10,api_version=0,correlation_id=0,client_id=Word Count Sample-a215918e-7abe-41f7-9de1-96afbab2d581-StreamThread-1-consumer}, responseBody={error_code=0,coordinator={node_id=0,host=tmon-jhkwon78-n.tmoncorp.com,port=9092}}) for group Word Count Sample
2017-06-12 17:02:34 INFO  AbstractCoordinator:586 - Discovered coordinator tmon-jhkwon78-n.tmoncorp.com:9092 (id: 2147483647 rack: null) for group Word Count Sample.
2017-06-12 17:02:34 DEBUG NetworkClient:627 - Initiating connection to node 2147483647 at tmon-jhkwon78-n.tmoncorp.com:9092.
2017-06-12 17:02:34 INFO  ConsumerCoordinator:397 - Revoking previously assigned partitions [] for group Word Count Sample
2017-06-12 17:02:34 INFO  StreamThread:248 - stream-thread [StreamThread-1] at state RUNNING: partitions [] revoked at the beginning of consumer rebalance.
2017-06-12 17:02:34 INFO  StreamThread:163 - stream-thread [StreamThread-1] State transition from RUNNING to PARTITIONS_REVOKED.
2017-06-12 17:02:34 INFO  KafkaStreams:224 - stream-client [Word Count Sample-a215918e-7abe-41f7-9de1-96afbab2d581] State transition from RUNNING to REBALANCING.
2017-06-12 17:02:34 DEBUG AbstractCoordinator:889 - Heartbeat thread for group Word Count Sample started
2017-06-12 17:02:34 DEBUG StreamThread:468 - stream-thread [StreamThread-1] suspendTasksAndState: suspending all active tasks [] and standby tasks []
2017-06-12 17:02:34 DEBUG KafkaConsumer:899 - Unsubscribed all topics or patterns and assigned partitions
2017-06-12 17:02:34 INFO  StreamThread:1042 - stream-thread [StreamThread-1] Updating suspended tasks to contain active tasks []
2017-06-12 17:02:34 INFO  StreamThread:1049 - stream-thread [StreamThread-1] Removing all active tasks []
2017-06-12 17:02:34 INFO  StreamThread:1064 - stream-thread [StreamThread-1] Removing all standby tasks []
2017-06-12 17:02:34 INFO  AbstractCoordinator:420 - (Re-)joining group Word Count Sample
2017-06-12 17:02:34 DEBUG StreamPartitionAssignor:248 - stream-thread [StreamThread-1] found [TextLinesTopic] topics possibly matching regex
2017-06-12 17:02:34 DEBUG TopologyBuilder:1381 - stream-thread [StreamThread-1] updating builder with SubscriptionUpdates{updatedTopicSubscriptions=[TextLinesTopic]} topic(s) with possible matching regex subscription(s)
2017-06-12 17:02:34 DEBUG AbstractCoordinator:428 - Sending JoinGroup ((type: JoinGroupRequest, groupId=Word Count Sample, sessionTimeout=10000, rebalanceTimeout=2147483647, memberId=, protocolType=consumer, groupProtocols=org.apache.kafka.common.requests.JoinGroupRequest$ProtocolMetadata@31d4ea3a)) to coordinator tmon-jhkwon78-n.tmoncorp.com:9092 (id: 2147483647 rack: null)
2017-06-12 17:02:34 DEBUG Metrics:335 - Added sensor with name node-2147483647.bytes-sent
2017-06-12 17:02:34 DEBUG Metrics:335 - Added sensor with name node-2147483647.bytes-received
2017-06-12 17:02:34 DEBUG Metrics:335 - Added sensor with name node-2147483647.latency
2017-06-12 17:02:34 DEBUG Selector:339 - Created socket with SO_RCVBUF = 65536, SO_SNDBUF = 131072, SO_TIMEOUT = 0 to node 2147483647
2017-06-12 17:02:34 DEBUG NetworkClient:590 - Completed connection to node 2147483647.  Fetching API versions.
2017-06-12 17:02:34 DEBUG NetworkClient:603 - Initiating API versions fetch from node 2147483647.
2017-06-12 17:02:34 DEBUG NetworkClient:558 - Recorded API versions for node 2147483647: (Produce(0): 0 to 2 [usable: 2], Fetch(1): 0 to 3 [usable: 3], Offsets(2): 0 to 1 [usable: 1], Metadata(3): 0 to 2 [usable: 2], LeaderAndIsr(4): 0 [usable: 0], StopReplica(5): 0 [usable: 0], UpdateMetadata(6): 0 to 3 [usable: 3], ControlledShutdown(7): 1 [usable: 1], OffsetCommit(8): 0 to 2 [usable: 2], OffsetFetch(9): 0 to 2 [usable: 2], GroupCoordinator(10): 0 [usable: 0], JoinGroup(11): 0 to 1 [usable: 1], Heartbeat(12): 0 [usable: 0], LeaveGroup(13): 0 [usable: 0], SyncGroup(14): 0 [usable: 0], DescribeGroups(15): 0 [usable: 0], ListGroups(16): 0 [usable: 0], SaslHandshake(17): 0 [usable: 0], ApiVersions(18): 0 [usable: 0], CreateTopics(19): 0 to 1 [usable: 1], DeleteTopics(20): 0 [usable: 0])
2017-06-12 17:02:34 DEBUG AbstractCoordinator:438 - Received successful JoinGroup response for group Word Count Sample: {error_code=0,generation_id=11,group_protocol=stream,leader_id=Word Count Sample-a215918e-7abe-41f7-9de1-96afbab2d581-StreamThread-1-consumer-f69585c8-b003-43c1-b8e9-054d370e5c33,member_id=Word Count Sample-a215918e-7abe-41f7-9de1-96afbab2d581-StreamThread-1-consumer-f69585c8-b003-43c1-b8e9-054d370e5c33,members=[{member_id=Word Count Sample-a215918e-7abe-41f7-9de1-96afbab2d581-StreamThread-1-consumer-f69585c8-b003-43c1-b8e9-054d370e5c33,member_metadata=java.nio.HeapByteBuffer[pos=0 lim=58 cap=58]}]}
2017-06-12 17:02:34 DEBUG ConsumerCoordinator:340 - Performing assignment for group Word Count Sample using strategy stream with subscriptions {Word Count Sample-a215918e-7abe-41f7-9de1-96afbab2d581-StreamThread-1-consumer-f69585c8-b003-43c1-b8e9-054d370e5c33=Subscription(topics=[TextLinesTopic])}
2017-06-12 17:02:34 INFO  StreamPartitionAssignor:300 - stream-thread [StreamThread-1] Constructed client metadata {a215918e-7abe-41f7-9de1-96afbab2d581=ClientMetadata{hostInfo=null, consumers=[Word Count Sample-a215918e-7abe-41f7-9de1-96afbab2d581-StreamThread-1-consumer-f69585c8-b003-43c1-b8e9-054d370e5c33], state=[activeTasks: ([]) assignedTasks: ([]) prevActiveTasks: ([]) prevAssignedTasks: ([]) capacity: 1.0 cost: 0.0]}} from the member subscriptions.
2017-06-12 17:02:34 DEBUG StreamPartitionAssignor:606 - stream-thread [StreamThread-1] Starting to validate internal topics in partition assignor.
2017-06-12 17:02:34 DEBUG Metadata:249 - Updated cluster metadata version 1 to Cluster(id = null, nodes = [localhost:9092 (id: -1 rack: null)], partitions = [])
2017-06-12 17:02:34 DEBUG NetworkClient:627 - Initiating connection to node -1 at localhost:9092.
2017-06-12 17:02:34 DEBUG Metrics:335 - Added sensor with name node--1.bytes-sent
2017-06-12 17:02:34 DEBUG Metrics:335 - Added sensor with name node--1.bytes-received
2017-06-12 17:02:34 DEBUG Metrics:335 - Added sensor with name node--1.latency
2017-06-12 17:02:34 DEBUG Selector:339 - Created socket with SO_RCVBUF = 32768, SO_SNDBUF = 131072, SO_TIMEOUT = 0 to node -1
2017-06-12 17:02:34 DEBUG NetworkClient:590 - Completed connection to node -1.  Fetching API versions.
2017-06-12 17:02:34 DEBUG NetworkClient:603 - Initiating API versions fetch from node -1.
2017-06-12 17:02:34 DEBUG NetworkClient:558 - Recorded API versions for node -1: (Produce(0): 0 to 2 [usable: 2], Fetch(1): 0 to 3 [usable: 3], Offsets(2): 0 to 1 [usable: 1], Metadata(3): 0 to 2 [usable: 2], LeaderAndIsr(4): 0 [usable: 0], StopReplica(5): 0 [usable: 0], UpdateMetadata(6): 0 to 3 [usable: 3], ControlledShutdown(7): 1 [usable: 1], OffsetCommit(8): 0 to 2 [usable: 2], OffsetFetch(9): 0 to 2 [usable: 2], GroupCoordinator(10): 0 [usable: 0], JoinGroup(11): 0 to 1 [usable: 1], Heartbeat(12): 0 [usable: 0], LeaveGroup(13): 0 [usable: 0], SyncGroup(14): 0 [usable: 0], DescribeGroups(15): 0 [usable: 0], ListGroups(16): 0 [usable: 0], SaslHandshake(17): 0 [usable: 0], ApiVersions(18): 0 [usable: 0], CreateTopics(19): 0 to 1 [usable: 1], DeleteTopics(20): 0 [usable: 0])
2017-06-12 17:02:34 DEBUG NetworkClient:627 - Initiating connection to node 0 at tmon-jhkwon78-n.tmoncorp.com:9092.
2017-06-12 17:02:34 DEBUG Metrics:335 - Added sensor with name node-0.bytes-sent
2017-06-12 17:02:34 DEBUG Metrics:335 - Added sensor with name node-0.bytes-received
2017-06-12 17:02:34 DEBUG Metrics:335 - Added sensor with name node-0.latency
2017-06-12 17:02:34 DEBUG Selector:339 - Created socket with SO_RCVBUF = 32768, SO_SNDBUF = 131072, SO_TIMEOUT = 0 to node 0
2017-06-12 17:02:34 DEBUG NetworkClient:590 - Completed connection to node 0.  Fetching API versions.
2017-06-12 17:02:34 DEBUG NetworkClient:603 - Initiating API versions fetch from node 0.
2017-06-12 17:02:34 DEBUG NetworkClient:558 - Recorded API versions for node 0: (Produce(0): 0 to 2 [usable: 2], Fetch(1): 0 to 3 [usable: 3], Offsets(2): 0 to 1 [usable: 1], Metadata(3): 0 to 2 [usable: 2], LeaderAndIsr(4): 0 [usable: 0], StopReplica(5): 0 [usable: 0], UpdateMetadata(6): 0 to 3 [usable: 3], ControlledShutdown(7): 1 [usable: 1], OffsetCommit(8): 0 to 2 [usable: 2], OffsetFetch(9): 0 to 2 [usable: 2], GroupCoordinator(10): 0 [usable: 0], JoinGroup(11): 0 to 1 [usable: 1], Heartbeat(12): 0 [usable: 0], LeaveGroup(13): 0 [usable: 0], SyncGroup(14): 0 [usable: 0], DescribeGroups(15): 0 [usable: 0], ListGroups(16): 0 [usable: 0], SaslHandshake(17): 0 [usable: 0], ApiVersions(18): 0 [usable: 0], CreateTopics(19): 0 to 1 [usable: 1], DeleteTopics(20): 0 [usable: 0])
2017-06-12 17:02:34 WARN  InternalTopicManager:76 - Could not create internal topics: Could not create topic: Word Count Sample-Counts-repartition due to topic name Word Count Sample-Counts-repartition is illegal, contains a character other than ASCII alphanumerics, '.', '_' and '-' Retry #0
2017-06-12 17:02:34 DEBUG Metadata:249 - Updated cluster metadata version 1 to Cluster(id = null, nodes = [localhost:9092 (id: -1 rack: null)], partitions = [])
2017-06-12 17:02:35 WARN  InternalTopicManager:76 - Could not create internal topics: Could not create topic: Word Count Sample-Counts-repartition due to topic name Word Count Sample-Counts-repartition is illegal, contains a character other than ASCII alphanumerics, '.', '_' and '-' Retry #1
2017-06-12 17:02:35 DEBUG Metadata:249 - Updated cluster metadata version 1 to Cluster(id = null, nodes = [localhost:9092 (id: -1 rack: null)], partitions = [])
2017-06-12 17:02:35 WARN  InternalTopicManager:76 - Could not create internal topics: Could not create topic: Word Count Sample-Counts-repartition due to topic name Word Count Sample-Counts-repartition is illegal, contains a character other than ASCII alphanumerics, '.', '_' and '-' Retry #2
2017-06-12 17:02:35 DEBUG Metadata:249 - Updated cluster metadata version 1 to Cluster(id = null, nodes = [localhost:9092 (id: -1 rack: null)], partitions = [])
2017-06-12 17:02:35 WARN  InternalTopicManager:76 - Could not create internal topics: Could not create topic: Word Count Sample-Counts-repartition due to topic name Word Count Sample-Counts-repartition is illegal, contains a character other than ASCII alphanumerics, '.', '_' and '-' Retry #3
2017-06-12 17:02:35 DEBUG Metadata:249 - Updated cluster metadata version 1 to Cluster(id = null, nodes = [localhost:9092 (id: -1 rack: null)], partitions = [])
2017-06-12 17:02:36 WARN  InternalTopicManager:76 - Could not create internal topics: Could not create topic: Word Count Sample-Counts-repartition due to topic name Word Count Sample-Counts-repartition is illegal, contains a character other than ASCII alphanumerics, '.', '_' and '-' Retry #4
2017-06-12 17:02:36 INFO  StreamThread:390 - stream-thread [StreamThread-1] Shutting down
2017-06-12 17:02:36 DEBUG StreamThread:439 - stream-thread [StreamThread-1] shutdownTasksAndState: shutting downactive tasks [], standby tasks [], suspended tasks [], and suspended standby tasks []
2017-06-12 17:02:36 DEBUG KafkaConsumer:899 - Unsubscribed all topics or patterns and assigned partitions
2017-06-12 17:02:36 INFO  KafkaProducer:689 - Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2017-06-12 17:02:36 DEBUG Sender:132 - Beginning shutdown of Kafka producer I/O thread, sending remaining records.
2017-06-12 17:02:36 DEBUG Metrics:368 - Removed sensor with name connections-closed:
2017-06-12 17:02:36 DEBUG Metrics:368 - Removed sensor with name connections-created:
2017-06-12 17:02:36 DEBUG Metrics:368 - Removed sensor with name bytes-sent-received:
2017-06-12 17:02:36 DEBUG Metrics:368 - Removed sensor with name bytes-sent:
2017-06-12 17:02:36 DEBUG Metrics:368 - Removed sensor with name bytes-received:
2017-06-12 17:02:36 DEBUG Metrics:368 - Removed sensor with name select-time:
2017-06-12 17:02:36 DEBUG Metrics:368 - Removed sensor with name io-time:
2017-06-12 17:02:36 DEBUG Sender:155 - Shutdown of Kafka producer I/O thread has completed.
2017-06-12 17:02:36 DEBUG KafkaProducer:732 - The Kafka producer has closed.
2017-06-12 17:02:36 DEBUG AbstractCoordinator:967 - Heartbeat thread for group Word Count Sample has closed
2017-06-12 17:02:36 DEBUG Metrics:368 - Removed sensor with name connections-closed:
2017-06-12 17:02:36 DEBUG Metrics:368 - Removed sensor with name connections-created:
2017-06-12 17:02:36 DEBUG Metrics:368 - Removed sensor with name bytes-sent-received:
2017-06-12 17:02:36 DEBUG Metrics:368 - Removed sensor with name bytes-sent:
2017-06-12 17:02:36 DEBUG Metrics:368 - Removed sensor with name bytes-received:
2017-06-12 17:02:36 DEBUG Metrics:368 - Removed sensor with name select-time:
2017-06-12 17:02:36 DEBUG Metrics:368 - Removed sensor with name io-time:
2017-06-12 17:02:36 DEBUG Metrics:368 - Removed sensor with name node--1.bytes-sent
2017-06-12 17:02:36 DEBUG Metrics:368 - Removed sensor with name node--1.bytes-received
2017-06-12 17:02:36 DEBUG Metrics:368 - Removed sensor with name node--1.latency
2017-06-12 17:02:36 DEBUG Metrics:368 - Removed sensor with name node-2147483647.bytes-sent
2017-06-12 17:02:36 DEBUG Metrics:368 - Removed sensor with name node-2147483647.bytes-received
2017-06-12 17:02:36 DEBUG Metrics:368 - Removed sensor with name node-2147483647.latency
2017-06-12 17:02:36 DEBUG KafkaConsumer:1568 - The Kafka consumer has closed.
2017-06-12 17:02:36 DEBUG Metrics:368 - Removed sensor with name connections-closed:
2017-06-12 17:02:36 DEBUG Metrics:368 - Removed sensor with name connections-created:
2017-06-12 17:02:36 DEBUG Metrics:368 - Removed sensor with name bytes-sent-received:
2017-06-12 17:02:36 DEBUG Metrics:368 - Removed sensor with name bytes-sent:
2017-06-12 17:02:36 DEBUG Metrics:368 - Removed sensor with name bytes-received:
2017-06-12 17:02:36 DEBUG Metrics:368 - Removed sensor with name select-time:
2017-06-12 17:02:36 DEBUG Metrics:368 - Removed sensor with name io-time:
2017-06-12 17:02:36 DEBUG KafkaConsumer:1568 - The Kafka consumer has closed.
2017-06-12 17:02:36 INFO  StreamThread:1049 - stream-thread [StreamThread-1] Removing all active tasks []
2017-06-12 17:02:36 INFO  StreamThread:1064 - stream-thread [StreamThread-1] Removing all standby tasks []
2017-06-12 17:02:36 INFO  StreamThread:420 - stream-thread [StreamThread-1] Stream thread shutdown complete
2017-06-12 17:02:36 WARN  StreamThread:161 - stream-thread [StreamThread-1] Unexpected state transition from PARTITIONS_REVOKED to NOT_RUNNING.
2017-06-12 17:02:36 DEBUG Metrics:368 - Removed sensor with name thread.Word Count Sample-a215918e-7abe-41f7-9de1-96afbab2d581-StreamThread-1.commit-latency
2017-06-12 17:02:36 DEBUG Metrics:368 - Removed sensor with name thread.Word Count Sample-a215918e-7abe-41f7-9de1-96afbab2d581-StreamThread-1.poll-latency
2017-06-12 17:02:36 DEBUG Metrics:368 - Removed sensor with name thread.Word Count Sample-a215918e-7abe-41f7-9de1-96afbab2d581-StreamThread-1.process-latency
2017-06-12 17:02:36 DEBUG Metrics:368 - Removed sensor with name thread.Word Count Sample-a215918e-7abe-41f7-9de1-96afbab2d581-StreamThread-1.punctuate-latency
2017-06-12 17:02:36 DEBUG Metrics:368 - Removed sensor with name thread.Word Count Sample-a215918e-7abe-41f7-9de1-96afbab2d581-StreamThread-1.task-created
2017-06-12 17:02:36 DEBUG Metrics:368 - Removed sensor with name thread.Word Count Sample-a215918e-7abe-41f7-9de1-96afbab2d581-StreamThread-1.task-closed
2017-06-12 17:02:36 DEBUG Metrics:368 - Removed sensor with name thread.Word Count Sample-a215918e-7abe-41f7-9de1-96afbab2d581-StreamThread-1.skipped-records
2017-06-12 17:03:26 INFO  ConsumerConfig:180 - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.LongDeserializer

2017-06-12 17:03:26 DEBUG KafkaConsumer:607 - Starting the Kafka consumer
2017-06-12 17:03:26 DEBUG Metadata:249 - Updated cluster metadata version 1 to Cluster(id = null, nodes = [localhost:9092 (id: -1 rack: null)], partitions = [])
2017-06-12 17:03:26 DEBUG Metrics:335 - Added sensor with name connections-closed:
2017-06-12 17:03:26 DEBUG Metrics:335 - Added sensor with name connections-created:
2017-06-12 17:03:26 DEBUG Metrics:335 - Added sensor with name bytes-sent-received:
2017-06-12 17:03:26 DEBUG Metrics:335 - Added sensor with name bytes-sent:
2017-06-12 17:03:26 DEBUG Metrics:335 - Added sensor with name bytes-received:
2017-06-12 17:03:26 DEBUG Metrics:335 - Added sensor with name select-time:
2017-06-12 17:03:26 DEBUG Metrics:335 - Added sensor with name io-time:
2017-06-12 17:03:26 DEBUG Metrics:335 - Added sensor with name heartbeat-latency
2017-06-12 17:03:26 DEBUG Metrics:335 - Added sensor with name join-latency
2017-06-12 17:03:26 DEBUG Metrics:335 - Added sensor with name sync-latency
2017-06-12 17:03:26 DEBUG Metrics:335 - Added sensor with name commit-latency
2017-06-12 17:03:26 DEBUG Metrics:335 - Added sensor with name bytes-fetched
2017-06-12 17:03:26 DEBUG Metrics:335 - Added sensor with name records-fetched
2017-06-12 17:03:26 DEBUG Metrics:335 - Added sensor with name fetch-latency
2017-06-12 17:03:26 DEBUG Metrics:335 - Added sensor with name records-lag
2017-06-12 17:03:26 DEBUG Metrics:335 - Added sensor with name fetch-throttle-time
2017-06-12 17:03:26 INFO  AppInfoParser:83 - Kafka version : 0.10.2.1
2017-06-12 17:03:26 INFO  AppInfoParser:84 - Kafka commitId : e89bffd6b2eff799
2017-06-12 17:03:26 DEBUG KafkaConsumer:711 - Kafka consumer created
2017-06-12 17:03:26 DEBUG KafkaConsumer:824 - Subscribed to topic(s): WordsWithCountsTopic
2017-06-12 17:03:26 DEBUG AbstractCoordinator:561 - Sending GroupCoordinator request for group test to broker localhost:9092 (id: -1 rack: null)
2017-06-12 17:03:26 DEBUG NetworkClient:627 - Initiating connection to node -1 at localhost:9092.
2017-06-12 17:03:26 DEBUG Metrics:335 - Added sensor with name node--1.bytes-sent
2017-06-12 17:03:26 DEBUG Metrics:335 - Added sensor with name node--1.bytes-received
2017-06-12 17:03:26 DEBUG Metrics:335 - Added sensor with name node--1.latency
2017-06-12 17:03:26 DEBUG Selector:339 - Created socket with SO_RCVBUF = 65536, SO_SNDBUF = 131072, SO_TIMEOUT = 0 to node -1
2017-06-12 17:03:26 DEBUG NetworkClient:590 - Completed connection to node -1.  Fetching API versions.
2017-06-12 17:03:26 DEBUG NetworkClient:603 - Initiating API versions fetch from node -1.
2017-06-12 17:03:26 DEBUG NetworkClient:558 - Recorded API versions for node -1: (Produce(0): 0 to 2 [usable: 2], Fetch(1): 0 to 3 [usable: 3], Offsets(2): 0 to 1 [usable: 1], Metadata(3): 0 to 2 [usable: 2], LeaderAndIsr(4): 0 [usable: 0], StopReplica(5): 0 [usable: 0], UpdateMetadata(6): 0 to 3 [usable: 3], ControlledShutdown(7): 1 [usable: 1], OffsetCommit(8): 0 to 2 [usable: 2], OffsetFetch(9): 0 to 2 [usable: 2], GroupCoordinator(10): 0 [usable: 0], JoinGroup(11): 0 to 1 [usable: 1], Heartbeat(12): 0 [usable: 0], LeaveGroup(13): 0 [usable: 0], SyncGroup(14): 0 [usable: 0], DescribeGroups(15): 0 [usable: 0], ListGroups(16): 0 [usable: 0], SaslHandshake(17): 0 [usable: 0], ApiVersions(18): 0 [usable: 0], CreateTopics(19): 0 to 1 [usable: 1], DeleteTopics(20): 0 [usable: 0])
2017-06-12 17:03:26 DEBUG NetworkClient:751 - Sending metadata request (type=MetadataRequest, topics=WordsWithCountsTopic) to node -1
2017-06-12 17:03:26 DEBUG Metadata:249 - Updated cluster metadata version 2 to Cluster(id = kbd_yf8GQXCf13cNvyCN1w, nodes = [tmon-jhkwon78-n.tmoncorp.com:9092 (id: 0 rack: null)], partitions = [Partition(topic = WordsWithCountsTopic, partition = 0, leader = 0, replicas = [0], isr = [0])])
2017-06-12 17:03:26 DEBUG AbstractCoordinator:572 - Received GroupCoordinator response ClientResponse(receivedTimeMs=1497254606329, latencyMs=27, disconnected=false, requestHeader={api_key=10,api_version=0,correlation_id=0,client_id=consumer-1}, responseBody={error_code=0,coordinator={node_id=0,host=tmon-jhkwon78-n.tmoncorp.com,port=9092}}) for group test
2017-06-12 17:03:26 INFO  AbstractCoordinator:586 - Discovered coordinator tmon-jhkwon78-n.tmoncorp.com:9092 (id: 2147483647 rack: null) for group test.
2017-06-12 17:03:26 DEBUG NetworkClient:627 - Initiating connection to node 2147483647 at tmon-jhkwon78-n.tmoncorp.com:9092.
2017-06-12 17:03:26 DEBUG ConsumerCoordinator:641 - Sending synchronous auto-commit of offsets {} for group test
2017-06-12 17:03:26 INFO  ConsumerCoordinator:397 - Revoking previously assigned partitions [] for group test
2017-06-12 17:03:26 INFO  AbstractCoordinator:420 - (Re-)joining group test
2017-06-12 17:03:26 DEBUG AbstractCoordinator:428 - Sending JoinGroup ((type: JoinGroupRequest, groupId=test, sessionTimeout=10000, rebalanceTimeout=300000, memberId=, protocolType=consumer, groupProtocols=org.apache.kafka.common.requests.JoinGroupRequest$ProtocolMetadata@4edbed33)) to coordinator tmon-jhkwon78-n.tmoncorp.com:9092 (id: 2147483647 rack: null)
2017-06-12 17:03:26 DEBUG AbstractCoordinator:889 - Heartbeat thread for group test started
2017-06-12 17:03:26 DEBUG Metrics:335 - Added sensor with name node-2147483647.bytes-sent
2017-06-12 17:03:26 DEBUG Metrics:335 - Added sensor with name node-2147483647.bytes-received
2017-06-12 17:03:26 DEBUG Metrics:335 - Added sensor with name node-2147483647.latency
2017-06-12 17:03:26 DEBUG Selector:339 - Created socket with SO_RCVBUF = 65536, SO_SNDBUF = 131072, SO_TIMEOUT = 0 to node 2147483647
2017-06-12 17:03:26 DEBUG NetworkClient:590 - Completed connection to node 2147483647.  Fetching API versions.
2017-06-12 17:03:26 DEBUG NetworkClient:603 - Initiating API versions fetch from node 2147483647.
2017-06-12 17:03:26 DEBUG NetworkClient:558 - Recorded API versions for node 2147483647: (Produce(0): 0 to 2 [usable: 2], Fetch(1): 0 to 3 [usable: 3], Offsets(2): 0 to 1 [usable: 1], Metadata(3): 0 to 2 [usable: 2], LeaderAndIsr(4): 0 [usable: 0], StopReplica(5): 0 [usable: 0], UpdateMetadata(6): 0 to 3 [usable: 3], ControlledShutdown(7): 1 [usable: 1], OffsetCommit(8): 0 to 2 [usable: 2], OffsetFetch(9): 0 to 2 [usable: 2], GroupCoordinator(10): 0 [usable: 0], JoinGroup(11): 0 to 1 [usable: 1], Heartbeat(12): 0 [usable: 0], LeaveGroup(13): 0 [usable: 0], SyncGroup(14): 0 [usable: 0], DescribeGroups(15): 0 [usable: 0], ListGroups(16): 0 [usable: 0], SaslHandshake(17): 0 [usable: 0], ApiVersions(18): 0 [usable: 0], CreateTopics(19): 0 to 1 [usable: 1], DeleteTopics(20): 0 [usable: 0])
2017-06-12 17:03:26 DEBUG AbstractCoordinator:438 - Received successful JoinGroup response for group test: {error_code=0,generation_id=42,group_protocol=range,leader_id=consumer-1-3b381238-798e-40de-a996-7711e8ae2eac,member_id=consumer-1-3b381238-798e-40de-a996-7711e8ae2eac,members=[{member_id=consumer-1-3b381238-798e-40de-a996-7711e8ae2eac,member_metadata=java.nio.HeapByteBuffer[pos=0 lim=32 cap=32]}]}
2017-06-12 17:03:26 DEBUG ConsumerCoordinator:340 - Performing assignment for group test using strategy range with subscriptions {consumer-1-3b381238-798e-40de-a996-7711e8ae2eac=Subscription(topics=[WordsWithCountsTopic])}
2017-06-12 17:03:26 DEBUG ConsumerCoordinator:379 - Finished assignment for group test: {consumer-1-3b381238-798e-40de-a996-7711e8ae2eac=Assignment(partitions=[WordsWithCountsTopic-0])}
2017-06-12 17:03:26 DEBUG AbstractCoordinator:506 - Sending leader SyncGroup for group test to coordinator tmon-jhkwon78-n.tmoncorp.com:9092 (id: 2147483647 rack: null): (type=SyncGroupRequest, groupId=test, generationId=42, memberId=consumer-1-3b381238-798e-40de-a996-7711e8ae2eac, groupAssignment=consumer-1-3b381238-798e-40de-a996-7711e8ae2eac)
2017-06-12 17:03:26 INFO  AbstractCoordinator:388 - Successfully joined group test with generation 42
2017-06-12 17:03:26 INFO  ConsumerCoordinator:256 - Setting newly assigned partitions [WordsWithCountsTopic-0] for group test
2017-06-12 17:03:26 DEBUG ConsumerCoordinator:804 - Group test fetching committed offsets for partitions: [WordsWithCountsTopic-0]
2017-06-12 17:03:26 DEBUG Fetcher:251 - Resetting offset for partition WordsWithCountsTopic-0 to the committed offset 50
2017-06-12 17:03:26 DEBUG Fetcher:181 - Sending fetch for partitions [WordsWithCountsTopic-0] to broker tmon-jhkwon78-n.tmoncorp.com:9092 (id: 0 rack: null)
2017-06-12 17:03:26 DEBUG NetworkClient:627 - Initiating connection to node 0 at tmon-jhkwon78-n.tmoncorp.com:9092.
2017-06-12 17:03:26 DEBUG Metrics:335 - Added sensor with name node-0.bytes-sent
2017-06-12 17:03:26 DEBUG Metrics:335 - Added sensor with name node-0.bytes-received
2017-06-12 17:03:26 DEBUG Metrics:335 - Added sensor with name node-0.latency
2017-06-12 17:03:26 DEBUG Selector:339 - Created socket with SO_RCVBUF = 65536, SO_SNDBUF = 131072, SO_TIMEOUT = 0 to node 0
2017-06-12 17:03:26 DEBUG NetworkClient:590 - Completed connection to node 0.  Fetching API versions.
2017-06-12 17:03:26 DEBUG NetworkClient:603 - Initiating API versions fetch from node 0.
2017-06-12 17:03:26 DEBUG NetworkClient:558 - Recorded API versions for node 0: (Produce(0): 0 to 2 [usable: 2], Fetch(1): 0 to 3 [usable: 3], Offsets(2): 0 to 1 [usable: 1], Metadata(3): 0 to 2 [usable: 2], LeaderAndIsr(4): 0 [usable: 0], StopReplica(5): 0 [usable: 0], UpdateMetadata(6): 0 to 3 [usable: 3], ControlledShutdown(7): 1 [usable: 1], OffsetCommit(8): 0 to 2 [usable: 2], OffsetFetch(9): 0 to 2 [usable: 2], GroupCoordinator(10): 0 [usable: 0], JoinGroup(11): 0 to 1 [usable: 1], Heartbeat(12): 0 [usable: 0], LeaveGroup(13): 0 [usable: 0], SyncGroup(14): 0 [usable: 0], DescribeGroups(15): 0 [usable: 0], ListGroups(16): 0 [usable: 0], SaslHandshake(17): 0 [usable: 0], ApiVersions(18): 0 [usable: 0], CreateTopics(19): 0 to 1 [usable: 1], DeleteTopics(20): 0 [usable: 0])
2017-06-12 17:03:27 DEBUG Metrics:335 - Added sensor with name topic.WordsWithCountsTopic.bytes-fetched
2017-06-12 17:03:27 DEBUG Metrics:335 - Added sensor with name topic.WordsWithCountsTopic.records-fetched
2017-06-12 17:03:27 DEBUG Metrics:335 - Added sensor with name WordsWithCountsTopic-0.records-lag
2017-06-12 17:03:27 DEBUG Fetcher:181 - Sending fetch for partitions [WordsWithCountsTopic-0] to broker tmon-jhkwon78-n.tmoncorp.com:9092 (id: 0 rack: null)
2017-06-12 17:03:27 DEBUG ConsumerCoordinator:620 - Sending asynchronous auto-commit of offsets {WordsWithCountsTopic-0=OffsetAndMetadata{offset=50, metadata=''}} for group test
2017-06-12 17:03:27 DEBUG ConsumerCoordinator:736 - Group test committed offset 50 for partition WordsWithCountsTopic-0
2017-06-12 17:03:27 DEBUG ConsumerCoordinator:631 - Completed auto-commit of offsets {WordsWithCountsTopic-0=OffsetAndMetadata{offset=50, metadata=''}} for group test
2017-06-12 17:03:27 DEBUG Fetcher:181 - Sending fetch for partitions [WordsWithCountsTopic-0] to broker tmon-jhkwon78-n.tmoncorp.com:9092 (id: 0 rack: null)
2017-06-12 17:03:34 INFO  StreamsConfig:180 - StreamsConfig values: 
	application.id = wordcount-lambda-example
	application.server = 
	bootstrap.servers = [localhost:9092]
	buffered.records.per.partition = 1000
	cache.max.bytes.buffering = 10485760
	client.id = 
	commit.interval.ms = 30000
	connections.max.idle.ms = 540000
	key.serde = class org.apache.kafka.common.serialization.Serdes$StringSerde
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	num.standby.replicas = 0
	num.stream.threads = 1
	partition.grouper = class org.apache.kafka.streams.processor.DefaultPartitionGrouper
	poll.ms = 100
	receive.buffer.bytes = 32768
	reconnect.backoff.ms = 50
	replication.factor = 1
	request.timeout.ms = 40000
	retry.backoff.ms = 100
	rocksdb.config.setter = null
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	state.cleanup.delay.ms = 60000
	state.dir = /tmp/kafka-streams
	timestamp.extractor = class org.apache.kafka.streams.processor.FailOnInvalidTimestamp
	value.serde = class org.apache.kafka.common.serialization.Serdes$StringSerde
	windowstore.changelog.additional.retention.ms = 86400000
	zookeeper.connect = 

2017-06-12 17:03:36 DEBUG Metrics:335 - Added sensor with name thread.wordcount-lambda-example-1488bda3-f635-4518-b604-d71e08791d1c-StreamThread-1.commit-latency
2017-06-12 17:03:36 DEBUG Metrics:335 - Added sensor with name thread.wordcount-lambda-example-1488bda3-f635-4518-b604-d71e08791d1c-StreamThread-1.poll-latency
2017-06-12 17:03:36 DEBUG Metrics:335 - Added sensor with name thread.wordcount-lambda-example-1488bda3-f635-4518-b604-d71e08791d1c-StreamThread-1.process-latency
2017-06-12 17:03:36 DEBUG Metrics:335 - Added sensor with name thread.wordcount-lambda-example-1488bda3-f635-4518-b604-d71e08791d1c-StreamThread-1.punctuate-latency
2017-06-12 17:03:36 DEBUG Metrics:335 - Added sensor with name thread.wordcount-lambda-example-1488bda3-f635-4518-b604-d71e08791d1c-StreamThread-1.task-created
2017-06-12 17:03:36 DEBUG Metrics:335 - Added sensor with name thread.wordcount-lambda-example-1488bda3-f635-4518-b604-d71e08791d1c-StreamThread-1.task-closed
2017-06-12 17:03:36 DEBUG Metrics:335 - Added sensor with name thread.wordcount-lambda-example-1488bda3-f635-4518-b604-d71e08791d1c-StreamThread-1.skipped-records
2017-06-12 17:03:36 INFO  StreamThread:304 - stream-thread [StreamThread-1] Creating producer client
2017-06-12 17:03:36 INFO  ProducerConfig:180 - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	block.on.buffer.full = false
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = wordcount-lambda-example-1488bda3-f635-4518-b604-d71e08791d1c-StreamThread-1-producer
	compression.type = none
	connections.max.idle.ms = 540000
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 100
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.fetch.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	timeout.ms = 30000
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2017-06-12 17:03:36 DEBUG Metrics:335 - Added sensor with name bufferpool-wait-time
2017-06-12 17:03:36 DEBUG Metrics:335 - Added sensor with name buffer-exhausted-records
2017-06-12 17:03:37 DEBUG Metadata:249 - Updated cluster metadata version 1 to Cluster(id = null, nodes = [localhost:9092 (id: -1 rack: null)], partitions = [])
2017-06-12 17:03:37 DEBUG Metrics:335 - Added sensor with name connections-closed:
2017-06-12 17:03:37 DEBUG Metrics:335 - Added sensor with name connections-created:
2017-06-12 17:03:37 DEBUG Metrics:335 - Added sensor with name bytes-sent-received:
2017-06-12 17:03:37 DEBUG Metrics:335 - Added sensor with name bytes-sent:
2017-06-12 17:03:37 DEBUG Metrics:335 - Added sensor with name bytes-received:
2017-06-12 17:03:37 DEBUG Metrics:335 - Added sensor with name select-time:
2017-06-12 17:03:37 DEBUG Metrics:335 - Added sensor with name io-time:
2017-06-12 17:03:37 DEBUG Metrics:335 - Added sensor with name batch-size
2017-06-12 17:03:37 DEBUG Metrics:335 - Added sensor with name compression-rate
2017-06-12 17:03:37 DEBUG Metrics:335 - Added sensor with name queue-time
2017-06-12 17:03:37 DEBUG Metrics:335 - Added sensor with name request-time
2017-06-12 17:03:37 DEBUG Metrics:335 - Added sensor with name produce-throttle-time
2017-06-12 17:03:37 DEBUG Metrics:335 - Added sensor with name records-per-request
2017-06-12 17:03:37 DEBUG Metrics:335 - Added sensor with name record-retries
2017-06-12 17:03:37 DEBUG Metrics:335 - Added sensor with name errors
2017-06-12 17:03:37 DEBUG Metrics:335 - Added sensor with name record-size-max
2017-06-12 17:03:37 DEBUG Sender:121 - Starting Kafka producer I/O thread.
2017-06-12 17:03:37 INFO  AppInfoParser:83 - Kafka version : 0.10.2.1
2017-06-12 17:03:37 INFO  AppInfoParser:84 - Kafka commitId : e89bffd6b2eff799
2017-06-12 17:03:37 DEBUG KafkaProducer:336 - Kafka producer started
2017-06-12 17:03:37 INFO  StreamThread:306 - stream-thread [StreamThread-1] Creating consumer client
2017-06-12 17:03:37 INFO  ConsumerConfig:180 - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = wordcount-lambda-example-1488bda3-f635-4518-b604-d71e08791d1c-StreamThread-1-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = wordcount-lambda-example
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 2147483647
	max.poll.records = 1000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamPartitionAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2017-06-12 17:03:37 DEBUG KafkaConsumer:607 - Starting the Kafka consumer
2017-06-12 17:03:37 DEBUG Metadata:249 - Updated cluster metadata version 1 to Cluster(id = null, nodes = [localhost:9092 (id: -1 rack: null)], partitions = [])
2017-06-12 17:03:37 DEBUG Metrics:335 - Added sensor with name connections-closed:
2017-06-12 17:03:37 DEBUG Metrics:335 - Added sensor with name connections-created:
2017-06-12 17:03:37 DEBUG Metrics:335 - Added sensor with name bytes-sent-received:
2017-06-12 17:03:37 DEBUG Metrics:335 - Added sensor with name bytes-sent:
2017-06-12 17:03:37 DEBUG Metrics:335 - Added sensor with name bytes-received:
2017-06-12 17:03:37 DEBUG Metrics:335 - Added sensor with name select-time:
2017-06-12 17:03:37 DEBUG Metrics:335 - Added sensor with name io-time:
2017-06-12 17:03:37 DEBUG Metadata:249 - Updated cluster metadata version 1 to Cluster(id = null, nodes = [localhost:9092 (id: -1 rack: null)], partitions = [])
2017-06-12 17:03:37 DEBUG Metrics:335 - Added sensor with name connections-closed:
2017-06-12 17:03:37 DEBUG Metrics:335 - Added sensor with name connections-created:
2017-06-12 17:03:37 DEBUG Metrics:335 - Added sensor with name bytes-sent-received:
2017-06-12 17:03:37 DEBUG Metrics:335 - Added sensor with name bytes-sent:
2017-06-12 17:03:37 DEBUG Metrics:335 - Added sensor with name bytes-received:
2017-06-12 17:03:37 DEBUG Metrics:335 - Added sensor with name select-time:
2017-06-12 17:03:37 DEBUG Metrics:335 - Added sensor with name io-time:
2017-06-12 17:03:37 DEBUG Metrics:335 - Added sensor with name heartbeat-latency
2017-06-12 17:03:37 DEBUG Metrics:335 - Added sensor with name join-latency
2017-06-12 17:03:37 DEBUG Metrics:335 - Added sensor with name sync-latency
2017-06-12 17:03:37 DEBUG Metrics:335 - Added sensor with name commit-latency
2017-06-12 17:03:37 DEBUG Metrics:335 - Added sensor with name bytes-fetched
2017-06-12 17:03:37 DEBUG Metrics:335 - Added sensor with name records-fetched
2017-06-12 17:03:37 DEBUG Metrics:335 - Added sensor with name fetch-latency
2017-06-12 17:03:37 DEBUG Metrics:335 - Added sensor with name records-lag
2017-06-12 17:03:37 DEBUG Metrics:335 - Added sensor with name fetch-throttle-time
2017-06-12 17:03:37 INFO  AppInfoParser:83 - Kafka version : 0.10.2.1
2017-06-12 17:03:37 INFO  AppInfoParser:84 - Kafka commitId : e89bffd6b2eff799
2017-06-12 17:03:37 DEBUG KafkaConsumer:711 - Kafka consumer created
2017-06-12 17:03:37 INFO  StreamThread:317 - stream-thread [StreamThread-1] Creating restore consumer client
2017-06-12 17:03:37 INFO  ConsumerConfig:180 - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = wordcount-lambda-example-1488bda3-f635-4518-b604-d71e08791d1c-StreamThread-1-restore-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 2147483647
	max.poll.records = 1000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2017-06-12 17:03:37 DEBUG KafkaConsumer:607 - Starting the Kafka consumer
2017-06-12 17:03:37 DEBUG Metadata:249 - Updated cluster metadata version 1 to Cluster(id = null, nodes = [localhost:9092 (id: -1 rack: null)], partitions = [])
2017-06-12 17:03:37 DEBUG Metrics:335 - Added sensor with name connections-closed:
2017-06-12 17:03:37 DEBUG Metrics:335 - Added sensor with name connections-created:
2017-06-12 17:03:37 DEBUG Metrics:335 - Added sensor with name bytes-sent-received:
2017-06-12 17:03:37 DEBUG Metrics:335 - Added sensor with name bytes-sent:
2017-06-12 17:03:37 DEBUG Metrics:335 - Added sensor with name bytes-received:
2017-06-12 17:03:37 DEBUG Metrics:335 - Added sensor with name select-time:
2017-06-12 17:03:37 DEBUG Metrics:335 - Added sensor with name io-time:
2017-06-12 17:03:37 DEBUG Metrics:335 - Added sensor with name heartbeat-latency
2017-06-12 17:03:37 DEBUG Metrics:335 - Added sensor with name join-latency
2017-06-12 17:03:37 DEBUG Metrics:335 - Added sensor with name sync-latency
2017-06-12 17:03:37 DEBUG Metrics:335 - Added sensor with name commit-latency
2017-06-12 17:03:37 DEBUG Metrics:335 - Added sensor with name bytes-fetched
2017-06-12 17:03:37 DEBUG Metrics:335 - Added sensor with name records-fetched
2017-06-12 17:03:37 DEBUG Metrics:335 - Added sensor with name fetch-latency
2017-06-12 17:03:37 DEBUG Metrics:335 - Added sensor with name records-lag
2017-06-12 17:03:37 DEBUG Metrics:335 - Added sensor with name fetch-throttle-time
2017-06-12 17:03:37 INFO  AppInfoParser:83 - Kafka version : 0.10.2.1
2017-06-12 17:03:37 INFO  AppInfoParser:84 - Kafka commitId : e89bffd6b2eff799
2017-06-12 17:03:37 DEBUG KafkaConsumer:711 - Kafka consumer created
2017-06-12 17:03:37 INFO  StreamThread:163 - stream-thread [StreamThread-1] State transition from NOT_RUNNING to RUNNING.
2017-06-12 17:03:37 DEBUG KafkaStreams:422 - stream-client [wordcount-lambda-example-1488bda3-f635-4518-b604-d71e08791d1c] Starting Kafka Stream process.
2017-06-12 17:03:37 DEBUG Metadata:249 - Updated cluster metadata version 1 to Cluster(id = null, nodes = [localhost:9092 (id: -1 rack: null)], partitions = [])
2017-06-12 17:03:37 DEBUG Metrics:335 - Added sensor with name connections-closed:
2017-06-12 17:03:37 DEBUG Metrics:335 - Added sensor with name connections-created:
2017-06-12 17:03:37 DEBUG Metrics:335 - Added sensor with name bytes-sent-received:
2017-06-12 17:03:37 DEBUG Metrics:335 - Added sensor with name bytes-sent:
2017-06-12 17:03:37 DEBUG Metrics:335 - Added sensor with name bytes-received:
2017-06-12 17:03:37 DEBUG Metrics:335 - Added sensor with name select-time:
2017-06-12 17:03:37 DEBUG Metrics:335 - Added sensor with name io-time:
2017-06-12 17:03:37 DEBUG Metadata:249 - Updated cluster metadata version 1 to Cluster(id = null, nodes = [localhost:9092 (id: -1 rack: null)], partitions = [])
2017-06-12 17:03:37 DEBUG NetworkClient:627 - Initiating connection to node -1 at localhost:9092.
2017-06-12 17:03:37 DEBUG Metrics:335 - Added sensor with name node--1.bytes-sent
2017-06-12 17:03:37 DEBUG Metrics:335 - Added sensor with name node--1.bytes-received
2017-06-12 17:03:37 DEBUG Metrics:335 - Added sensor with name node--1.latency
2017-06-12 17:03:37 DEBUG Selector:339 - Created socket with SO_RCVBUF = 32768, SO_SNDBUF = 131072, SO_TIMEOUT = 0 to node -1
2017-06-12 17:03:37 DEBUG NetworkClient:590 - Completed connection to node -1.  Fetching API versions.
2017-06-12 17:03:37 DEBUG NetworkClient:603 - Initiating API versions fetch from node -1.
2017-06-12 17:03:37 DEBUG NetworkClient:558 - Recorded API versions for node -1: (Produce(0): 0 to 2 [usable: 2], Fetch(1): 0 to 3 [usable: 3], Offsets(2): 0 to 1 [usable: 1], Metadata(3): 0 to 2 [usable: 2], LeaderAndIsr(4): 0 [usable: 0], StopReplica(5): 0 [usable: 0], UpdateMetadata(6): 0 to 3 [usable: 3], ControlledShutdown(7): 1 [usable: 1], OffsetCommit(8): 0 to 2 [usable: 2], OffsetFetch(9): 0 to 2 [usable: 2], GroupCoordinator(10): 0 [usable: 0], JoinGroup(11): 0 to 1 [usable: 1], Heartbeat(12): 0 [usable: 0], LeaveGroup(13): 0 [usable: 0], SyncGroup(14): 0 [usable: 0], DescribeGroups(15): 0 [usable: 0], ListGroups(16): 0 [usable: 0], SaslHandshake(17): 0 [usable: 0], ApiVersions(18): 0 [usable: 0], CreateTopics(19): 0 to 1 [usable: 1], DeleteTopics(20): 0 [usable: 0])
2017-06-12 17:03:37 INFO  KafkaStreams:224 - stream-client [wordcount-lambda-example-1488bda3-f635-4518-b604-d71e08791d1c] State transition from CREATED to RUNNING.
2017-06-12 17:03:37 INFO  KafkaStreams:436 - stream-client [wordcount-lambda-example-1488bda3-f635-4518-b604-d71e08791d1c] Started Kafka Stream process
2017-06-12 17:03:37 INFO  StreamThread:358 - stream-thread [StreamThread-1] Starting
2017-06-12 17:03:37 DEBUG KafkaConsumer:882 - Subscribed to pattern: TextLinesTopic|wordcount-lambda-example-Counts-repartition
2017-06-12 17:03:37 DEBUG AbstractCoordinator:561 - Sending GroupCoordinator request for group wordcount-lambda-example to broker localhost:9092 (id: -1 rack: null)
2017-06-12 17:03:37 DEBUG NetworkClient:627 - Initiating connection to node -1 at localhost:9092.
2017-06-12 17:03:37 DEBUG Metrics:335 - Added sensor with name node--1.bytes-sent
2017-06-12 17:03:37 DEBUG Metrics:335 - Added sensor with name node--1.bytes-received
2017-06-12 17:03:37 DEBUG Metrics:335 - Added sensor with name node--1.latency
2017-06-12 17:03:37 DEBUG Selector:339 - Created socket with SO_RCVBUF = 65536, SO_SNDBUF = 131072, SO_TIMEOUT = 0 to node -1
2017-06-12 17:03:37 DEBUG NetworkClient:590 - Completed connection to node -1.  Fetching API versions.
2017-06-12 17:03:37 DEBUG NetworkClient:603 - Initiating API versions fetch from node -1.
2017-06-12 17:03:37 DEBUG NetworkClient:558 - Recorded API versions for node -1: (Produce(0): 0 to 2 [usable: 2], Fetch(1): 0 to 3 [usable: 3], Offsets(2): 0 to 1 [usable: 1], Metadata(3): 0 to 2 [usable: 2], LeaderAndIsr(4): 0 [usable: 0], StopReplica(5): 0 [usable: 0], UpdateMetadata(6): 0 to 3 [usable: 3], ControlledShutdown(7): 1 [usable: 1], OffsetCommit(8): 0 to 2 [usable: 2], OffsetFetch(9): 0 to 2 [usable: 2], GroupCoordinator(10): 0 [usable: 0], JoinGroup(11): 0 to 1 [usable: 1], Heartbeat(12): 0 [usable: 0], LeaveGroup(13): 0 [usable: 0], SyncGroup(14): 0 [usable: 0], DescribeGroups(15): 0 [usable: 0], ListGroups(16): 0 [usable: 0], SaslHandshake(17): 0 [usable: 0], ApiVersions(18): 0 [usable: 0], CreateTopics(19): 0 to 1 [usable: 1], DeleteTopics(20): 0 [usable: 0])
2017-06-12 17:03:37 DEBUG NetworkClient:751 - Sending metadata request (type=MetadataRequest, topics=<ALL>) to node -1
2017-06-12 17:03:37 DEBUG Metadata:249 - Updated cluster metadata version 2 to Cluster(id = kbd_yf8GQXCf13cNvyCN1w, nodes = [tmon-jhkwon78-n.tmoncorp.com:9092 (id: 0 rack: null)], partitions = [Partition(topic = wordcount-lambda-example-Counts-repartition, partition = 0, leader = 0, replicas = [0], isr = [0]), Partition(topic = TextLinesTopic, partition = 0, leader = 0, replicas = [0], isr = [0])])
2017-06-12 17:03:37 DEBUG AbstractCoordinator:572 - Received GroupCoordinator response ClientResponse(receivedTimeMs=1497254617768, latencyMs=156, disconnected=false, requestHeader={api_key=10,api_version=0,correlation_id=0,client_id=wordcount-lambda-example-1488bda3-f635-4518-b604-d71e08791d1c-StreamThread-1-consumer}, responseBody={error_code=0,coordinator={node_id=0,host=tmon-jhkwon78-n.tmoncorp.com,port=9092}}) for group wordcount-lambda-example
2017-06-12 17:03:37 INFO  AbstractCoordinator:586 - Discovered coordinator tmon-jhkwon78-n.tmoncorp.com:9092 (id: 2147483647 rack: null) for group wordcount-lambda-example.
2017-06-12 17:03:37 DEBUG NetworkClient:627 - Initiating connection to node 2147483647 at tmon-jhkwon78-n.tmoncorp.com:9092.
2017-06-12 17:03:37 INFO  ConsumerCoordinator:397 - Revoking previously assigned partitions [] for group wordcount-lambda-example
2017-06-12 17:03:37 INFO  StreamThread:248 - stream-thread [StreamThread-1] at state RUNNING: partitions [] revoked at the beginning of consumer rebalance.
2017-06-12 17:03:37 INFO  StreamThread:163 - stream-thread [StreamThread-1] State transition from RUNNING to PARTITIONS_REVOKED.
2017-06-12 17:03:37 INFO  KafkaStreams:224 - stream-client [wordcount-lambda-example-1488bda3-f635-4518-b604-d71e08791d1c] State transition from RUNNING to REBALANCING.
2017-06-12 17:03:37 DEBUG AbstractCoordinator:889 - Heartbeat thread for group wordcount-lambda-example started
2017-06-12 17:03:37 DEBUG StreamThread:468 - stream-thread [StreamThread-1] suspendTasksAndState: suspending all active tasks [] and standby tasks []
2017-06-12 17:03:37 DEBUG KafkaConsumer:899 - Unsubscribed all topics or patterns and assigned partitions
2017-06-12 17:03:37 INFO  StreamThread:1042 - stream-thread [StreamThread-1] Updating suspended tasks to contain active tasks []
2017-06-12 17:03:37 INFO  StreamThread:1049 - stream-thread [StreamThread-1] Removing all active tasks []
2017-06-12 17:03:37 INFO  StreamThread:1064 - stream-thread [StreamThread-1] Removing all standby tasks []
2017-06-12 17:03:37 INFO  AbstractCoordinator:420 - (Re-)joining group wordcount-lambda-example
2017-06-12 17:03:37 DEBUG StreamPartitionAssignor:248 - stream-thread [StreamThread-1] found [wordcount-lambda-example-Counts-repartition, TextLinesTopic] topics possibly matching regex
2017-06-12 17:03:37 DEBUG TopologyBuilder:1381 - stream-thread [StreamThread-1] updating builder with SubscriptionUpdates{updatedTopicSubscriptions=[wordcount-lambda-example-Counts-repartition, TextLinesTopic]} topic(s) with possible matching regex subscription(s)
2017-06-12 17:03:37 DEBUG AbstractCoordinator:428 - Sending JoinGroup ((type: JoinGroupRequest, groupId=wordcount-lambda-example, sessionTimeout=10000, rebalanceTimeout=2147483647, memberId=, protocolType=consumer, groupProtocols=org.apache.kafka.common.requests.JoinGroupRequest$ProtocolMetadata@20bc76ff)) to coordinator tmon-jhkwon78-n.tmoncorp.com:9092 (id: 2147483647 rack: null)
2017-06-12 17:03:37 DEBUG Metrics:335 - Added sensor with name node-2147483647.bytes-sent
2017-06-12 17:03:37 DEBUG Metrics:335 - Added sensor with name node-2147483647.bytes-received
2017-06-12 17:03:37 DEBUG Metrics:335 - Added sensor with name node-2147483647.latency
2017-06-12 17:03:37 DEBUG Selector:339 - Created socket with SO_RCVBUF = 65536, SO_SNDBUF = 131072, SO_TIMEOUT = 0 to node 2147483647
2017-06-12 17:03:37 DEBUG NetworkClient:590 - Completed connection to node 2147483647.  Fetching API versions.
2017-06-12 17:03:37 DEBUG NetworkClient:603 - Initiating API versions fetch from node 2147483647.
2017-06-12 17:03:37 DEBUG NetworkClient:558 - Recorded API versions for node 2147483647: (Produce(0): 0 to 2 [usable: 2], Fetch(1): 0 to 3 [usable: 3], Offsets(2): 0 to 1 [usable: 1], Metadata(3): 0 to 2 [usable: 2], LeaderAndIsr(4): 0 [usable: 0], StopReplica(5): 0 [usable: 0], UpdateMetadata(6): 0 to 3 [usable: 3], ControlledShutdown(7): 1 [usable: 1], OffsetCommit(8): 0 to 2 [usable: 2], OffsetFetch(9): 0 to 2 [usable: 2], GroupCoordinator(10): 0 [usable: 0], JoinGroup(11): 0 to 1 [usable: 1], Heartbeat(12): 0 [usable: 0], LeaveGroup(13): 0 [usable: 0], SyncGroup(14): 0 [usable: 0], DescribeGroups(15): 0 [usable: 0], ListGroups(16): 0 [usable: 0], SaslHandshake(17): 0 [usable: 0], ApiVersions(18): 0 [usable: 0], CreateTopics(19): 0 to 1 [usable: 1], DeleteTopics(20): 0 [usable: 0])
2017-06-12 17:03:37 DEBUG AbstractCoordinator:438 - Received successful JoinGroup response for group wordcount-lambda-example: {error_code=0,generation_id=33,group_protocol=stream,leader_id=wordcount-lambda-example-1488bda3-f635-4518-b604-d71e08791d1c-StreamThread-1-consumer-e3d9f09f-c1b6-47c6-8e7b-12d92803cdbb,member_id=wordcount-lambda-example-1488bda3-f635-4518-b604-d71e08791d1c-StreamThread-1-consumer-e3d9f09f-c1b6-47c6-8e7b-12d92803cdbb,members=[{member_id=wordcount-lambda-example-1488bda3-f635-4518-b604-d71e08791d1c-StreamThread-1-consumer-e3d9f09f-c1b6-47c6-8e7b-12d92803cdbb,member_metadata=java.nio.HeapByteBuffer[pos=0 lim=103 cap=103]}]}
2017-06-12 17:03:37 DEBUG ConsumerCoordinator:340 - Performing assignment for group wordcount-lambda-example using strategy stream with subscriptions {wordcount-lambda-example-1488bda3-f635-4518-b604-d71e08791d1c-StreamThread-1-consumer-e3d9f09f-c1b6-47c6-8e7b-12d92803cdbb=Subscription(topics=[wordcount-lambda-example-Counts-repartition, TextLinesTopic])}
2017-06-12 17:03:37 INFO  StreamPartitionAssignor:300 - stream-thread [StreamThread-1] Constructed client metadata {1488bda3-f635-4518-b604-d71e08791d1c=ClientMetadata{hostInfo=null, consumers=[wordcount-lambda-example-1488bda3-f635-4518-b604-d71e08791d1c-StreamThread-1-consumer-e3d9f09f-c1b6-47c6-8e7b-12d92803cdbb], state=[activeTasks: ([]) assignedTasks: ([]) prevActiveTasks: ([]) prevAssignedTasks: ([]) capacity: 1.0 cost: 0.0]}} from the member subscriptions.
2017-06-12 17:03:37 DEBUG StreamPartitionAssignor:606 - stream-thread [StreamThread-1] Starting to validate internal topics in partition assignor.
2017-06-12 17:03:37 DEBUG Metadata:249 - Updated cluster metadata version 1 to Cluster(id = null, nodes = [localhost:9092 (id: -1 rack: null)], partitions = [])
2017-06-12 17:03:37 DEBUG NetworkClient:627 - Initiating connection to node -1 at localhost:9092.
2017-06-12 17:03:37 DEBUG Metrics:335 - Added sensor with name node--1.bytes-sent
2017-06-12 17:03:37 DEBUG Metrics:335 - Added sensor with name node--1.bytes-received
2017-06-12 17:03:37 DEBUG Metrics:335 - Added sensor with name node--1.latency
2017-06-12 17:03:37 DEBUG Selector:339 - Created socket with SO_RCVBUF = 32768, SO_SNDBUF = 131072, SO_TIMEOUT = 0 to node -1
2017-06-12 17:03:37 DEBUG NetworkClient:590 - Completed connection to node -1.  Fetching API versions.
2017-06-12 17:03:37 DEBUG NetworkClient:603 - Initiating API versions fetch from node -1.
2017-06-12 17:03:37 DEBUG NetworkClient:558 - Recorded API versions for node -1: (Produce(0): 0 to 2 [usable: 2], Fetch(1): 0 to 3 [usable: 3], Offsets(2): 0 to 1 [usable: 1], Metadata(3): 0 to 2 [usable: 2], LeaderAndIsr(4): 0 [usable: 0], StopReplica(5): 0 [usable: 0], UpdateMetadata(6): 0 to 3 [usable: 3], ControlledShutdown(7): 1 [usable: 1], OffsetCommit(8): 0 to 2 [usable: 2], OffsetFetch(9): 0 to 2 [usable: 2], GroupCoordinator(10): 0 [usable: 0], JoinGroup(11): 0 to 1 [usable: 1], Heartbeat(12): 0 [usable: 0], LeaveGroup(13): 0 [usable: 0], SyncGroup(14): 0 [usable: 0], DescribeGroups(15): 0 [usable: 0], ListGroups(16): 0 [usable: 0], SaslHandshake(17): 0 [usable: 0], ApiVersions(18): 0 [usable: 0], CreateTopics(19): 0 to 1 [usable: 1], DeleteTopics(20): 0 [usable: 0])
2017-06-12 17:03:37 DEBUG NetworkClient:627 - Initiating connection to node 0 at tmon-jhkwon78-n.tmoncorp.com:9092.
2017-06-12 17:03:37 DEBUG Metrics:335 - Added sensor with name node-0.bytes-sent
2017-06-12 17:03:37 DEBUG Metrics:335 - Added sensor with name node-0.bytes-received
2017-06-12 17:03:37 DEBUG Metrics:335 - Added sensor with name node-0.latency
2017-06-12 17:03:37 DEBUG Selector:339 - Created socket with SO_RCVBUF = 32768, SO_SNDBUF = 131072, SO_TIMEOUT = 0 to node 0
2017-06-12 17:03:37 DEBUG NetworkClient:590 - Completed connection to node 0.  Fetching API versions.
2017-06-12 17:03:37 DEBUG NetworkClient:603 - Initiating API versions fetch from node 0.
2017-06-12 17:03:37 DEBUG NetworkClient:558 - Recorded API versions for node 0: (Produce(0): 0 to 2 [usable: 2], Fetch(1): 0 to 3 [usable: 3], Offsets(2): 0 to 1 [usable: 1], Metadata(3): 0 to 2 [usable: 2], LeaderAndIsr(4): 0 [usable: 0], StopReplica(5): 0 [usable: 0], UpdateMetadata(6): 0 to 3 [usable: 3], ControlledShutdown(7): 1 [usable: 1], OffsetCommit(8): 0 to 2 [usable: 2], OffsetFetch(9): 0 to 2 [usable: 2], GroupCoordinator(10): 0 [usable: 0], JoinGroup(11): 0 to 1 [usable: 1], Heartbeat(12): 0 [usable: 0], LeaveGroup(13): 0 [usable: 0], SyncGroup(14): 0 [usable: 0], DescribeGroups(15): 0 [usable: 0], ListGroups(16): 0 [usable: 0], SaslHandshake(17): 0 [usable: 0], ApiVersions(18): 0 [usable: 0], CreateTopics(19): 0 to 1 [usable: 1], DeleteTopics(20): 0 [usable: 0])
2017-06-12 17:03:38 DEBUG Metadata:249 - Updated cluster metadata version 1 to Cluster(id = null, nodes = [localhost:9092 (id: -1 rack: null)], partitions = [])
2017-06-12 17:03:38 INFO  StreamPartitionAssignor:640 - stream-thread [StreamThread-1] Completed validating internal topics in partition assignor
2017-06-12 17:03:38 DEBUG StreamPartitionAssignor:386 - stream-thread [StreamThread-1] Created repartition topics [Partition(topic = wordcount-lambda-example-Counts-repartition, partition = 0, leader = none, replicas = [], isr = [])] from the parsed topology.
2017-06-12 17:03:38 DEBUG StreamPartitionAssignor:606 - stream-thread [StreamThread-1] Starting to validate internal topics in partition assignor.
2017-06-12 17:03:38 DEBUG Metadata:249 - Updated cluster metadata version 1 to Cluster(id = null, nodes = [localhost:9092 (id: -1 rack: null)], partitions = [])
2017-06-12 17:03:38 DEBUG Metadata:249 - Updated cluster metadata version 1 to Cluster(id = null, nodes = [localhost:9092 (id: -1 rack: null)], partitions = [])
2017-06-12 17:03:38 INFO  StreamPartitionAssignor:640 - stream-thread [StreamThread-1] Completed validating internal topics in partition assignor
2017-06-12 17:03:38 DEBUG StreamPartitionAssignor:461 - stream-thread [StreamThread-1] Created state changelog topics {wordcount-lambda-example-Counts-changelog=org.apache.kafka.streams.processor.internals.StreamPartitionAssignor$InternalTopicMetadata@788695fe} from the parsed topology.
2017-06-12 17:03:38 DEBUG StreamPartitionAssignor:471 - stream-thread [StreamThread-1] Assigning tasks [0_0, 1_0] to clients {1488bda3-f635-4518-b604-d71e08791d1c=[activeTasks: ([]) assignedTasks: ([]) prevActiveTasks: ([]) prevAssignedTasks: ([]) capacity: 1.0 cost: 0.0]} with number of replicas 0
2017-06-12 17:03:38 INFO  StreamPartitionAssignor:476 - stream-thread [StreamThread-1] Assigned tasks to clients as {1488bda3-f635-4518-b604-d71e08791d1c=[activeTasks: ([0_0, 1_0]) assignedTasks: ([0_0, 1_0]) prevActiveTasks: ([]) prevAssignedTasks: ([]) capacity: 1.0 cost: 1.0]}.
2017-06-12 17:03:38 DEBUG ConsumerCoordinator:379 - Finished assignment for group wordcount-lambda-example: {wordcount-lambda-example-1488bda3-f635-4518-b604-d71e08791d1c-StreamThread-1-consumer-e3d9f09f-c1b6-47c6-8e7b-12d92803cdbb=Assignment(partitions=[TextLinesTopic-0, wordcount-lambda-example-Counts-repartition-0])}
2017-06-12 17:03:38 DEBUG AbstractCoordinator:506 - Sending leader SyncGroup for group wordcount-lambda-example to coordinator tmon-jhkwon78-n.tmoncorp.com:9092 (id: 2147483647 rack: null): (type=SyncGroupRequest, groupId=wordcount-lambda-example, generationId=33, memberId=wordcount-lambda-example-1488bda3-f635-4518-b604-d71e08791d1c-StreamThread-1-consumer-e3d9f09f-c1b6-47c6-8e7b-12d92803cdbb, groupAssignment=wordcount-lambda-example-1488bda3-f635-4518-b604-d71e08791d1c-StreamThread-1-consumer-e3d9f09f-c1b6-47c6-8e7b-12d92803cdbb)
2017-06-12 17:03:38 INFO  AbstractCoordinator:388 - Successfully joined group wordcount-lambda-example with generation 33
2017-06-12 17:03:38 INFO  ConsumerCoordinator:256 - Setting newly assigned partitions [TextLinesTopic-0, wordcount-lambda-example-Counts-repartition-0] for group wordcount-lambda-example
2017-06-12 17:03:38 INFO  StreamThread:228 - stream-thread [StreamThread-1] at state PARTITIONS_REVOKED: new partitions [TextLinesTopic-0, wordcount-lambda-example-Counts-repartition-0] assigned at the end of consumer rebalance.
2017-06-12 17:03:38 INFO  StreamThread:163 - stream-thread [StreamThread-1] State transition from PARTITIONS_REVOKED to ASSIGNING_PARTITIONS.
2017-06-12 17:03:38 INFO  KafkaStreams:224 - stream-client [wordcount-lambda-example-1488bda3-f635-4518-b604-d71e08791d1c] State transition from REBALANCING to REBALANCING.
2017-06-12 17:03:38 DEBUG StreamThread:1236 - stream-thread [StreamThread-1] creating new task 0_0
2017-06-12 17:03:38 INFO  StreamThread:858 - stream-thread [StreamThread-1] Creating active task 0_0 with assigned partitions [TextLinesTopic-0]
2017-06-12 17:03:38 DEBUG Metrics:335 - Added sensor with name commit
2017-06-12 17:03:38 DEBUG Metrics:335 - Added sensor with name 0_0-commit
2017-06-12 17:03:38 INFO  StreamTask:140 - task [0_0] Initializing state stores
2017-06-12 17:03:38 DEBUG ConsumerCoordinator:804 - Group wordcount-lambda-example fetching committed offsets for partitions: [TextLinesTopic-0, wordcount-lambda-example-Counts-repartition-0]
2017-06-12 17:03:38 INFO  StreamTask:333 - task [0_0] Initializing processor nodes of the topology
2017-06-12 17:03:38 DEBUG Metrics:335 - Added sensor with name process
2017-06-12 17:03:38 DEBUG Metrics:335 - Added sensor with name task.0_0.KSTREAM-SOURCE-0000000000-process
2017-06-12 17:03:38 DEBUG Metrics:335 - Added sensor with name punctuate
2017-06-12 17:03:38 DEBUG Metrics:335 - Added sensor with name task.0_0.KSTREAM-SOURCE-0000000000-punctuate
2017-06-12 17:03:38 DEBUG Metrics:335 - Added sensor with name create
2017-06-12 17:03:38 DEBUG Metrics:335 - Added sensor with name task.0_0.KSTREAM-SOURCE-0000000000-create
2017-06-12 17:03:38 DEBUG Metrics:335 - Added sensor with name destroy
2017-06-12 17:03:38 DEBUG Metrics:335 - Added sensor with name task.0_0.KSTREAM-SOURCE-0000000000-destroy
2017-06-12 17:03:38 DEBUG Metrics:335 - Added sensor with name forward
2017-06-12 17:03:38 DEBUG Metrics:335 - Added sensor with name task.0_0.KSTREAM-SOURCE-0000000000-forward
2017-06-12 17:03:38 DEBUG Metrics:335 - Added sensor with name task.0_0.KSTREAM-FLATMAPVALUES-0000000001-process
2017-06-12 17:03:38 DEBUG Metrics:335 - Added sensor with name task.0_0.KSTREAM-FLATMAPVALUES-0000000001-punctuate
2017-06-12 17:03:38 DEBUG Metrics:335 - Added sensor with name task.0_0.KSTREAM-FLATMAPVALUES-0000000001-create
2017-06-12 17:03:38 DEBUG Metrics:335 - Added sensor with name task.0_0.KSTREAM-FLATMAPVALUES-0000000001-destroy
2017-06-12 17:03:38 DEBUG Metrics:335 - Added sensor with name task.0_0.KSTREAM-FLATMAPVALUES-0000000001-forward
2017-06-12 17:03:38 DEBUG Metrics:335 - Added sensor with name task.0_0.KSTREAM-KEY-SELECT-0000000002-process
2017-06-12 17:03:38 DEBUG Metrics:335 - Added sensor with name task.0_0.KSTREAM-KEY-SELECT-0000000002-punctuate
2017-06-12 17:03:38 DEBUG Metrics:335 - Added sensor with name task.0_0.KSTREAM-KEY-SELECT-0000000002-create
2017-06-12 17:03:38 DEBUG Metrics:335 - Added sensor with name task.0_0.KSTREAM-KEY-SELECT-0000000002-destroy
2017-06-12 17:03:38 DEBUG Metrics:335 - Added sensor with name task.0_0.KSTREAM-KEY-SELECT-0000000002-forward
2017-06-12 17:03:38 DEBUG Metrics:335 - Added sensor with name task.0_0.KSTREAM-FILTER-0000000005-process
2017-06-12 17:03:38 DEBUG Metrics:335 - Added sensor with name task.0_0.KSTREAM-FILTER-0000000005-punctuate
2017-06-12 17:03:38 DEBUG Metrics:335 - Added sensor with name task.0_0.KSTREAM-FILTER-0000000005-create
2017-06-12 17:03:38 DEBUG Metrics:335 - Added sensor with name task.0_0.KSTREAM-FILTER-0000000005-destroy
2017-06-12 17:03:38 DEBUG Metrics:335 - Added sensor with name task.0_0.KSTREAM-FILTER-0000000005-forward
2017-06-12 17:03:38 DEBUG Metrics:335 - Added sensor with name task.0_0.KSTREAM-SINK-0000000004-process
2017-06-12 17:03:38 DEBUG Metrics:335 - Added sensor with name task.0_0.KSTREAM-SINK-0000000004-punctuate
2017-06-12 17:03:38 DEBUG Metrics:335 - Added sensor with name task.0_0.KSTREAM-SINK-0000000004-create
2017-06-12 17:03:38 DEBUG Metrics:335 - Added sensor with name task.0_0.KSTREAM-SINK-0000000004-destroy
2017-06-12 17:03:38 DEBUG Metrics:335 - Added sensor with name task.0_0.KSTREAM-SINK-0000000004-forward
2017-06-12 17:03:38 DEBUG StreamThread:1236 - stream-thread [StreamThread-1] creating new task 1_0
2017-06-12 17:03:38 INFO  StreamThread:858 - stream-thread [StreamThread-1] Creating active task 1_0 with assigned partitions [wordcount-lambda-example-Counts-repartition-0]
2017-06-12 17:03:38 DEBUG Metrics:335 - Added sensor with name 1_0-commit
2017-06-12 17:03:38 INFO  StreamTask:140 - task [1_0] Initializing state stores
2017-06-12 17:03:38 DEBUG Metrics:335 - Added sensor with name put
2017-06-12 17:03:38 DEBUG Metrics:335 - Added sensor with name Counts-put
2017-06-12 17:03:38 DEBUG Metrics:335 - Added sensor with name put-if-absent
2017-06-12 17:03:38 DEBUG Metrics:335 - Added sensor with name Counts-put-if-absent
2017-06-12 17:03:38 DEBUG Metrics:335 - Added sensor with name get
2017-06-12 17:03:38 DEBUG Metrics:335 - Added sensor with name Counts-get
2017-06-12 17:03:38 DEBUG Metrics:335 - Added sensor with name delete
2017-06-12 17:03:38 DEBUG Metrics:335 - Added sensor with name Counts-delete
2017-06-12 17:03:38 DEBUG Metrics:335 - Added sensor with name put-all
2017-06-12 17:03:38 DEBUG Metrics:335 - Added sensor with name Counts-put-all
2017-06-12 17:03:38 DEBUG Metrics:335 - Added sensor with name all
2017-06-12 17:03:38 DEBUG Metrics:335 - Added sensor with name Counts-all
2017-06-12 17:03:38 DEBUG Metrics:335 - Added sensor with name range
2017-06-12 17:03:38 DEBUG Metrics:335 - Added sensor with name Counts-range
2017-06-12 17:03:38 DEBUG Metrics:335 - Added sensor with name flush
2017-06-12 17:03:38 DEBUG Metrics:335 - Added sensor with name Counts-flush
2017-06-12 17:03:38 DEBUG Metrics:335 - Added sensor with name restore
2017-06-12 17:03:38 DEBUG Metrics:335 - Added sensor with name Counts-restore
2017-06-12 17:03:39 DEBUG ProcessorStateManager:139 - task [1_0] Registering state store Counts to its state manager
2017-06-12 17:03:39 DEBUG NetworkClient:627 - Initiating connection to node -1 at localhost:9092.
2017-06-12 17:03:39 DEBUG Metrics:335 - Added sensor with name node--1.bytes-sent
2017-06-12 17:03:39 DEBUG Metrics:335 - Added sensor with name node--1.bytes-received
2017-06-12 17:03:39 DEBUG Metrics:335 - Added sensor with name node--1.latency
2017-06-12 17:03:39 DEBUG Selector:339 - Created socket with SO_RCVBUF = 65536, SO_SNDBUF = 131072, SO_TIMEOUT = 0 to node -1
2017-06-12 17:03:39 DEBUG NetworkClient:590 - Completed connection to node -1.  Fetching API versions.
2017-06-12 17:03:39 DEBUG NetworkClient:603 - Initiating API versions fetch from node -1.
2017-06-12 17:03:39 DEBUG NetworkClient:558 - Recorded API versions for node -1: (Produce(0): 0 to 2 [usable: 2], Fetch(1): 0 to 3 [usable: 3], Offsets(2): 0 to 1 [usable: 1], Metadata(3): 0 to 2 [usable: 2], LeaderAndIsr(4): 0 [usable: 0], StopReplica(5): 0 [usable: 0], UpdateMetadata(6): 0 to 3 [usable: 3], ControlledShutdown(7): 1 [usable: 1], OffsetCommit(8): 0 to 2 [usable: 2], OffsetFetch(9): 0 to 2 [usable: 2], GroupCoordinator(10): 0 [usable: 0], JoinGroup(11): 0 to 1 [usable: 1], Heartbeat(12): 0 [usable: 0], LeaveGroup(13): 0 [usable: 0], SyncGroup(14): 0 [usable: 0], DescribeGroups(15): 0 [usable: 0], ListGroups(16): 0 [usable: 0], SaslHandshake(17): 0 [usable: 0], ApiVersions(18): 0 [usable: 0], CreateTopics(19): 0 to 1 [usable: 1], DeleteTopics(20): 0 [usable: 0])
2017-06-12 17:03:39 DEBUG NetworkClient:751 - Sending metadata request (type=MetadataRequest, topics=) to node -1
2017-06-12 17:03:39 DEBUG Metadata:249 - Updated cluster metadata version 2 to Cluster(id = kbd_yf8GQXCf13cNvyCN1w, nodes = [tmon-jhkwon78-n.tmoncorp.com:9092 (id: 0 rack: null)], partitions = [])
2017-06-12 17:03:39 DEBUG KafkaConsumer:944 - Subscribed to partition(s): wordcount-lambda-example-Counts-changelog-0
2017-06-12 17:03:39 DEBUG KafkaConsumer:1234 - Seeking to end of partition wordcount-lambda-example-Counts-changelog-0
2017-06-12 17:03:39 DEBUG Fetcher:363 - Resetting offset for partition wordcount-lambda-example-Counts-changelog-0 to latest offset.
2017-06-12 17:03:39 DEBUG Fetcher:566 - Partition wordcount-lambda-example-Counts-changelog-0 is unknown for fetching offset, wait for metadata refresh
2017-06-12 17:03:39 DEBUG NetworkClient:767 - Initialize connection to node 0 for sending metadata request
2017-06-12 17:03:39 DEBUG NetworkClient:627 - Initiating connection to node 0 at tmon-jhkwon78-n.tmoncorp.com:9092.
2017-06-12 17:03:39 DEBUG Metrics:335 - Added sensor with name node-0.bytes-sent
2017-06-12 17:03:39 DEBUG Metrics:335 - Added sensor with name node-0.bytes-received
2017-06-12 17:03:39 DEBUG Metrics:335 - Added sensor with name node-0.latency
2017-06-12 17:03:39 DEBUG Selector:339 - Created socket with SO_RCVBUF = 65536, SO_SNDBUF = 131072, SO_TIMEOUT = 0 to node 0
2017-06-12 17:03:39 DEBUG NetworkClient:590 - Completed connection to node 0.  Fetching API versions.
2017-06-12 17:03:39 DEBUG NetworkClient:603 - Initiating API versions fetch from node 0.
2017-06-12 17:03:39 DEBUG NetworkClient:558 - Recorded API versions for node 0: (Produce(0): 0 to 2 [usable: 2], Fetch(1): 0 to 3 [usable: 3], Offsets(2): 0 to 1 [usable: 1], Metadata(3): 0 to 2 [usable: 2], LeaderAndIsr(4): 0 [usable: 0], StopReplica(5): 0 [usable: 0], UpdateMetadata(6): 0 to 3 [usable: 3], ControlledShutdown(7): 1 [usable: 1], OffsetCommit(8): 0 to 2 [usable: 2], OffsetFetch(9): 0 to 2 [usable: 2], GroupCoordinator(10): 0 [usable: 0], JoinGroup(11): 0 to 1 [usable: 1], Heartbeat(12): 0 [usable: 0], LeaveGroup(13): 0 [usable: 0], SyncGroup(14): 0 [usable: 0], DescribeGroups(15): 0 [usable: 0], ListGroups(16): 0 [usable: 0], SaslHandshake(17): 0 [usable: 0], ApiVersions(18): 0 [usable: 0], CreateTopics(19): 0 to 1 [usable: 1], DeleteTopics(20): 0 [usable: 0])
2017-06-12 17:03:39 DEBUG NetworkClient:751 - Sending metadata request (type=MetadataRequest, topics=wordcount-lambda-example-Counts-changelog) to node 0
2017-06-12 17:03:39 DEBUG Metadata:249 - Updated cluster metadata version 3 to Cluster(id = kbd_yf8GQXCf13cNvyCN1w, nodes = [tmon-jhkwon78-n.tmoncorp.com:9092 (id: 0 rack: null)], partitions = [Partition(topic = wordcount-lambda-example-Counts-changelog, partition = 0, leader = 0, replicas = [0], isr = [0])])
2017-06-12 17:03:39 DEBUG Fetcher:675 - Handling ListOffsetResponse response for wordcount-lambda-example-Counts-changelog-0. Fetched offset 50, timestamp -1
2017-06-12 17:03:39 DEBUG KafkaConsumer:1216 - Seeking to beginning of partition wordcount-lambda-example-Counts-changelog-0
2017-06-12 17:03:39 DEBUG Fetcher:363 - Resetting offset for partition wordcount-lambda-example-Counts-changelog-0 to earliest offset.
2017-06-12 17:03:39 DEBUG Fetcher:675 - Handling ListOffsetResponse response for wordcount-lambda-example-Counts-changelog-0. Fetched offset 0, timestamp -1
2017-06-12 17:03:39 DEBUG ProcessorStateManager:230 - restoring partition wordcount-lambda-example-Counts-changelog-0 from offset 0 to endOffset 50
2017-06-12 17:03:39 DEBUG Fetcher:181 - Sending fetch for partitions [wordcount-lambda-example-Counts-changelog-0] to broker tmon-jhkwon78-n.tmoncorp.com:9092 (id: 0 rack: null)
2017-06-12 17:03:39 DEBUG Metrics:335 - Added sensor with name topic.wordcount-lambda-example-Counts-changelog.bytes-fetched
2017-06-12 17:03:39 DEBUG Metrics:335 - Added sensor with name topic.wordcount-lambda-example-Counts-changelog.records-fetched
2017-06-12 17:03:39 DEBUG Metrics:335 - Added sensor with name wordcount-lambda-example-Counts-changelog-0.records-lag
2017-06-12 17:03:39 DEBUG Fetcher:539 - Ignoring fetched records for wordcount-lambda-example-Counts-changelog-0 at offset 0 since the current position is 50
2017-06-12 17:03:39 DEBUG Fetcher:181 - Sending fetch for partitions [wordcount-lambda-example-Counts-changelog-0] to broker tmon-jhkwon78-n.tmoncorp.com:9092 (id: 0 rack: null)
2017-06-12 17:03:39 DEBUG KafkaConsumer:899 - Unsubscribed all topics or patterns and assigned partitions
2017-06-12 17:03:39 DEBUG Metrics:368 - Removed sensor with name wordcount-lambda-example-Counts-changelog-0.records-lag
2017-06-12 17:03:39 DEBUG Metrics:335 - Added sensor with name 1_0-Counts-hitRatio
2017-06-12 17:03:39 INFO  StreamTask:333 - task [1_0] Initializing processor nodes of the topology
2017-06-12 17:03:39 DEBUG Metrics:335 - Added sensor with name task.1_0.KSTREAM-SOURCE-0000000006-process
2017-06-12 17:03:39 DEBUG Metrics:335 - Added sensor with name task.1_0.KSTREAM-SOURCE-0000000006-punctuate
2017-06-12 17:03:39 DEBUG Metrics:335 - Added sensor with name task.1_0.KSTREAM-SOURCE-0000000006-create
2017-06-12 17:03:39 DEBUG Metrics:335 - Added sensor with name task.1_0.KSTREAM-SOURCE-0000000006-destroy
2017-06-12 17:03:39 DEBUG Metrics:335 - Added sensor with name task.1_0.KSTREAM-SOURCE-0000000006-forward
2017-06-12 17:03:39 DEBUG Metrics:335 - Added sensor with name task.1_0.KSTREAM-AGGREGATE-0000000003-process
2017-06-12 17:03:39 DEBUG Metrics:335 - Added sensor with name task.1_0.KSTREAM-AGGREGATE-0000000003-punctuate
2017-06-12 17:03:39 DEBUG Metrics:335 - Added sensor with name task.1_0.KSTREAM-AGGREGATE-0000000003-create
2017-06-12 17:03:39 DEBUG Metrics:335 - Added sensor with name task.1_0.KSTREAM-AGGREGATE-0000000003-destroy
2017-06-12 17:03:39 DEBUG Metrics:335 - Added sensor with name task.1_0.KSTREAM-AGGREGATE-0000000003-forward
2017-06-12 17:03:39 DEBUG Metrics:335 - Added sensor with name task.1_0.KTABLE-TOSTREAM-0000000007-process
2017-06-12 17:03:39 DEBUG Metrics:335 - Added sensor with name task.1_0.KTABLE-TOSTREAM-0000000007-punctuate
2017-06-12 17:03:39 DEBUG Metrics:335 - Added sensor with name task.1_0.KTABLE-TOSTREAM-0000000007-create
2017-06-12 17:03:39 DEBUG Metrics:335 - Added sensor with name task.1_0.KTABLE-TOSTREAM-0000000007-destroy
2017-06-12 17:03:39 DEBUG Metrics:335 - Added sensor with name task.1_0.KTABLE-TOSTREAM-0000000007-forward
2017-06-12 17:03:39 DEBUG Metrics:335 - Added sensor with name task.1_0.KSTREAM-SINK-0000000008-process
2017-06-12 17:03:39 DEBUG Metrics:335 - Added sensor with name task.1_0.KSTREAM-SINK-0000000008-punctuate
2017-06-12 17:03:39 DEBUG Metrics:335 - Added sensor with name task.1_0.KSTREAM-SINK-0000000008-create
2017-06-12 17:03:39 DEBUG Metrics:335 - Added sensor with name task.1_0.KSTREAM-SINK-0000000008-destroy
2017-06-12 17:03:39 DEBUG Metrics:335 - Added sensor with name task.1_0.KSTREAM-SINK-0000000008-forward
2017-06-12 17:03:39 DEBUG KafkaConsumer:899 - Unsubscribed all topics or patterns and assigned partitions
2017-06-12 17:03:39 INFO  StreamThread:163 - stream-thread [StreamThread-1] State transition from ASSIGNING_PARTITIONS to RUNNING.
2017-06-12 17:03:39 INFO  KafkaStreams:224 - stream-client [wordcount-lambda-example-1488bda3-f635-4518-b604-d71e08791d1c] State transition from REBALANCING to RUNNING.
2017-06-12 17:03:39 DEBUG Fetcher:251 - Resetting offset for partition TextLinesTopic-0 to the committed offset 43
2017-06-12 17:03:39 DEBUG Fetcher:251 - Resetting offset for partition wordcount-lambda-example-Counts-repartition-0 to the committed offset 196
2017-06-12 17:03:39 DEBUG Fetcher:181 - Sending fetch for partitions [wordcount-lambda-example-Counts-repartition-0, TextLinesTopic-0] to broker tmon-jhkwon78-n.tmoncorp.com:9092 (id: 0 rack: null)
2017-06-12 17:03:39 DEBUG NetworkClient:627 - Initiating connection to node 0 at tmon-jhkwon78-n.tmoncorp.com:9092.
2017-06-12 17:03:39 DEBUG Metrics:335 - Added sensor with name node-0.bytes-sent
2017-06-12 17:03:39 DEBUG Metrics:335 - Added sensor with name node-0.bytes-received
2017-06-12 17:03:39 DEBUG Metrics:335 - Added sensor with name node-0.latency
2017-06-12 17:03:39 DEBUG Selector:339 - Created socket with SO_RCVBUF = 65536, SO_SNDBUF = 131072, SO_TIMEOUT = 0 to node 0
2017-06-12 17:03:39 DEBUG NetworkClient:590 - Completed connection to node 0.  Fetching API versions.
2017-06-12 17:03:39 DEBUG NetworkClient:603 - Initiating API versions fetch from node 0.
2017-06-12 17:03:39 DEBUG NetworkClient:558 - Recorded API versions for node 0: (Produce(0): 0 to 2 [usable: 2], Fetch(1): 0 to 3 [usable: 3], Offsets(2): 0 to 1 [usable: 1], Metadata(3): 0 to 2 [usable: 2], LeaderAndIsr(4): 0 [usable: 0], StopReplica(5): 0 [usable: 0], UpdateMetadata(6): 0 to 3 [usable: 3], ControlledShutdown(7): 1 [usable: 1], OffsetCommit(8): 0 to 2 [usable: 2], OffsetFetch(9): 0 to 2 [usable: 2], GroupCoordinator(10): 0 [usable: 0], JoinGroup(11): 0 to 1 [usable: 1], Heartbeat(12): 0 [usable: 0], LeaveGroup(13): 0 [usable: 0], SyncGroup(14): 0 [usable: 0], DescribeGroups(15): 0 [usable: 0], ListGroups(16): 0 [usable: 0], SaslHandshake(17): 0 [usable: 0], ApiVersions(18): 0 [usable: 0], CreateTopics(19): 0 to 1 [usable: 1], DeleteTopics(20): 0 [usable: 0])
2017-06-12 17:03:40 DEBUG Metrics:335 - Added sensor with name wordcount-lambda-example-Counts-repartition-0.records-lag
2017-06-12 17:03:40 DEBUG Metrics:335 - Added sensor with name topic.wordcount-lambda-example-Counts-repartition.bytes-fetched
2017-06-12 17:03:40 DEBUG Metrics:335 - Added sensor with name topic.wordcount-lambda-example-Counts-repartition.records-fetched
2017-06-12 17:03:40 DEBUG Metrics:335 - Added sensor with name topic.TextLinesTopic.bytes-fetched
2017-06-12 17:03:40 DEBUG Metrics:335 - Added sensor with name topic.TextLinesTopic.records-fetched
2017-06-12 17:03:40 DEBUG Metrics:335 - Added sensor with name TextLinesTopic-0.records-lag
2017-06-12 17:03:40 DEBUG Fetcher:181 - Sending fetch for partitions [wordcount-lambda-example-Counts-repartition-0, TextLinesTopic-0] to broker tmon-jhkwon78-n.tmoncorp.com:9092 (id: 0 rack: null)
2017-06-12 17:03:40 DEBUG Fetcher:181 - Sending fetch for partitions [wordcount-lambda-example-Counts-repartition-0, TextLinesTopic-0] to broker tmon-jhkwon78-n.tmoncorp.com:9092 (id: 0 rack: null)
2017-06-12 17:03:41 DEBUG Fetcher:181 - Sending fetch for partitions [wordcount-lambda-example-Counts-repartition-0, TextLinesTopic-0] to broker tmon-jhkwon78-n.tmoncorp.com:9092 (id: 0 rack: null)
2017-06-12 17:03:41 DEBUG AbstractCoordinator:724 - Sending Heartbeat request for group wordcount-lambda-example to coordinator tmon-jhkwon78-n.tmoncorp.com:9092 (id: 2147483647 rack: null)
2017-06-12 17:03:41 DEBUG AbstractCoordinator:737 - Received successful Heartbeat response for group wordcount-lambda-example
2017-06-12 17:03:41 DEBUG Fetcher:181 - Sending fetch for partitions [wordcount-lambda-example-Counts-repartition-0, TextLinesTopic-0] to broker tmon-jhkwon78-n.tmoncorp.com:9092 (id: 0 rack: null)
2017-06-12 17:03:42 DEBUG Fetcher:181 - Sending fetch for partitions [wordcount-lambda-example-Counts-repartition-0, TextLinesTopic-0] to broker tmon-jhkwon78-n.tmoncorp.com:9092 (id: 0 rack: null)
2017-06-12 17:03:42 DEBUG Fetcher:181 - Sending fetch for partitions [wordcount-lambda-example-Counts-repartition-0, TextLinesTopic-0] to broker tmon-jhkwon78-n.tmoncorp.com:9092 (id: 0 rack: null)
2017-06-12 17:03:43 DEBUG Fetcher:181 - Sending fetch for partitions [wordcount-lambda-example-Counts-repartition-0, TextLinesTopic-0] to broker tmon-jhkwon78-n.tmoncorp.com:9092 (id: 0 rack: null)
2017-06-12 17:03:45 INFO  ConsumerConfig:180 - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.LongDeserializer

2017-06-12 17:03:45 DEBUG KafkaConsumer:607 - Starting the Kafka consumer
2017-06-12 17:03:45 DEBUG Metadata:249 - Updated cluster metadata version 1 to Cluster(id = null, nodes = [localhost:9092 (id: -1 rack: null)], partitions = [])
2017-06-12 17:03:46 DEBUG AbstractCoordinator:724 - Sending Heartbeat request for group wordcount-lambda-example to coordinator tmon-jhkwon78-n.tmoncorp.com:9092 (id: 2147483647 rack: null)
2017-06-12 17:03:46 DEBUG Fetcher:181 - Sending fetch for partitions [wordcount-lambda-example-Counts-repartition-0, TextLinesTopic-0] to broker tmon-jhkwon78-n.tmoncorp.com:9092 (id: 0 rack: null)
2017-06-12 17:03:46 DEBUG Metrics:335 - Added sensor with name connections-closed:
2017-06-12 17:03:46 DEBUG Metrics:335 - Added sensor with name connections-created:
2017-06-12 17:03:46 DEBUG Metrics:335 - Added sensor with name bytes-sent-received:
2017-06-12 17:03:46 DEBUG Metrics:335 - Added sensor with name bytes-sent:
2017-06-12 17:03:46 DEBUG Metrics:335 - Added sensor with name bytes-received:
2017-06-12 17:03:46 DEBUG Metrics:335 - Added sensor with name select-time:
2017-06-12 17:03:46 DEBUG Metrics:335 - Added sensor with name io-time:
2017-06-12 17:03:46 DEBUG Metrics:335 - Added sensor with name heartbeat-latency
2017-06-12 17:03:46 DEBUG Metrics:335 - Added sensor with name join-latency
2017-06-12 17:03:46 DEBUG Metrics:335 - Added sensor with name sync-latency
2017-06-12 17:03:46 DEBUG Metrics:335 - Added sensor with name commit-latency
2017-06-12 17:03:46 DEBUG Metrics:335 - Added sensor with name bytes-fetched
2017-06-12 17:03:46 DEBUG Metrics:335 - Added sensor with name records-fetched
2017-06-12 17:03:46 DEBUG Metrics:335 - Added sensor with name fetch-latency
2017-06-12 17:03:46 DEBUG Metrics:335 - Added sensor with name records-lag
2017-06-12 17:03:46 DEBUG Metrics:335 - Added sensor with name fetch-throttle-time
2017-06-12 17:03:46 INFO  AppInfoParser:83 - Kafka version : 0.10.2.1
2017-06-12 17:03:46 INFO  AppInfoParser:84 - Kafka commitId : e89bffd6b2eff799
2017-06-12 17:03:46 DEBUG KafkaConsumer:711 - Kafka consumer created
2017-06-12 17:03:46 DEBUG KafkaConsumer:824 - Subscribed to topic(s): WordsWithCountsTopic
2017-06-12 17:03:46 DEBUG AbstractCoordinator:561 - Sending GroupCoordinator request for group test to broker localhost:9092 (id: -1 rack: null)
2017-06-12 17:03:46 DEBUG NetworkClient:627 - Initiating connection to node -1 at localhost:9092.
2017-06-12 17:03:46 DEBUG AbstractCoordinator:737 - Received successful Heartbeat response for group wordcount-lambda-example
2017-06-12 17:03:46 DEBUG Metrics:335 - Added sensor with name node--1.bytes-sent
2017-06-12 17:03:46 DEBUG Metrics:335 - Added sensor with name node--1.bytes-received
2017-06-12 17:03:46 DEBUG Metrics:335 - Added sensor with name node--1.latency
2017-06-12 17:03:46 DEBUG Selector:339 - Created socket with SO_RCVBUF = 65536, SO_SNDBUF = 131072, SO_TIMEOUT = 0 to node -1
2017-06-12 17:03:46 DEBUG NetworkClient:590 - Completed connection to node -1.  Fetching API versions.
2017-06-12 17:03:46 DEBUG NetworkClient:603 - Initiating API versions fetch from node -1.
2017-06-12 17:03:46 DEBUG NetworkClient:558 - Recorded API versions for node -1: (Produce(0): 0 to 2 [usable: 2], Fetch(1): 0 to 3 [usable: 3], Offsets(2): 0 to 1 [usable: 1], Metadata(3): 0 to 2 [usable: 2], LeaderAndIsr(4): 0 [usable: 0], StopReplica(5): 0 [usable: 0], UpdateMetadata(6): 0 to 3 [usable: 3], ControlledShutdown(7): 1 [usable: 1], OffsetCommit(8): 0 to 2 [usable: 2], OffsetFetch(9): 0 to 2 [usable: 2], GroupCoordinator(10): 0 [usable: 0], JoinGroup(11): 0 to 1 [usable: 1], Heartbeat(12): 0 [usable: 0], LeaveGroup(13): 0 [usable: 0], SyncGroup(14): 0 [usable: 0], DescribeGroups(15): 0 [usable: 0], ListGroups(16): 0 [usable: 0], SaslHandshake(17): 0 [usable: 0], ApiVersions(18): 0 [usable: 0], CreateTopics(19): 0 to 1 [usable: 1], DeleteTopics(20): 0 [usable: 0])
2017-06-12 17:03:46 DEBUG NetworkClient:751 - Sending metadata request (type=MetadataRequest, topics=WordsWithCountsTopic) to node -1
2017-06-12 17:03:46 DEBUG Metadata:249 - Updated cluster metadata version 2 to Cluster(id = kbd_yf8GQXCf13cNvyCN1w, nodes = [tmon-jhkwon78-n.tmoncorp.com:9092 (id: 0 rack: null)], partitions = [Partition(topic = WordsWithCountsTopic, partition = 0, leader = 0, replicas = [0], isr = [0])])
2017-06-12 17:03:46 DEBUG AbstractCoordinator:572 - Received GroupCoordinator response ClientResponse(receivedTimeMs=1497254626031, latencyMs=20, disconnected=false, requestHeader={api_key=10,api_version=0,correlation_id=0,client_id=consumer-1}, responseBody={error_code=0,coordinator={node_id=0,host=tmon-jhkwon78-n.tmoncorp.com,port=9092}}) for group test
2017-06-12 17:03:46 INFO  AbstractCoordinator:586 - Discovered coordinator tmon-jhkwon78-n.tmoncorp.com:9092 (id: 2147483647 rack: null) for group test.
2017-06-12 17:03:46 DEBUG NetworkClient:627 - Initiating connection to node 2147483647 at tmon-jhkwon78-n.tmoncorp.com:9092.
2017-06-12 17:03:46 DEBUG ConsumerCoordinator:641 - Sending synchronous auto-commit of offsets {} for group test
2017-06-12 17:03:46 DEBUG AbstractCoordinator:889 - Heartbeat thread for group test started
2017-06-12 17:03:46 INFO  ConsumerCoordinator:397 - Revoking previously assigned partitions [] for group test
2017-06-12 17:03:46 INFO  AbstractCoordinator:420 - (Re-)joining group test
2017-06-12 17:03:46 DEBUG AbstractCoordinator:428 - Sending JoinGroup ((type: JoinGroupRequest, groupId=test, sessionTimeout=10000, rebalanceTimeout=300000, memberId=, protocolType=consumer, groupProtocols=org.apache.kafka.common.requests.JoinGroupRequest$ProtocolMetadata@2b9d0cec)) to coordinator tmon-jhkwon78-n.tmoncorp.com:9092 (id: 2147483647 rack: null)
2017-06-12 17:03:46 DEBUG Metrics:335 - Added sensor with name node-2147483647.bytes-sent
2017-06-12 17:03:46 DEBUG Metrics:335 - Added sensor with name node-2147483647.bytes-received
2017-06-12 17:03:46 DEBUG Metrics:335 - Added sensor with name node-2147483647.latency
2017-06-12 17:03:46 DEBUG Selector:339 - Created socket with SO_RCVBUF = 65536, SO_SNDBUF = 131072, SO_TIMEOUT = 0 to node 2147483647
2017-06-12 17:03:46 DEBUG NetworkClient:590 - Completed connection to node 2147483647.  Fetching API versions.
2017-06-12 17:03:46 DEBUG NetworkClient:603 - Initiating API versions fetch from node 2147483647.
2017-06-12 17:03:46 DEBUG NetworkClient:558 - Recorded API versions for node 2147483647: (Produce(0): 0 to 2 [usable: 2], Fetch(1): 0 to 3 [usable: 3], Offsets(2): 0 to 1 [usable: 1], Metadata(3): 0 to 2 [usable: 2], LeaderAndIsr(4): 0 [usable: 0], StopReplica(5): 0 [usable: 0], UpdateMetadata(6): 0 to 3 [usable: 3], ControlledShutdown(7): 1 [usable: 1], OffsetCommit(8): 0 to 2 [usable: 2], OffsetFetch(9): 0 to 2 [usable: 2], GroupCoordinator(10): 0 [usable: 0], JoinGroup(11): 0 to 1 [usable: 1], Heartbeat(12): 0 [usable: 0], LeaveGroup(13): 0 [usable: 0], SyncGroup(14): 0 [usable: 0], DescribeGroups(15): 0 [usable: 0], ListGroups(16): 0 [usable: 0], SaslHandshake(17): 0 [usable: 0], ApiVersions(18): 0 [usable: 0], CreateTopics(19): 0 to 1 [usable: 1], DeleteTopics(20): 0 [usable: 0])
2017-06-12 17:03:46 DEBUG AbstractCoordinator:438 - Received successful JoinGroup response for group test: {error_code=0,generation_id=44,group_protocol=range,leader_id=consumer-1-4e10cce2-f457-45cd-b4e0-e241a3610f1c,member_id=consumer-1-4e10cce2-f457-45cd-b4e0-e241a3610f1c,members=[{member_id=consumer-1-4e10cce2-f457-45cd-b4e0-e241a3610f1c,member_metadata=java.nio.HeapByteBuffer[pos=0 lim=32 cap=32]}]}
2017-06-12 17:03:46 DEBUG ConsumerCoordinator:340 - Performing assignment for group test using strategy range with subscriptions {consumer-1-4e10cce2-f457-45cd-b4e0-e241a3610f1c=Subscription(topics=[WordsWithCountsTopic])}
2017-06-12 17:03:46 DEBUG ConsumerCoordinator:379 - Finished assignment for group test: {consumer-1-4e10cce2-f457-45cd-b4e0-e241a3610f1c=Assignment(partitions=[WordsWithCountsTopic-0])}
2017-06-12 17:03:46 DEBUG AbstractCoordinator:506 - Sending leader SyncGroup for group test to coordinator tmon-jhkwon78-n.tmoncorp.com:9092 (id: 2147483647 rack: null): (type=SyncGroupRequest, groupId=test, generationId=44, memberId=consumer-1-4e10cce2-f457-45cd-b4e0-e241a3610f1c, groupAssignment=consumer-1-4e10cce2-f457-45cd-b4e0-e241a3610f1c)
2017-06-12 17:03:46 INFO  AbstractCoordinator:388 - Successfully joined group test with generation 44
2017-06-12 17:03:46 INFO  ConsumerCoordinator:256 - Setting newly assigned partitions [WordsWithCountsTopic-0] for group test
2017-06-12 17:03:46 DEBUG ConsumerCoordinator:804 - Group test fetching committed offsets for partitions: [WordsWithCountsTopic-0]
2017-06-12 17:03:46 DEBUG Fetcher:251 - Resetting offset for partition WordsWithCountsTopic-0 to the committed offset 50
2017-06-12 17:03:46 DEBUG Fetcher:181 - Sending fetch for partitions [WordsWithCountsTopic-0] to broker tmon-jhkwon78-n.tmoncorp.com:9092 (id: 0 rack: null)
2017-06-12 17:03:46 DEBUG NetworkClient:627 - Initiating connection to node 0 at tmon-jhkwon78-n.tmoncorp.com:9092.
2017-06-12 17:03:46 DEBUG Metrics:335 - Added sensor with name node-0.bytes-sent
2017-06-12 17:03:46 DEBUG Metrics:335 - Added sensor with name node-0.bytes-received
2017-06-12 17:03:46 DEBUG Metrics:335 - Added sensor with name node-0.latency
2017-06-12 17:03:46 DEBUG Selector:339 - Created socket with SO_RCVBUF = 65536, SO_SNDBUF = 131072, SO_TIMEOUT = 0 to node 0
2017-06-12 17:03:46 DEBUG NetworkClient:590 - Completed connection to node 0.  Fetching API versions.
2017-06-12 17:03:46 DEBUG NetworkClient:603 - Initiating API versions fetch from node 0.
2017-06-12 17:03:46 DEBUG NetworkClient:558 - Recorded API versions for node 0: (Produce(0): 0 to 2 [usable: 2], Fetch(1): 0 to 3 [usable: 3], Offsets(2): 0 to 1 [usable: 1], Metadata(3): 0 to 2 [usable: 2], LeaderAndIsr(4): 0 [usable: 0], StopReplica(5): 0 [usable: 0], UpdateMetadata(6): 0 to 3 [usable: 3], ControlledShutdown(7): 1 [usable: 1], OffsetCommit(8): 0 to 2 [usable: 2], OffsetFetch(9): 0 to 2 [usable: 2], GroupCoordinator(10): 0 [usable: 0], JoinGroup(11): 0 to 1 [usable: 1], Heartbeat(12): 0 [usable: 0], LeaveGroup(13): 0 [usable: 0], SyncGroup(14): 0 [usable: 0], DescribeGroups(15): 0 [usable: 0], ListGroups(16): 0 [usable: 0], SaslHandshake(17): 0 [usable: 0], ApiVersions(18): 0 [usable: 0], CreateTopics(19): 0 to 1 [usable: 1], DeleteTopics(20): 0 [usable: 0])
2017-06-12 17:03:46 DEBUG Fetcher:181 - Sending fetch for partitions [wordcount-lambda-example-Counts-repartition-0, TextLinesTopic-0] to broker tmon-jhkwon78-n.tmoncorp.com:9092 (id: 0 rack: null)
2017-06-12 17:03:46 DEBUG Metrics:335 - Added sensor with name topic.WordsWithCountsTopic.bytes-fetched
2017-06-12 17:03:46 DEBUG Metrics:335 - Added sensor with name topic.WordsWithCountsTopic.records-fetched
2017-06-12 17:03:46 DEBUG Metrics:335 - Added sensor with name WordsWithCountsTopic-0.records-lag
2017-06-12 17:03:46 DEBUG Fetcher:181 - Sending fetch for partitions [WordsWithCountsTopic-0] to broker tmon-jhkwon78-n.tmoncorp.com:9092 (id: 0 rack: null)
2017-06-12 17:03:47 DEBUG Fetcher:181 - Sending fetch for partitions [wordcount-lambda-example-Counts-repartition-0, TextLinesTopic-0] to broker tmon-jhkwon78-n.tmoncorp.com:9092 (id: 0 rack: null)
2017-06-12 17:03:47 DEBUG ConsumerCoordinator:620 - Sending asynchronous auto-commit of offsets {WordsWithCountsTopic-0=OffsetAndMetadata{offset=50, metadata=''}} for group test
2017-06-12 17:03:47 DEBUG ConsumerCoordinator:736 - Group test committed offset 50 for partition WordsWithCountsTopic-0
2017-06-12 17:03:47 DEBUG ConsumerCoordinator:631 - Completed auto-commit of offsets {WordsWithCountsTopic-0=OffsetAndMetadata{offset=50, metadata=''}} for group test
2017-06-12 17:03:47 DEBUG Fetcher:181 - Sending fetch for partitions [WordsWithCountsTopic-0] to broker tmon-jhkwon78-n.tmoncorp.com:9092 (id: 0 rack: null)
2017-06-12 17:03:47 DEBUG Fetcher:181 - Sending fetch for partitions [wordcount-lambda-example-Counts-repartition-0, TextLinesTopic-0] to broker tmon-jhkwon78-n.tmoncorp.com:9092 (id: 0 rack: null)
2017-06-12 17:03:47 DEBUG Fetcher:181 - Sending fetch for partitions [WordsWithCountsTopic-0] to broker tmon-jhkwon78-n.tmoncorp.com:9092 (id: 0 rack: null)
2017-06-12 17:03:48 DEBUG Fetcher:181 - Sending fetch for partitions [wordcount-lambda-example-Counts-repartition-0, TextLinesTopic-0] to broker tmon-jhkwon78-n.tmoncorp.com:9092 (id: 0 rack: null)
2017-06-12 17:03:48 DEBUG ConsumerCoordinator:620 - Sending asynchronous auto-commit of offsets {WordsWithCountsTopic-0=OffsetAndMetadata{offset=50, metadata=''}} for group test
2017-06-12 17:03:48 DEBUG ConsumerCoordinator:736 - Group test committed offset 50 for partition WordsWithCountsTopic-0
2017-06-12 17:03:48 DEBUG ConsumerCoordinator:631 - Completed auto-commit of offsets {WordsWithCountsTopic-0=OffsetAndMetadata{offset=50, metadata=''}} for group test
2017-06-12 17:03:48 DEBUG Fetcher:181 - Sending fetch for partitions [WordsWithCountsTopic-0] to broker tmon-jhkwon78-n.tmoncorp.com:9092 (id: 0 rack: null)
2017-06-12 17:03:48 DEBUG Fetcher:181 - Sending fetch for partitions [wordcount-lambda-example-Counts-repartition-0, TextLinesTopic-0] to broker tmon-jhkwon78-n.tmoncorp.com:9092 (id: 0 rack: null)
2017-06-12 17:03:48 DEBUG Fetcher:181 - Sending fetch for partitions [WordsWithCountsTopic-0] to broker tmon-jhkwon78-n.tmoncorp.com:9092 (id: 0 rack: null)
2017-06-12 17:03:49 DEBUG AbstractCoordinator:724 - Sending Heartbeat request for group wordcount-lambda-example to coordinator tmon-jhkwon78-n.tmoncorp.com:9092 (id: 2147483647 rack: null)
2017-06-12 17:03:49 DEBUG AbstractCoordinator:737 - Received successful Heartbeat response for group wordcount-lambda-example
2017-06-12 17:03:49 DEBUG Fetcher:181 - Sending fetch for partitions [wordcount-lambda-example-Counts-repartition-0, TextLinesTopic-0] to broker tmon-jhkwon78-n.tmoncorp.com:9092 (id: 0 rack: null)
2017-06-12 17:03:49 DEBUG AbstractCoordinator:724 - Sending Heartbeat request for group test to coordinator tmon-jhkwon78-n.tmoncorp.com:9092 (id: 2147483647 rack: null)
2017-06-12 17:03:49 DEBUG ConsumerCoordinator:620 - Sending asynchronous auto-commit of offsets {WordsWithCountsTopic-0=OffsetAndMetadata{offset=50, metadata=''}} for group test
2017-06-12 17:03:49 DEBUG AbstractCoordinator:737 - Received successful Heartbeat response for group test
2017-06-12 17:03:49 DEBUG ConsumerCoordinator:736 - Group test committed offset 50 for partition WordsWithCountsTopic-0
2017-06-12 17:03:49 DEBUG ConsumerCoordinator:631 - Completed auto-commit of offsets {WordsWithCountsTopic-0=OffsetAndMetadata{offset=50, metadata=''}} for group test
2017-06-12 17:03:49 DEBUG Fetcher:181 - Sending fetch for partitions [WordsWithCountsTopic-0] to broker tmon-jhkwon78-n.tmoncorp.com:9092 (id: 0 rack: null)
2017-06-12 17:03:49 DEBUG Fetcher:181 - Sending fetch for partitions [wordcount-lambda-example-Counts-repartition-0, TextLinesTopic-0] to broker tmon-jhkwon78-n.tmoncorp.com:9092 (id: 0 rack: null)
2017-06-12 17:03:49 DEBUG Fetcher:181 - Sending fetch for partitions [WordsWithCountsTopic-0] to broker tmon-jhkwon78-n.tmoncorp.com:9092 (id: 0 rack: null)
2017-06-12 17:03:50 DEBUG Fetcher:181 - Sending fetch for partitions [wordcount-lambda-example-Counts-repartition-0, TextLinesTopic-0] to broker tmon-jhkwon78-n.tmoncorp.com:9092 (id: 0 rack: null)
2017-06-12 17:03:50 DEBUG ConsumerCoordinator:620 - Sending asynchronous auto-commit of offsets {WordsWithCountsTopic-0=OffsetAndMetadata{offset=50, metadata=''}} for group test
2017-06-12 17:03:50 DEBUG ConsumerCoordinator:736 - Group test committed offset 50 for partition WordsWithCountsTopic-0
2017-06-12 17:03:50 DEBUG ConsumerCoordinator:631 - Completed auto-commit of offsets {WordsWithCountsTopic-0=OffsetAndMetadata{offset=50, metadata=''}} for group test
2017-06-12 17:03:50 DEBUG Fetcher:181 - Sending fetch for partitions [WordsWithCountsTopic-0] to broker tmon-jhkwon78-n.tmoncorp.com:9092 (id: 0 rack: null)
2017-06-12 17:03:50 DEBUG Fetcher:181 - Sending fetch for partitions [wordcount-lambda-example-Counts-repartition-0, TextLinesTopic-0] to broker tmon-jhkwon78-n.tmoncorp.com:9092 (id: 0 rack: null)
2017-06-12 17:03:50 DEBUG Fetcher:181 - Sending fetch for partitions [WordsWithCountsTopic-0] to broker tmon-jhkwon78-n.tmoncorp.com:9092 (id: 0 rack: null)
2017-06-12 17:03:51 DEBUG ConsumerCoordinator:620 - Sending asynchronous auto-commit of offsets {WordsWithCountsTopic-0=OffsetAndMetadata{offset=50, metadata=''}} for group test
2017-06-12 17:03:51 DEBUG ConsumerCoordinator:736 - Group test committed offset 50 for partition WordsWithCountsTopic-0
2017-06-12 17:03:51 DEBUG ConsumerCoordinator:631 - Completed auto-commit of offsets {WordsWithCountsTopic-0=OffsetAndMetadata{offset=50, metadata=''}} for group test
2017-06-12 17:03:51 DEBUG Fetcher:181 - Sending fetch for partitions [wordcount-lambda-example-Counts-repartition-0, TextLinesTopic-0] to broker tmon-jhkwon78-n.tmoncorp.com:9092 (id: 0 rack: null)
2017-06-12 17:03:51 DEBUG Fetcher:181 - Sending fetch for partitions [WordsWithCountsTopic-0] to broker tmon-jhkwon78-n.tmoncorp.com:9092 (id: 0 rack: null)
2017-06-12 17:03:51 DEBUG Fetcher:181 - Sending fetch for partitions [wordcount-lambda-example-Counts-repartition-0, TextLinesTopic-0] to broker tmon-jhkwon78-n.tmoncorp.com:9092 (id: 0 rack: null)
2017-06-12 17:03:51 DEBUG Fetcher:181 - Sending fetch for partitions [WordsWithCountsTopic-0] to broker tmon-jhkwon78-n.tmoncorp.com:9092 (id: 0 rack: null)
2017-06-12 17:03:52 DEBUG AbstractCoordinator:724 - Sending Heartbeat request for group wordcount-lambda-example to coordinator tmon-jhkwon78-n.tmoncorp.com:9092 (id: 2147483647 rack: null)
2017-06-12 17:03:52 DEBUG AbstractCoordinator:724 - Sending Heartbeat request for group test to coordinator tmon-jhkwon78-n.tmoncorp.com:9092 (id: 2147483647 rack: null)
2017-06-12 17:03:52 DEBUG ConsumerCoordinator:620 - Sending asynchronous auto-commit of offsets {WordsWithCountsTopic-0=OffsetAndMetadata{offset=50, metadata=''}} for group test
2017-06-12 17:03:52 DEBUG AbstractCoordinator:737 - Received successful Heartbeat response for group test
2017-06-12 17:03:52 DEBUG AbstractCoordinator:737 - Received successful Heartbeat response for group wordcount-lambda-example
2017-06-12 17:03:52 DEBUG ConsumerCoordinator:736 - Group test committed offset 50 for partition WordsWithCountsTopic-0
2017-06-12 17:03:52 DEBUG ConsumerCoordinator:631 - Completed auto-commit of offsets {WordsWithCountsTopic-0=OffsetAndMetadata{offset=50, metadata=''}} for group test
2017-06-12 17:03:52 DEBUG Fetcher:181 - Sending fetch for partitions [wordcount-lambda-example-Counts-repartition-0, TextLinesTopic-0] to broker tmon-jhkwon78-n.tmoncorp.com:9092 (id: 0 rack: null)
2017-06-12 17:03:52 DEBUG Fetcher:181 - Sending fetch for partitions [WordsWithCountsTopic-0] to broker tmon-jhkwon78-n.tmoncorp.com:9092 (id: 0 rack: null)
2017-06-12 17:03:52 DEBUG Fetcher:181 - Sending fetch for partitions [wordcount-lambda-example-Counts-repartition-0, TextLinesTopic-0] to broker tmon-jhkwon78-n.tmoncorp.com:9092 (id: 0 rack: null)
2017-06-12 17:03:52 DEBUG Fetcher:181 - Sending fetch for partitions [WordsWithCountsTopic-0] to broker tmon-jhkwon78-n.tmoncorp.com:9092 (id: 0 rack: null)
2017-06-12 17:03:53 DEBUG ConsumerCoordinator:620 - Sending asynchronous auto-commit of offsets {WordsWithCountsTopic-0=OffsetAndMetadata{offset=50, metadata=''}} for group test
2017-06-12 17:03:53 DEBUG ConsumerCoordinator:736 - Group test committed offset 50 for partition WordsWithCountsTopic-0
2017-06-12 17:03:53 DEBUG ConsumerCoordinator:631 - Completed auto-commit of offsets {WordsWithCountsTopic-0=OffsetAndMetadata{offset=50, metadata=''}} for group test
2017-06-12 17:03:53 DEBUG Fetcher:181 - Sending fetch for partitions [wordcount-lambda-example-Counts-repartition-0, TextLinesTopic-0] to broker tmon-jhkwon78-n.tmoncorp.com:9092 (id: 0 rack: null)
2017-06-12 17:03:53 DEBUG Fetcher:181 - Sending fetch for partitions [WordsWithCountsTopic-0] to broker tmon-jhkwon78-n.tmoncorp.com:9092 (id: 0 rack: null)
2017-06-12 17:03:53 DEBUG Fetcher:181 - Sending fetch for partitions [wordcount-lambda-example-Counts-repartition-0, TextLinesTopic-0] to broker tmon-jhkwon78-n.tmoncorp.com:9092 (id: 0 rack: null)
2017-06-12 17:03:53 DEBUG Fetcher:181 - Sending fetch for partitions [WordsWithCountsTopic-0] to broker tmon-jhkwon78-n.tmoncorp.com:9092 (id: 0 rack: null)
2017-06-12 17:03:54 DEBUG ConsumerCoordinator:620 - Sending asynchronous auto-commit of offsets {WordsWithCountsTopic-0=OffsetAndMetadata{offset=50, metadata=''}} for group test
2017-06-12 17:03:54 DEBUG ConsumerCoordinator:736 - Group test committed offset 50 for partition WordsWithCountsTopic-0
2017-06-12 17:03:54 DEBUG ConsumerCoordinator:631 - Completed auto-commit of offsets {WordsWithCountsTopic-0=OffsetAndMetadata{offset=50, metadata=''}} for group test
2017-06-12 17:03:54 DEBUG Fetcher:181 - Sending fetch for partitions [wordcount-lambda-example-Counts-repartition-0, TextLinesTopic-0] to broker tmon-jhkwon78-n.tmoncorp.com:9092 (id: 0 rack: null)
2017-06-12 17:03:54 DEBUG Fetcher:181 - Sending fetch for partitions [WordsWithCountsTopic-0] to broker tmon-jhkwon78-n.tmoncorp.com:9092 (id: 0 rack: null)
2017-06-12 17:03:54 DEBUG Fetcher:181 - Sending fetch for partitions [wordcount-lambda-example-Counts-repartition-0, TextLinesTopic-0] to broker tmon-jhkwon78-n.tmoncorp.com:9092 (id: 0 rack: null)
2017-06-12 17:03:54 DEBUG Fetcher:181 - Sending fetch for partitions [WordsWithCountsTopic-0] to broker tmon-jhkwon78-n.tmoncorp.com:9092 (id: 0 rack: null)
2017-06-12 17:03:55 DEBUG AbstractCoordinator:724 - Sending Heartbeat request for group wordcount-lambda-example to coordinator tmon-jhkwon78-n.tmoncorp.com:9092 (id: 2147483647 rack: null)
2017-06-12 17:03:55 DEBUG AbstractCoordinator:724 - Sending Heartbeat request for group test to coordinator tmon-jhkwon78-n.tmoncorp.com:9092 (id: 2147483647 rack: null)
2017-06-12 17:03:55 DEBUG ConsumerCoordinator:620 - Sending asynchronous auto-commit of offsets {WordsWithCountsTopic-0=OffsetAndMetadata{offset=50, metadata=''}} for group test
2017-06-12 17:03:55 DEBUG AbstractCoordinator:737 - Received successful Heartbeat response for group test
2017-06-12 17:03:55 DEBUG AbstractCoordinator:737 - Received successful Heartbeat response for group wordcount-lambda-example
2017-06-12 17:03:55 DEBUG ConsumerCoordinator:736 - Group test committed offset 50 for partition WordsWithCountsTopic-0
2017-06-12 17:03:55 DEBUG ConsumerCoordinator:631 - Completed auto-commit of offsets {WordsWithCountsTopic-0=OffsetAndMetadata{offset=50, metadata=''}} for group test
2017-06-12 17:03:55 DEBUG Fetcher:181 - Sending fetch for partitions [wordcount-lambda-example-Counts-repartition-0, TextLinesTopic-0] to broker tmon-jhkwon78-n.tmoncorp.com:9092 (id: 0 rack: null)
2017-06-12 17:03:55 DEBUG Fetcher:181 - Sending fetch for partitions [WordsWithCountsTopic-0] to broker tmon-jhkwon78-n.tmoncorp.com:9092 (id: 0 rack: null)
2017-06-12 17:03:55 DEBUG Fetcher:181 - Sending fetch for partitions [wordcount-lambda-example-Counts-repartition-0, TextLinesTopic-0] to broker tmon-jhkwon78-n.tmoncorp.com:9092 (id: 0 rack: null)
2017-06-12 17:03:55 DEBUG Fetcher:181 - Sending fetch for partitions [WordsWithCountsTopic-0] to broker tmon-jhkwon78-n.tmoncorp.com:9092 (id: 0 rack: null)
2017-06-12 17:03:56 DEBUG ConsumerCoordinator:620 - Sending asynchronous auto-commit of offsets {WordsWithCountsTopic-0=OffsetAndMetadata{offset=50, metadata=''}} for group test
2017-06-12 17:03:56 DEBUG ConsumerCoordinator:736 - Group test committed offset 50 for partition WordsWithCountsTopic-0
2017-06-12 17:03:56 DEBUG ConsumerCoordinator:631 - Completed auto-commit of offsets {WordsWithCountsTopic-0=OffsetAndMetadata{offset=50, metadata=''}} for group test
2017-06-12 17:03:56 DEBUG Fetcher:181 - Sending fetch for partitions [wordcount-lambda-example-Counts-repartition-0, TextLinesTopic-0] to broker tmon-jhkwon78-n.tmoncorp.com:9092 (id: 0 rack: null)
2017-06-12 17:03:56 DEBUG Fetcher:181 - Sending fetch for partitions [WordsWithCountsTopic-0] to broker tmon-jhkwon78-n.tmoncorp.com:9092 (id: 0 rack: null)
2017-06-12 17:03:56 DEBUG Fetcher:181 - Sending fetch for partitions [wordcount-lambda-example-Counts-repartition-0, TextLinesTopic-0] to broker tmon-jhkwon78-n.tmoncorp.com:9092 (id: 0 rack: null)
2017-06-12 17:03:56 DEBUG Fetcher:181 - Sending fetch for partitions [WordsWithCountsTopic-0] to broker tmon-jhkwon78-n.tmoncorp.com:9092 (id: 0 rack: null)
2017-06-12 17:03:57 DEBUG ConsumerCoordinator:620 - Sending asynchronous auto-commit of offsets {WordsWithCountsTopic-0=OffsetAndMetadata{offset=50, metadata=''}} for group test
2017-06-12 17:03:57 DEBUG ConsumerCoordinator:736 - Group test committed offset 50 for partition WordsWithCountsTopic-0
2017-06-12 17:03:57 DEBUG ConsumerCoordinator:631 - Completed auto-commit of offsets {WordsWithCountsTopic-0=OffsetAndMetadata{offset=50, metadata=''}} for group test
2017-06-12 17:03:57 DEBUG Fetcher:181 - Sending fetch for partitions [wordcount-lambda-example-Counts-repartition-0, TextLinesTopic-0] to broker tmon-jhkwon78-n.tmoncorp.com:9092 (id: 0 rack: null)
2017-06-12 17:03:57 DEBUG Fetcher:181 - Sending fetch for partitions [WordsWithCountsTopic-0] to broker tmon-jhkwon78-n.tmoncorp.com:9092 (id: 0 rack: null)
2017-06-12 17:03:57 DEBUG Fetcher:181 - Sending fetch for partitions [wordcount-lambda-example-Counts-repartition-0, TextLinesTopic-0] to broker tmon-jhkwon78-n.tmoncorp.com:9092 (id: 0 rack: null)
2017-06-12 17:03:57 DEBUG Fetcher:181 - Sending fetch for partitions [WordsWithCountsTopic-0] to broker tmon-jhkwon78-n.tmoncorp.com:9092 (id: 0 rack: null)
2017-06-12 17:03:58 DEBUG AbstractCoordinator:724 - Sending Heartbeat request for group wordcount-lambda-example to coordinator tmon-jhkwon78-n.tmoncorp.com:9092 (id: 2147483647 rack: null)
2017-06-12 17:03:58 DEBUG AbstractCoordinator:724 - Sending Heartbeat request for group test to coordinator tmon-jhkwon78-n.tmoncorp.com:9092 (id: 2147483647 rack: null)
2017-06-12 17:03:58 DEBUG ConsumerCoordinator:620 - Sending asynchronous auto-commit of offsets {WordsWithCountsTopic-0=OffsetAndMetadata{offset=50, metadata=''}} for group test
2017-06-12 17:03:58 DEBUG AbstractCoordinator:737 - Received successful Heartbeat response for group wordcount-lambda-example
2017-06-12 17:03:58 DEBUG AbstractCoordinator:737 - Received successful Heartbeat response for group test
2017-06-12 17:03:58 DEBUG ConsumerCoordinator:736 - Group test committed offset 50 for partition WordsWithCountsTopic-0
2017-06-12 17:03:58 DEBUG ConsumerCoordinator:631 - Completed auto-commit of offsets {WordsWithCountsTopic-0=OffsetAndMetadata{offset=50, metadata=''}} for group test
2017-06-12 17:03:58 DEBUG Fetcher:181 - Sending fetch for partitions [wordcount-lambda-example-Counts-repartition-0, TextLinesTopic-0] to broker tmon-jhkwon78-n.tmoncorp.com:9092 (id: 0 rack: null)
2017-06-12 17:03:58 DEBUG Fetcher:181 - Sending fetch for partitions [WordsWithCountsTopic-0] to broker tmon-jhkwon78-n.tmoncorp.com:9092 (id: 0 rack: null)
2017-06-12 17:03:59 DEBUG Fetcher:181 - Sending fetch for partitions [wordcount-lambda-example-Counts-repartition-0, TextLinesTopic-0] to broker tmon-jhkwon78-n.tmoncorp.com:9092 (id: 0 rack: null)
2017-06-12 17:03:59 DEBUG Fetcher:181 - Sending fetch for partitions [WordsWithCountsTopic-0] to broker tmon-jhkwon78-n.tmoncorp.com:9092 (id: 0 rack: null)
2017-06-12 17:03:59 DEBUG ConsumerCoordinator:620 - Sending asynchronous auto-commit of offsets {WordsWithCountsTopic-0=OffsetAndMetadata{offset=50, metadata=''}} for group test
2017-06-12 17:03:59 DEBUG ConsumerCoordinator:736 - Group test committed offset 50 for partition WordsWithCountsTopic-0
2017-06-12 17:03:59 DEBUG ConsumerCoordinator:631 - Completed auto-commit of offsets {WordsWithCountsTopic-0=OffsetAndMetadata{offset=50, metadata=''}} for group test
2017-06-12 17:03:59 DEBUG Fetcher:181 - Sending fetch for partitions [WordsWithCountsTopic-0] to broker tmon-jhkwon78-n.tmoncorp.com:9092 (id: 0 rack: null)
2017-06-12 17:03:59 DEBUG Fetcher:181 - Sending fetch for partitions [wordcount-lambda-example-Counts-repartition-0, TextLinesTopic-0] to broker tmon-jhkwon78-n.tmoncorp.com:9092 (id: 0 rack: null)
2017-06-12 17:04:00 DEBUG Fetcher:181 - Sending fetch for partitions [WordsWithCountsTopic-0] to broker tmon-jhkwon78-n.tmoncorp.com:9092 (id: 0 rack: null)
2017-06-12 17:04:00 DEBUG Fetcher:181 - Sending fetch for partitions [wordcount-lambda-example-Counts-repartition-0, TextLinesTopic-0] to broker tmon-jhkwon78-n.tmoncorp.com:9092 (id: 0 rack: null)
2017-06-12 17:04:00 DEBUG ConsumerCoordinator:620 - Sending asynchronous auto-commit of offsets {WordsWithCountsTopic-0=OffsetAndMetadata{offset=50, metadata=''}} for group test
2017-06-12 17:04:00 DEBUG ConsumerCoordinator:736 - Group test committed offset 50 for partition WordsWithCountsTopic-0
2017-06-12 17:04:00 DEBUG ConsumerCoordinator:631 - Completed auto-commit of offsets {WordsWithCountsTopic-0=OffsetAndMetadata{offset=50, metadata=''}} for group test
2017-06-12 17:04:00 DEBUG Fetcher:181 - Sending fetch for partitions [WordsWithCountsTopic-0] to broker tmon-jhkwon78-n.tmoncorp.com:9092 (id: 0 rack: null)
2017-06-12 17:04:00 DEBUG Fetcher:181 - Sending fetch for partitions [wordcount-lambda-example-Counts-repartition-0, TextLinesTopic-0] to broker tmon-jhkwon78-n.tmoncorp.com:9092 (id: 0 rack: null)
2017-06-12 17:04:01 DEBUG Fetcher:181 - Sending fetch for partitions [WordsWithCountsTopic-0] to broker tmon-jhkwon78-n.tmoncorp.com:9092 (id: 0 rack: null)
2017-06-12 17:04:01 DEBUG Fetcher:181 - Sending fetch for partitions [wordcount-lambda-example-Counts-repartition-0, TextLinesTopic-0] to broker tmon-jhkwon78-n.tmoncorp.com:9092 (id: 0 rack: null)
2017-06-12 17:04:01 DEBUG AbstractCoordinator:724 - Sending Heartbeat request for group test to coordinator tmon-jhkwon78-n.tmoncorp.com:9092 (id: 2147483647 rack: null)
2017-06-12 17:04:01 DEBUG ConsumerCoordinator:620 - Sending asynchronous auto-commit of offsets {WordsWithCountsTopic-0=OffsetAndMetadata{offset=50, metadata=''}} for group test
2017-06-12 17:04:01 DEBUG AbstractCoordinator:737 - Received successful Heartbeat response for group test
2017-06-12 17:04:01 DEBUG ConsumerCoordinator:736 - Group test committed offset 50 for partition WordsWithCountsTopic-0
2017-06-12 17:04:01 DEBUG ConsumerCoordinator:631 - Completed auto-commit of offsets {WordsWithCountsTopic-0=OffsetAndMetadata{offset=50, metadata=''}} for group test
2017-06-12 17:04:01 DEBUG AbstractCoordinator:724 - Sending Heartbeat request for group wordcount-lambda-example to coordinator tmon-jhkwon78-n.tmoncorp.com:9092 (id: 2147483647 rack: null)
2017-06-12 17:04:01 DEBUG AbstractCoordinator:737 - Received successful Heartbeat response for group wordcount-lambda-example
2017-06-12 17:04:01 DEBUG Fetcher:181 - Sending fetch for partitions [WordsWithCountsTopic-0] to broker tmon-jhkwon78-n.tmoncorp.com:9092 (id: 0 rack: null)
2017-06-12 17:04:01 DEBUG Fetcher:181 - Sending fetch for partitions [wordcount-lambda-example-Counts-repartition-0, TextLinesTopic-0] to broker tmon-jhkwon78-n.tmoncorp.com:9092 (id: 0 rack: null)
2017-06-12 17:04:02 DEBUG Fetcher:181 - Sending fetch for partitions [WordsWithCountsTopic-0] to broker tmon-jhkwon78-n.tmoncorp.com:9092 (id: 0 rack: null)
2017-06-12 17:04:02 DEBUG Fetcher:181 - Sending fetch for partitions [wordcount-lambda-example-Counts-repartition-0, TextLinesTopic-0] to broker tmon-jhkwon78-n.tmoncorp.com:9092 (id: 0 rack: null)
2017-06-12 17:04:02 DEBUG ConsumerCoordinator:620 - Sending asynchronous auto-commit of offsets {WordsWithCountsTopic-0=OffsetAndMetadata{offset=50, metadata=''}} for group test
2017-06-12 17:04:02 DEBUG ConsumerCoordinator:736 - Group test committed offset 50 for partition WordsWithCountsTopic-0
2017-06-12 17:04:02 DEBUG ConsumerCoordinator:631 - Completed auto-commit of offsets {WordsWithCountsTopic-0=OffsetAndMetadata{offset=50, metadata=''}} for group test
2017-06-12 17:04:02 DEBUG Fetcher:181 - Sending fetch for partitions [WordsWithCountsTopic-0] to broker tmon-jhkwon78-n.tmoncorp.com:9092 (id: 0 rack: null)
2017-06-12 17:04:02 DEBUG Fetcher:181 - Sending fetch for partitions [wordcount-lambda-example-Counts-repartition-0, TextLinesTopic-0] to broker tmon-jhkwon78-n.tmoncorp.com:9092 (id: 0 rack: null)
2017-06-12 17:04:03 DEBUG Fetcher:181 - Sending fetch for partitions [WordsWithCountsTopic-0] to broker tmon-jhkwon78-n.tmoncorp.com:9092 (id: 0 rack: null)
2017-06-12 17:04:03 DEBUG Fetcher:181 - Sending fetch for partitions [wordcount-lambda-example-Counts-repartition-0, TextLinesTopic-0] to broker tmon-jhkwon78-n.tmoncorp.com:9092 (id: 0 rack: null)
2017-06-12 17:04:03 DEBUG ConsumerCoordinator:620 - Sending asynchronous auto-commit of offsets {WordsWithCountsTopic-0=OffsetAndMetadata{offset=50, metadata=''}} for group test
2017-06-12 17:04:03 DEBUG ConsumerCoordinator:736 - Group test committed offset 50 for partition WordsWithCountsTopic-0
2017-06-12 17:04:03 DEBUG ConsumerCoordinator:631 - Completed auto-commit of offsets {WordsWithCountsTopic-0=OffsetAndMetadata{offset=50, metadata=''}} for group test
2017-06-12 17:04:03 DEBUG Fetcher:181 - Sending fetch for partitions [WordsWithCountsTopic-0] to broker tmon-jhkwon78-n.tmoncorp.com:9092 (id: 0 rack: null)
2017-06-12 17:04:03 DEBUG Fetcher:181 - Sending fetch for partitions [wordcount-lambda-example-Counts-repartition-0, TextLinesTopic-0] to broker tmon-jhkwon78-n.tmoncorp.com:9092 (id: 0 rack: null)
2017-06-12 17:04:04 DEBUG Fetcher:181 - Sending fetch for partitions [WordsWithCountsTopic-0] to broker tmon-jhkwon78-n.tmoncorp.com:9092 (id: 0 rack: null)
2017-06-12 17:04:04 DEBUG Fetcher:181 - Sending fetch for partitions [wordcount-lambda-example-Counts-repartition-0, TextLinesTopic-0] to broker tmon-jhkwon78-n.tmoncorp.com:9092 (id: 0 rack: null)
2017-06-12 17:04:04 DEBUG AbstractCoordinator:724 - Sending Heartbeat request for group test to coordinator tmon-jhkwon78-n.tmoncorp.com:9092 (id: 2147483647 rack: null)
2017-06-12 17:04:04 DEBUG ConsumerCoordinator:620 - Sending asynchronous auto-commit of offsets {WordsWithCountsTopic-0=OffsetAndMetadata{offset=50, metadata=''}} for group test
2017-06-12 17:04:04 DEBUG AbstractCoordinator:737 - Received successful Heartbeat response for group test
2017-06-12 17:04:04 DEBUG ConsumerCoordinator:736 - Group test committed offset 50 for partition WordsWithCountsTopic-0
2017-06-12 17:04:04 DEBUG ConsumerCoordinator:631 - Completed auto-commit of offsets {WordsWithCountsTopic-0=OffsetAndMetadata{offset=50, metadata=''}} for group test
2017-06-12 17:04:04 DEBUG AbstractCoordinator:724 - Sending Heartbeat request for group wordcount-lambda-example to coordinator tmon-jhkwon78-n.tmoncorp.com:9092 (id: 2147483647 rack: null)
2017-06-12 17:04:04 DEBUG AbstractCoordinator:737 - Received successful Heartbeat response for group wordcount-lambda-example
2017-06-12 17:04:04 DEBUG Fetcher:181 - Sending fetch for partitions [WordsWithCountsTopic-0] to broker tmon-jhkwon78-n.tmoncorp.com:9092 (id: 0 rack: null)
2017-06-12 17:04:04 DEBUG Fetcher:181 - Sending fetch for partitions [wordcount-lambda-example-Counts-repartition-0, TextLinesTopic-0] to broker tmon-jhkwon78-n.tmoncorp.com:9092 (id: 0 rack: null)
2017-06-12 17:04:05 DEBUG Fetcher:181 - Sending fetch for partitions [WordsWithCountsTopic-0] to broker tmon-jhkwon78-n.tmoncorp.com:9092 (id: 0 rack: null)
2017-06-12 17:04:05 DEBUG Fetcher:181 - Sending fetch for partitions [wordcount-lambda-example-Counts-repartition-0, TextLinesTopic-0] to broker tmon-jhkwon78-n.tmoncorp.com:9092 (id: 0 rack: null)
2017-06-12 17:04:05 DEBUG ConsumerCoordinator:620 - Sending asynchronous auto-commit of offsets {WordsWithCountsTopic-0=OffsetAndMetadata{offset=50, metadata=''}} for group test
2017-06-12 17:04:05 DEBUG ConsumerCoordinator:736 - Group test committed offset 50 for partition WordsWithCountsTopic-0
2017-06-12 17:04:05 DEBUG ConsumerCoordinator:631 - Completed auto-commit of offsets {WordsWithCountsTopic-0=OffsetAndMetadata{offset=50, metadata=''}} for group test
2017-06-12 17:04:05 DEBUG Fetcher:181 - Sending fetch for partitions [WordsWithCountsTopic-0] to broker tmon-jhkwon78-n.tmoncorp.com:9092 (id: 0 rack: null)
2017-06-12 17:04:05 DEBUG Fetcher:181 - Sending fetch for partitions [wordcount-lambda-example-Counts-repartition-0, TextLinesTopic-0] to broker tmon-jhkwon78-n.tmoncorp.com:9092 (id: 0 rack: null)
2017-06-12 17:04:20 INFO  StreamsConfig:180 - StreamsConfig values: 
	application.id = wordcount-lambda-example
	application.server = 
	bootstrap.servers = [localhost:9092]
	buffered.records.per.partition = 1000
	cache.max.bytes.buffering = 10485760
	client.id = 
	commit.interval.ms = 30000
	connections.max.idle.ms = 540000
	key.serde = class org.apache.kafka.common.serialization.Serdes$StringSerde
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	num.standby.replicas = 0
	num.stream.threads = 1
	partition.grouper = class org.apache.kafka.streams.processor.DefaultPartitionGrouper
	poll.ms = 100
	receive.buffer.bytes = 32768
	reconnect.backoff.ms = 50
	replication.factor = 1
	request.timeout.ms = 40000
	retry.backoff.ms = 100
	rocksdb.config.setter = null
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	state.cleanup.delay.ms = 60000
	state.dir = /tmp/kafka-streams
	timestamp.extractor = class org.apache.kafka.streams.processor.FailOnInvalidTimestamp
	value.serde = class org.apache.kafka.common.serialization.Serdes$StringSerde
	windowstore.changelog.additional.retention.ms = 86400000
	zookeeper.connect = 

2017-06-12 17:04:23 INFO  StreamThread:304 - stream-thread [StreamThread-1] Creating producer client
2017-06-12 17:04:23 INFO  ProducerConfig:180 - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	block.on.buffer.full = false
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = wordcount-lambda-example-86333745-5e01-4ef6-8db2-de12d342ae40-StreamThread-1-producer
	compression.type = none
	connections.max.idle.ms = 540000
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 100
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.fetch.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	timeout.ms = 30000
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2017-06-12 17:04:23 INFO  AppInfoParser:83 - Kafka version : 0.10.2.1
2017-06-12 17:04:23 INFO  AppInfoParser:84 - Kafka commitId : e89bffd6b2eff799
2017-06-12 17:04:23 INFO  StreamThread:306 - stream-thread [StreamThread-1] Creating consumer client
2017-06-12 17:04:23 INFO  ConsumerConfig:180 - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = wordcount-lambda-example-86333745-5e01-4ef6-8db2-de12d342ae40-StreamThread-1-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = wordcount-lambda-example
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 2147483647
	max.poll.records = 1000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamPartitionAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2017-06-12 17:04:23 INFO  AppInfoParser:83 - Kafka version : 0.10.2.1
2017-06-12 17:04:23 INFO  AppInfoParser:84 - Kafka commitId : e89bffd6b2eff799
2017-06-12 17:04:23 INFO  StreamThread:317 - stream-thread [StreamThread-1] Creating restore consumer client
2017-06-12 17:04:23 INFO  ConsumerConfig:180 - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = wordcount-lambda-example-86333745-5e01-4ef6-8db2-de12d342ae40-StreamThread-1-restore-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 2147483647
	max.poll.records = 1000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2017-06-12 17:04:23 INFO  AppInfoParser:83 - Kafka version : 0.10.2.1
2017-06-12 17:04:23 INFO  AppInfoParser:84 - Kafka commitId : e89bffd6b2eff799
2017-06-12 17:04:23 INFO  StreamThread:163 - stream-thread [StreamThread-1] State transition from NOT_RUNNING to RUNNING.
2017-06-12 17:04:24 INFO  KafkaStreams:224 - stream-client [wordcount-lambda-example-86333745-5e01-4ef6-8db2-de12d342ae40] State transition from CREATED to RUNNING.
2017-06-12 17:04:24 INFO  KafkaStreams:436 - stream-client [wordcount-lambda-example-86333745-5e01-4ef6-8db2-de12d342ae40] Started Kafka Stream process
2017-06-12 17:04:24 INFO  StreamThread:358 - stream-thread [StreamThread-1] Starting
2017-06-12 17:04:24 INFO  AbstractCoordinator:586 - Discovered coordinator tmon-jhkwon78-n.tmoncorp.com:9092 (id: 2147483647 rack: null) for group wordcount-lambda-example.
2017-06-12 17:04:24 INFO  ConsumerCoordinator:397 - Revoking previously assigned partitions [] for group wordcount-lambda-example
2017-06-12 17:04:24 INFO  StreamThread:248 - stream-thread [StreamThread-1] at state RUNNING: partitions [] revoked at the beginning of consumer rebalance.
2017-06-12 17:04:24 INFO  StreamThread:163 - stream-thread [StreamThread-1] State transition from RUNNING to PARTITIONS_REVOKED.
2017-06-12 17:04:24 INFO  KafkaStreams:224 - stream-client [wordcount-lambda-example-86333745-5e01-4ef6-8db2-de12d342ae40] State transition from RUNNING to REBALANCING.
2017-06-12 17:04:24 INFO  StreamThread:1042 - stream-thread [StreamThread-1] Updating suspended tasks to contain active tasks []
2017-06-12 17:04:24 INFO  StreamThread:1049 - stream-thread [StreamThread-1] Removing all active tasks []
2017-06-12 17:04:24 INFO  StreamThread:1064 - stream-thread [StreamThread-1] Removing all standby tasks []
2017-06-12 17:04:24 INFO  AbstractCoordinator:420 - (Re-)joining group wordcount-lambda-example
2017-06-12 17:04:24 INFO  StreamPartitionAssignor:300 - stream-thread [StreamThread-1] Constructed client metadata {86333745-5e01-4ef6-8db2-de12d342ae40=ClientMetadata{hostInfo=null, consumers=[wordcount-lambda-example-86333745-5e01-4ef6-8db2-de12d342ae40-StreamThread-1-consumer-77467f32-1434-44c4-9c4c-dd8a1b2bb0d6], state=[activeTasks: ([]) assignedTasks: ([]) prevActiveTasks: ([]) prevAssignedTasks: ([]) capacity: 1.0 cost: 0.0]}} from the member subscriptions.
2017-06-12 17:04:24 INFO  StreamPartitionAssignor:640 - stream-thread [StreamThread-1] Completed validating internal topics in partition assignor
2017-06-12 17:04:24 INFO  StreamPartitionAssignor:640 - stream-thread [StreamThread-1] Completed validating internal topics in partition assignor
2017-06-12 17:04:24 INFO  StreamPartitionAssignor:476 - stream-thread [StreamThread-1] Assigned tasks to clients as {86333745-5e01-4ef6-8db2-de12d342ae40=[activeTasks: ([0_0, 1_0]) assignedTasks: ([0_0, 1_0]) prevActiveTasks: ([]) prevAssignedTasks: ([]) capacity: 1.0 cost: 1.0]}.
2017-06-12 17:04:24 INFO  AbstractCoordinator:388 - Successfully joined group wordcount-lambda-example with generation 35
2017-06-12 17:04:24 INFO  ConsumerCoordinator:256 - Setting newly assigned partitions [TextLinesTopic-0, wordcount-lambda-example-Counts-repartition-0] for group wordcount-lambda-example
2017-06-12 17:04:24 INFO  StreamThread:228 - stream-thread [StreamThread-1] at state PARTITIONS_REVOKED: new partitions [TextLinesTopic-0, wordcount-lambda-example-Counts-repartition-0] assigned at the end of consumer rebalance.
2017-06-12 17:04:24 INFO  StreamThread:163 - stream-thread [StreamThread-1] State transition from PARTITIONS_REVOKED to ASSIGNING_PARTITIONS.
2017-06-12 17:04:24 INFO  KafkaStreams:224 - stream-client [wordcount-lambda-example-86333745-5e01-4ef6-8db2-de12d342ae40] State transition from REBALANCING to REBALANCING.
2017-06-12 17:04:24 INFO  StreamThread:858 - stream-thread [StreamThread-1] Creating active task 0_0 with assigned partitions [TextLinesTopic-0]
2017-06-12 17:04:25 INFO  StreamTask:140 - task [0_0] Initializing state stores
2017-06-12 17:04:25 INFO  StreamTask:333 - task [0_0] Initializing processor nodes of the topology
2017-06-12 17:04:25 INFO  StreamThread:858 - stream-thread [StreamThread-1] Creating active task 1_0 with assigned partitions [wordcount-lambda-example-Counts-repartition-0]
2017-06-12 17:04:25 INFO  StreamTask:140 - task [1_0] Initializing state stores
2017-06-12 17:04:25 INFO  StreamTask:333 - task [1_0] Initializing processor nodes of the topology
2017-06-12 17:04:25 INFO  StreamThread:163 - stream-thread [StreamThread-1] State transition from ASSIGNING_PARTITIONS to RUNNING.
2017-06-12 17:04:25 INFO  KafkaStreams:224 - stream-client [wordcount-lambda-example-86333745-5e01-4ef6-8db2-de12d342ae40] State transition from REBALANCING to RUNNING.
2017-06-12 17:04:34 INFO  ConsumerConfig:180 - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.LongDeserializer

2017-06-12 17:04:34 INFO  AppInfoParser:83 - Kafka version : 0.10.2.1
2017-06-12 17:04:34 INFO  AppInfoParser:84 - Kafka commitId : e89bffd6b2eff799
2017-06-12 17:04:34 INFO  AbstractCoordinator:586 - Discovered coordinator tmon-jhkwon78-n.tmoncorp.com:9092 (id: 2147483647 rack: null) for group test.
2017-06-12 17:04:34 INFO  ConsumerCoordinator:397 - Revoking previously assigned partitions [] for group test
2017-06-12 17:04:34 INFO  AbstractCoordinator:420 - (Re-)joining group test
2017-06-12 17:04:34 INFO  AbstractCoordinator:388 - Successfully joined group test with generation 46
2017-06-12 17:04:34 INFO  ConsumerCoordinator:256 - Setting newly assigned partitions [WordsWithCountsTopic-0] for group test
2017-06-12 17:05:27 INFO  App:52 - groupBy hello
2017-06-12 17:05:27 INFO  AbstractCoordinator:631 - Marking the coordinator tmon-jhkwon78-n.tmoncorp.com:9092 (id: 2147483647 rack: null) dead for group test
2017-06-12 17:05:27 INFO  AbstractCoordinator:631 - Marking the coordinator tmon-jhkwon78-n.tmoncorp.com:9092 (id: 2147483647 rack: null) dead for group wordcount-lambda-example
2017-06-12 17:06:35 INFO  AbstractCoordinator:586 - Discovered coordinator tmon-jhkwon78-n.tmoncorp.com:9092 (id: 2147483647 rack: null) for group test.
2017-06-12 17:06:35 WARN  ConsumerCoordinator:626 - Auto-commit of offsets {WordsWithCountsTopic-0=OffsetAndMetadata{offset=50, metadata=''}} failed for group test: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
2017-06-12 17:06:35 WARN  ConsumerCoordinator:652 - Auto-commit of offsets {WordsWithCountsTopic-0=OffsetAndMetadata{offset=50, metadata=''}} failed for group test: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
2017-06-12 17:06:35 INFO  ConsumerCoordinator:397 - Revoking previously assigned partitions [WordsWithCountsTopic-0] for group test
2017-06-12 17:06:35 INFO  AbstractCoordinator:420 - (Re-)joining group test
2017-06-12 17:06:35 INFO  AbstractCoordinator:388 - Successfully joined group test with generation 48
2017-06-12 17:06:35 INFO  ConsumerCoordinator:256 - Setting newly assigned partitions [WordsWithCountsTopic-0] for group test
2017-06-12 17:06:35 INFO  StreamThread:767 - stream-thread [StreamThread-1] Committing all tasks because the commit interval 30000ms has elapsed
2017-06-12 17:06:35 INFO  StreamThread:805 - stream-thread [StreamThread-1] Committing task StreamTask 0_0
2017-06-12 17:06:35 INFO  AbstractCoordinator:586 - Discovered coordinator tmon-jhkwon78-n.tmoncorp.com:9092 (id: 2147483647 rack: null) for group wordcount-lambda-example.
2017-06-12 17:06:35 WARN  StreamThread:810 - stream-thread [StreamThread-1] Failed to commit StreamTask 0_0 state: 
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:770)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:716)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:784)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:765)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:186)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:149)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:116)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:493)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:322)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:253)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:188)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.commitOffsetsSync(ConsumerCoordinator.java:582)
	at org.apache.kafka.clients.consumer.KafkaConsumer.commitSync(KafkaConsumer.java:1124)
	at org.apache.kafka.streams.processor.internals.StreamTask.commitOffsets(StreamTask.java:296)
	at org.apache.kafka.streams.processor.internals.StreamTask$1.run(StreamTask.java:79)
	at org.apache.kafka.streams.processor.internals.StreamsMetricsImpl.measureLatencyNs(StreamsMetricsImpl.java:188)
	at org.apache.kafka.streams.processor.internals.StreamTask.commit(StreamTask.java:280)
	at org.apache.kafka.streams.processor.internals.StreamThread.commitOne(StreamThread.java:807)
	at org.apache.kafka.streams.processor.internals.StreamThread.commitAll(StreamThread.java:794)
	at org.apache.kafka.streams.processor.internals.StreamThread.maybeCommit(StreamThread.java:769)
	at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:647)
	at org.apache.kafka.streams.processor.internals.StreamThread.run(StreamThread.java:361)
2017-06-12 17:06:35 INFO  StreamThread:805 - stream-thread [StreamThread-1] Committing task StreamTask 1_0
2017-06-12 17:06:35 INFO  ConsumerCoordinator:397 - Revoking previously assigned partitions [TextLinesTopic-0, wordcount-lambda-example-Counts-repartition-0] for group wordcount-lambda-example
2017-06-12 17:06:35 INFO  StreamThread:248 - stream-thread [StreamThread-1] at state RUNNING: partitions [TextLinesTopic-0, wordcount-lambda-example-Counts-repartition-0] revoked at the beginning of consumer rebalance.
2017-06-12 17:06:35 INFO  StreamThread:163 - stream-thread [StreamThread-1] State transition from RUNNING to PARTITIONS_REVOKED.
2017-06-12 17:06:35 INFO  KafkaStreams:224 - stream-client [wordcount-lambda-example-86333745-5e01-4ef6-8db2-de12d342ae40] State transition from RUNNING to REBALANCING.
2017-06-12 17:06:35 INFO  StreamThread:1086 - stream-thread [StreamThread-1] Closing task's topology 0_0
2017-06-12 17:06:35 INFO  StreamThread:1086 - stream-thread [StreamThread-1] Closing task's topology 1_0
2017-06-12 17:06:35 INFO  StreamThread:554 - stream-thread [StreamThread-1] Flushing state stores of task 0_0
2017-06-12 17:06:35 INFO  StreamThread:554 - stream-thread [StreamThread-1] Flushing state stores of task 1_0
2017-06-12 17:06:35 INFO  StreamThread:544 - stream-thread [StreamThread-1] Committing consumer offsets of task 0_0
2017-06-12 17:06:35 ERROR StreamThread:503 - stream-thread [StreamThread-1] Failed while executing StreamTask 0_0 due to commit consumer offsets: 
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.sendOffsetCommitRequest(ConsumerCoordinator.java:702)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.commitOffsetsSync(ConsumerCoordinator.java:581)
	at org.apache.kafka.clients.consumer.KafkaConsumer.commitSync(KafkaConsumer.java:1124)
	at org.apache.kafka.streams.processor.internals.StreamTask.commitOffsets(StreamTask.java:296)
	at org.apache.kafka.streams.processor.internals.StreamThread$3.apply(StreamThread.java:545)
	at org.apache.kafka.streams.processor.internals.StreamThread.performOnTasks(StreamThread.java:501)
	at org.apache.kafka.streams.processor.internals.StreamThread.commitOffsets(StreamThread.java:541)
	at org.apache.kafka.streams.processor.internals.StreamThread.suspendTasksAndState(StreamThread.java:479)
	at org.apache.kafka.streams.processor.internals.StreamThread.access$1300(StreamThread.java:69)
	at org.apache.kafka.streams.processor.internals.StreamThread$1.onPartitionsRevoked(StreamThread.java:252)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.onJoinPrepare(ConsumerCoordinator.java:400)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.joinGroupIfNeeded(AbstractCoordinator.java:342)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureActiveGroup(AbstractCoordinator.java:303)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:290)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1029)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:995)
	at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:592)
	at org.apache.kafka.streams.processor.internals.StreamThread.run(StreamThread.java:361)
2017-06-12 17:06:35 INFO  StreamThread:544 - stream-thread [StreamThread-1] Committing consumer offsets of task 1_0
2017-06-12 17:06:35 INFO  StreamThread:1042 - stream-thread [StreamThread-1] Updating suspended tasks to contain active tasks [0_0, 1_0]
2017-06-12 17:06:35 INFO  StreamThread:1049 - stream-thread [StreamThread-1] Removing all active tasks [0_0, 1_0]
2017-06-12 17:06:35 INFO  StreamThread:1064 - stream-thread [StreamThread-1] Removing all standby tasks []
2017-06-12 17:06:35 INFO  AbstractCoordinator:420 - (Re-)joining group wordcount-lambda-example
2017-06-12 17:06:35 INFO  StreamPartitionAssignor:300 - stream-thread [StreamThread-1] Constructed client metadata {86333745-5e01-4ef6-8db2-de12d342ae40=ClientMetadata{hostInfo=null, consumers=[wordcount-lambda-example-86333745-5e01-4ef6-8db2-de12d342ae40-StreamThread-1-consumer-88ba01dd-2f2d-48f0-a36c-985b284ab180], state=[activeTasks: ([]) assignedTasks: ([]) prevActiveTasks: ([0_0, 1_0]) prevAssignedTasks: ([0_0, 1_0]) capacity: 1.0 cost: 0.0]}} from the member subscriptions.
2017-06-12 17:06:35 INFO  StreamPartitionAssignor:640 - stream-thread [StreamThread-1] Completed validating internal topics in partition assignor
2017-06-12 17:06:43 INFO  StreamsConfig:180 - StreamsConfig values: 
	application.id = wordcount-lambda-example
	application.server = 
	bootstrap.servers = [localhost:9092]
	buffered.records.per.partition = 1000
	cache.max.bytes.buffering = 10485760
	client.id = 
	commit.interval.ms = 30000
	connections.max.idle.ms = 540000
	key.serde = class org.apache.kafka.common.serialization.Serdes$StringSerde
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	num.standby.replicas = 0
	num.stream.threads = 1
	partition.grouper = class org.apache.kafka.streams.processor.DefaultPartitionGrouper
	poll.ms = 100
	receive.buffer.bytes = 32768
	reconnect.backoff.ms = 50
	replication.factor = 1
	request.timeout.ms = 40000
	retry.backoff.ms = 100
	rocksdb.config.setter = null
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	state.cleanup.delay.ms = 60000
	state.dir = /tmp/kafka-streams
	timestamp.extractor = class org.apache.kafka.streams.processor.FailOnInvalidTimestamp
	value.serde = class org.apache.kafka.common.serialization.Serdes$StringSerde
	windowstore.changelog.additional.retention.ms = 86400000
	zookeeper.connect = 

2017-06-12 17:06:46 INFO  StreamThread:304 - stream-thread [StreamThread-1] Creating producer client
2017-06-12 17:06:46 INFO  ProducerConfig:180 - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	block.on.buffer.full = false
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = wordcount-lambda-example-89c19bfc-8855-49f4-ba10-f40abbd90264-StreamThread-1-producer
	compression.type = none
	connections.max.idle.ms = 540000
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 100
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.fetch.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	timeout.ms = 30000
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2017-06-12 17:06:46 INFO  AppInfoParser:83 - Kafka version : 0.10.2.1
2017-06-12 17:06:46 INFO  AppInfoParser:84 - Kafka commitId : e89bffd6b2eff799
2017-06-12 17:06:46 INFO  StreamThread:306 - stream-thread [StreamThread-1] Creating consumer client
2017-06-12 17:06:46 INFO  ConsumerConfig:180 - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = wordcount-lambda-example-89c19bfc-8855-49f4-ba10-f40abbd90264-StreamThread-1-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = wordcount-lambda-example
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 2147483647
	max.poll.records = 1000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamPartitionAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2017-06-12 17:06:46 INFO  AppInfoParser:83 - Kafka version : 0.10.2.1
2017-06-12 17:06:46 INFO  AppInfoParser:84 - Kafka commitId : e89bffd6b2eff799
2017-06-12 17:06:46 INFO  StreamThread:317 - stream-thread [StreamThread-1] Creating restore consumer client
2017-06-12 17:06:46 INFO  ConsumerConfig:180 - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = wordcount-lambda-example-89c19bfc-8855-49f4-ba10-f40abbd90264-StreamThread-1-restore-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 2147483647
	max.poll.records = 1000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2017-06-12 17:06:46 INFO  AppInfoParser:83 - Kafka version : 0.10.2.1
2017-06-12 17:06:46 INFO  AppInfoParser:84 - Kafka commitId : e89bffd6b2eff799
2017-06-12 17:06:46 INFO  StreamThread:163 - stream-thread [StreamThread-1] State transition from NOT_RUNNING to RUNNING.
2017-06-12 17:06:46 INFO  KafkaStreams:224 - stream-client [wordcount-lambda-example-89c19bfc-8855-49f4-ba10-f40abbd90264] State transition from CREATED to RUNNING.
2017-06-12 17:06:46 INFO  KafkaStreams:436 - stream-client [wordcount-lambda-example-89c19bfc-8855-49f4-ba10-f40abbd90264] Started Kafka Stream process
2017-06-12 17:06:46 INFO  StreamThread:358 - stream-thread [StreamThread-1] Starting
2017-06-12 17:06:46 INFO  AbstractCoordinator:586 - Discovered coordinator tmon-jhkwon78-n.tmoncorp.com:9092 (id: 2147483647 rack: null) for group wordcount-lambda-example.
2017-06-12 17:06:46 INFO  ConsumerCoordinator:397 - Revoking previously assigned partitions [] for group wordcount-lambda-example
2017-06-12 17:06:46 INFO  StreamThread:248 - stream-thread [StreamThread-1] at state RUNNING: partitions [] revoked at the beginning of consumer rebalance.
2017-06-12 17:06:46 INFO  StreamThread:163 - stream-thread [StreamThread-1] State transition from RUNNING to PARTITIONS_REVOKED.
2017-06-12 17:06:46 INFO  KafkaStreams:224 - stream-client [wordcount-lambda-example-89c19bfc-8855-49f4-ba10-f40abbd90264] State transition from RUNNING to REBALANCING.
2017-06-12 17:06:46 INFO  StreamThread:1042 - stream-thread [StreamThread-1] Updating suspended tasks to contain active tasks []
2017-06-12 17:06:46 INFO  StreamThread:1049 - stream-thread [StreamThread-1] Removing all active tasks []
2017-06-12 17:06:46 INFO  StreamThread:1064 - stream-thread [StreamThread-1] Removing all standby tasks []
2017-06-12 17:06:46 INFO  AbstractCoordinator:420 - (Re-)joining group wordcount-lambda-example
2017-06-12 17:06:46 INFO  StreamPartitionAssignor:300 - stream-thread [StreamThread-1] Constructed client metadata {89c19bfc-8855-49f4-ba10-f40abbd90264=ClientMetadata{hostInfo=null, consumers=[wordcount-lambda-example-89c19bfc-8855-49f4-ba10-f40abbd90264-StreamThread-1-consumer-ca4f16ca-a442-42d6-9fda-c27bba00d790], state=[activeTasks: ([]) assignedTasks: ([]) prevActiveTasks: ([]) prevAssignedTasks: ([]) capacity: 1.0 cost: 0.0]}} from the member subscriptions.
2017-06-12 17:06:47 INFO  StreamPartitionAssignor:640 - stream-thread [StreamThread-1] Completed validating internal topics in partition assignor
2017-06-12 17:06:47 INFO  StreamPartitionAssignor:640 - stream-thread [StreamThread-1] Completed validating internal topics in partition assignor
2017-06-12 17:06:47 INFO  StreamPartitionAssignor:476 - stream-thread [StreamThread-1] Assigned tasks to clients as {89c19bfc-8855-49f4-ba10-f40abbd90264=[activeTasks: ([0_0, 1_0]) assignedTasks: ([0_0, 1_0]) prevActiveTasks: ([]) prevAssignedTasks: ([]) capacity: 1.0 cost: 1.0]}.
2017-06-12 17:06:47 INFO  AbstractCoordinator:388 - Successfully joined group wordcount-lambda-example with generation 39
2017-06-12 17:06:47 INFO  ConsumerCoordinator:256 - Setting newly assigned partitions [TextLinesTopic-0, wordcount-lambda-example-Counts-repartition-0] for group wordcount-lambda-example
2017-06-12 17:06:47 INFO  StreamThread:228 - stream-thread [StreamThread-1] at state PARTITIONS_REVOKED: new partitions [TextLinesTopic-0, wordcount-lambda-example-Counts-repartition-0] assigned at the end of consumer rebalance.
2017-06-12 17:06:47 INFO  StreamThread:163 - stream-thread [StreamThread-1] State transition from PARTITIONS_REVOKED to ASSIGNING_PARTITIONS.
2017-06-12 17:06:47 INFO  KafkaStreams:224 - stream-client [wordcount-lambda-example-89c19bfc-8855-49f4-ba10-f40abbd90264] State transition from REBALANCING to REBALANCING.
2017-06-12 17:06:47 INFO  StreamThread:858 - stream-thread [StreamThread-1] Creating active task 0_0 with assigned partitions [TextLinesTopic-0]
2017-06-12 17:06:47 INFO  StreamTask:140 - task [0_0] Initializing state stores
2017-06-12 17:06:47 INFO  StreamTask:333 - task [0_0] Initializing processor nodes of the topology
2017-06-12 17:06:47 INFO  StreamThread:858 - stream-thread [StreamThread-1] Creating active task 1_0 with assigned partitions [wordcount-lambda-example-Counts-repartition-0]
2017-06-12 17:06:47 INFO  StreamTask:140 - task [1_0] Initializing state stores
2017-06-12 17:06:48 INFO  StreamTask:333 - task [1_0] Initializing processor nodes of the topology
2017-06-12 17:06:48 INFO  StreamThread:163 - stream-thread [StreamThread-1] State transition from ASSIGNING_PARTITIONS to RUNNING.
2017-06-12 17:06:48 INFO  KafkaStreams:224 - stream-client [wordcount-lambda-example-89c19bfc-8855-49f4-ba10-f40abbd90264] State transition from REBALANCING to RUNNING.
2017-06-12 17:06:48 INFO  App:44 - value HELLO
2017-06-12 17:06:54 INFO  App:55 - groupBy hello
2017-06-12 17:06:54 INFO  ConsumerConfig:180 - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.LongDeserializer

2017-06-12 17:06:54 INFO  AppInfoParser:83 - Kafka version : 0.10.2.1
2017-06-12 17:06:54 INFO  AppInfoParser:84 - Kafka commitId : e89bffd6b2eff799
2017-06-12 17:06:54 INFO  AbstractCoordinator:586 - Discovered coordinator tmon-jhkwon78-n.tmoncorp.com:9092 (id: 2147483647 rack: null) for group test.
2017-06-12 17:06:54 INFO  ConsumerCoordinator:397 - Revoking previously assigned partitions [] for group test
2017-06-12 17:06:54 INFO  AbstractCoordinator:420 - (Re-)joining group test
2017-06-12 17:06:54 INFO  AbstractCoordinator:388 - Successfully joined group test with generation 50
2017-06-12 17:06:54 INFO  ConsumerCoordinator:256 - Setting newly assigned partitions [WordsWithCountsTopic-0] for group test
2017-06-12 17:07:01 INFO  App:44 - value test
2017-06-12 17:07:07 INFO  App:55 - groupBy test
2017-06-12 17:07:16 INFO  StreamThread:767 - stream-thread [StreamThread-1] Committing all tasks because the commit interval 30000ms has elapsed
2017-06-12 17:07:16 INFO  StreamThread:805 - stream-thread [StreamThread-1] Committing task StreamTask 0_0
2017-06-12 17:07:16 INFO  StreamThread:805 - stream-thread [StreamThread-1] Committing task StreamTask 1_0
2017-06-12 17:07:16 INFO  App:91 - offset 50 key hello value 20
2017-06-12 17:07:16 INFO  App:91 - offset 51 key test value 7
2017-06-12 17:07:46 INFO  StreamThread:767 - stream-thread [StreamThread-1] Committing all tasks because the commit interval 30000ms has elapsed
2017-06-12 17:07:46 INFO  StreamThread:805 - stream-thread [StreamThread-1] Committing task StreamTask 0_0
2017-06-12 17:07:46 INFO  StreamThread:805 - stream-thread [StreamThread-1] Committing task StreamTask 1_0
2017-06-12 17:08:16 INFO  StreamThread:767 - stream-thread [StreamThread-1] Committing all tasks because the commit interval 30000ms has elapsed
2017-06-12 17:08:16 INFO  StreamThread:805 - stream-thread [StreamThread-1] Committing task StreamTask 0_0
2017-06-12 17:08:16 INFO  StreamThread:805 - stream-thread [StreamThread-1] Committing task StreamTask 1_0
2017-06-12 17:08:28 INFO  StreamsConfig:180 - StreamsConfig values: 
	application.id = wordcount-lambda-example
	application.server = 
	bootstrap.servers = [localhost:9092]
	buffered.records.per.partition = 1000
	cache.max.bytes.buffering = 10485760
	client.id = 
	commit.interval.ms = 30000
	connections.max.idle.ms = 540000
	key.serde = class org.apache.kafka.common.serialization.Serdes$StringSerde
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	num.standby.replicas = 0
	num.stream.threads = 1
	partition.grouper = class org.apache.kafka.streams.processor.DefaultPartitionGrouper
	poll.ms = 100
	receive.buffer.bytes = 32768
	reconnect.backoff.ms = 50
	replication.factor = 1
	request.timeout.ms = 40000
	retry.backoff.ms = 100
	rocksdb.config.setter = null
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	state.cleanup.delay.ms = 60000
	state.dir = /tmp/kafka-streams
	timestamp.extractor = class org.apache.kafka.streams.processor.FailOnInvalidTimestamp
	value.serde = class org.apache.kafka.common.serialization.Serdes$StringSerde
	windowstore.changelog.additional.retention.ms = 86400000
	zookeeper.connect = 

2017-06-12 17:08:29 INFO  ConsumerConfig:180 - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.LongDeserializer

2017-06-12 17:08:29 INFO  AppInfoParser:83 - Kafka version : 0.10.2.1
2017-06-12 17:08:29 INFO  AppInfoParser:84 - Kafka commitId : e89bffd6b2eff799
2017-06-12 17:08:30 INFO  AbstractCoordinator:586 - Discovered coordinator tmon-jhkwon78-n.tmoncorp.com:9092 (id: 2147483647 rack: null) for group test.
2017-06-12 17:08:30 INFO  ConsumerCoordinator:397 - Revoking previously assigned partitions [] for group test
2017-06-12 17:08:30 INFO  AbstractCoordinator:420 - (Re-)joining group test
2017-06-12 17:08:30 INFO  StreamThread:304 - stream-thread [StreamThread-1] Creating producer client
2017-06-12 17:08:30 INFO  AbstractCoordinator:388 - Successfully joined group test with generation 51
2017-06-12 17:08:30 INFO  ProducerConfig:180 - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	block.on.buffer.full = false
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = wordcount-lambda-example-67e9b407-0a64-4b07-a768-429967fbd29b-StreamThread-1-producer
	compression.type = none
	connections.max.idle.ms = 540000
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 100
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.fetch.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	timeout.ms = 30000
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2017-06-12 17:08:30 INFO  ConsumerCoordinator:256 - Setting newly assigned partitions [WordsWithCountsTopic-0] for group test
2017-06-12 17:08:30 INFO  AppInfoParser:83 - Kafka version : 0.10.2.1
2017-06-12 17:08:31 INFO  AppInfoParser:84 - Kafka commitId : e89bffd6b2eff799
2017-06-12 17:08:31 INFO  StreamThread:306 - stream-thread [StreamThread-1] Creating consumer client
2017-06-12 17:08:31 INFO  ConsumerConfig:180 - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = wordcount-lambda-example-67e9b407-0a64-4b07-a768-429967fbd29b-StreamThread-1-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = wordcount-lambda-example
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 2147483647
	max.poll.records = 1000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamPartitionAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2017-06-12 17:08:31 INFO  AppInfoParser:83 - Kafka version : 0.10.2.1
2017-06-12 17:08:31 INFO  AppInfoParser:84 - Kafka commitId : e89bffd6b2eff799
2017-06-12 17:08:31 INFO  StreamThread:317 - stream-thread [StreamThread-1] Creating restore consumer client
2017-06-12 17:08:31 INFO  ConsumerConfig:180 - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = wordcount-lambda-example-67e9b407-0a64-4b07-a768-429967fbd29b-StreamThread-1-restore-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 2147483647
	max.poll.records = 1000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2017-06-12 17:08:31 INFO  AppInfoParser:83 - Kafka version : 0.10.2.1
2017-06-12 17:08:31 INFO  AppInfoParser:84 - Kafka commitId : e89bffd6b2eff799
2017-06-12 17:08:31 INFO  StreamThread:163 - stream-thread [StreamThread-1] State transition from NOT_RUNNING to RUNNING.
2017-06-12 17:08:31 INFO  KafkaStreams:224 - stream-client [wordcount-lambda-example-67e9b407-0a64-4b07-a768-429967fbd29b] State transition from CREATED to RUNNING.
2017-06-12 17:08:31 INFO  KafkaStreams:436 - stream-client [wordcount-lambda-example-67e9b407-0a64-4b07-a768-429967fbd29b] Started Kafka Stream process
2017-06-12 17:08:31 INFO  StreamThread:358 - stream-thread [StreamThread-1] Starting
2017-06-12 17:08:31 INFO  AbstractCoordinator:586 - Discovered coordinator tmon-jhkwon78-n.tmoncorp.com:9092 (id: 2147483647 rack: null) for group wordcount-lambda-example.
2017-06-12 17:08:31 INFO  ConsumerCoordinator:397 - Revoking previously assigned partitions [] for group wordcount-lambda-example
2017-06-12 17:08:31 INFO  StreamThread:248 - stream-thread [StreamThread-1] at state RUNNING: partitions [] revoked at the beginning of consumer rebalance.
2017-06-12 17:08:31 INFO  StreamThread:163 - stream-thread [StreamThread-1] State transition from RUNNING to PARTITIONS_REVOKED.
2017-06-12 17:08:31 INFO  KafkaStreams:224 - stream-client [wordcount-lambda-example-67e9b407-0a64-4b07-a768-429967fbd29b] State transition from RUNNING to REBALANCING.
2017-06-12 17:08:31 INFO  StreamThread:1042 - stream-thread [StreamThread-1] Updating suspended tasks to contain active tasks []
2017-06-12 17:08:31 INFO  StreamThread:1049 - stream-thread [StreamThread-1] Removing all active tasks []
2017-06-12 17:08:31 INFO  StreamThread:1064 - stream-thread [StreamThread-1] Removing all standby tasks []
2017-06-12 17:08:31 INFO  AbstractCoordinator:420 - (Re-)joining group wordcount-lambda-example
2017-06-12 17:08:31 INFO  StreamPartitionAssignor:300 - stream-thread [StreamThread-1] Constructed client metadata {67e9b407-0a64-4b07-a768-429967fbd29b=ClientMetadata{hostInfo=null, consumers=[wordcount-lambda-example-67e9b407-0a64-4b07-a768-429967fbd29b-StreamThread-1-consumer-ecd8fc8f-6e4f-4802-bb8f-764e5fa536e8], state=[activeTasks: ([]) assignedTasks: ([]) prevActiveTasks: ([]) prevAssignedTasks: ([]) capacity: 1.0 cost: 0.0]}} from the member subscriptions.
2017-06-12 17:08:31 INFO  StreamPartitionAssignor:640 - stream-thread [StreamThread-1] Completed validating internal topics in partition assignor
2017-06-12 17:08:31 INFO  StreamPartitionAssignor:640 - stream-thread [StreamThread-1] Completed validating internal topics in partition assignor
2017-06-12 17:08:31 INFO  StreamPartitionAssignor:476 - stream-thread [StreamThread-1] Assigned tasks to clients as {67e9b407-0a64-4b07-a768-429967fbd29b=[activeTasks: ([0_0, 1_0]) assignedTasks: ([0_0, 1_0]) prevActiveTasks: ([]) prevAssignedTasks: ([]) capacity: 1.0 cost: 1.0]}.
2017-06-12 17:08:31 INFO  AbstractCoordinator:388 - Successfully joined group wordcount-lambda-example with generation 41
2017-06-12 17:08:31 INFO  ConsumerCoordinator:256 - Setting newly assigned partitions [TextLinesTopic-0, wordcount-lambda-example-Counts-repartition-0] for group wordcount-lambda-example
2017-06-12 17:08:31 INFO  StreamThread:228 - stream-thread [StreamThread-1] at state PARTITIONS_REVOKED: new partitions [TextLinesTopic-0, wordcount-lambda-example-Counts-repartition-0] assigned at the end of consumer rebalance.
2017-06-12 17:08:31 INFO  StreamThread:163 - stream-thread [StreamThread-1] State transition from PARTITIONS_REVOKED to ASSIGNING_PARTITIONS.
2017-06-12 17:08:31 INFO  KafkaStreams:224 - stream-client [wordcount-lambda-example-67e9b407-0a64-4b07-a768-429967fbd29b] State transition from REBALANCING to REBALANCING.
2017-06-12 17:08:31 INFO  StreamThread:858 - stream-thread [StreamThread-1] Creating active task 0_0 with assigned partitions [TextLinesTopic-0]
2017-06-12 17:08:31 INFO  StreamTask:140 - task [0_0] Initializing state stores
2017-06-12 17:08:32 INFO  StreamTask:333 - task [0_0] Initializing processor nodes of the topology
2017-06-12 17:08:32 INFO  StreamThread:858 - stream-thread [StreamThread-1] Creating active task 1_0 with assigned partitions [wordcount-lambda-example-Counts-repartition-0]
2017-06-12 17:08:32 INFO  StreamTask:140 - task [1_0] Initializing state stores
2017-06-12 17:08:32 INFO  StreamTask:333 - task [1_0] Initializing processor nodes of the topology
2017-06-12 17:08:32 INFO  StreamThread:163 - stream-thread [StreamThread-1] State transition from ASSIGNING_PARTITIONS to RUNNING.
2017-06-12 17:08:32 INFO  KafkaStreams:224 - stream-client [wordcount-lambda-example-67e9b407-0a64-4b07-a768-429967fbd29b] State transition from REBALANCING to RUNNING.
2017-06-12 17:09:01 INFO  StreamThread:767 - stream-thread [StreamThread-1] Committing all tasks because the commit interval 30000ms has elapsed
2017-06-12 17:09:01 INFO  StreamThread:805 - stream-thread [StreamThread-1] Committing task StreamTask 0_0
2017-06-12 17:09:01 INFO  StreamThread:805 - stream-thread [StreamThread-1] Committing task StreamTask 1_0
2017-06-12 17:09:15 INFO  App:43 - value mbc
2017-06-12 17:09:21 INFO  App:54 - groupBy mbc
2017-06-12 17:09:31 INFO  StreamThread:767 - stream-thread [StreamThread-1] Committing all tasks because the commit interval 30000ms has elapsed
2017-06-12 17:09:31 INFO  StreamThread:805 - stream-thread [StreamThread-1] Committing task StreamTask 0_0
2017-06-12 17:09:31 INFO  StreamThread:805 - stream-thread [StreamThread-1] Committing task StreamTask 1_0
2017-06-12 17:09:31 INFO  App:96 - offset 52 key mbc value 6
2017-06-12 17:10:01 INFO  StreamThread:767 - stream-thread [StreamThread-1] Committing all tasks because the commit interval 30000ms has elapsed
2017-06-12 17:10:01 INFO  StreamThread:805 - stream-thread [StreamThread-1] Committing task StreamTask 0_0
2017-06-12 17:10:01 INFO  StreamThread:805 - stream-thread [StreamThread-1] Committing task StreamTask 1_0
2017-06-12 17:10:48 INFO  StreamsConfig:180 - StreamsConfig values: 
	application.id = wordcount-lambda-example
	application.server = 
	bootstrap.servers = [localhost:9092]
	buffered.records.per.partition = 1000
	cache.max.bytes.buffering = 10485760
	client.id = 
	commit.interval.ms = 30000
	connections.max.idle.ms = 540000
	key.serde = class org.apache.kafka.common.serialization.Serdes$StringSerde
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	num.standby.replicas = 0
	num.stream.threads = 1
	partition.grouper = class org.apache.kafka.streams.processor.DefaultPartitionGrouper
	poll.ms = 100
	receive.buffer.bytes = 32768
	reconnect.backoff.ms = 50
	replication.factor = 1
	request.timeout.ms = 40000
	retry.backoff.ms = 100
	rocksdb.config.setter = null
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	state.cleanup.delay.ms = 60000
	state.dir = /tmp/kafka-streams
	timestamp.extractor = class org.apache.kafka.streams.processor.FailOnInvalidTimestamp
	value.serde = class org.apache.kafka.common.serialization.Serdes$StringSerde
	windowstore.changelog.additional.retention.ms = 86400000
	zookeeper.connect = 

2017-06-12 17:10:50 INFO  ConsumerConfig:180 - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.LongDeserializer

2017-06-12 17:10:50 INFO  AppInfoParser:83 - Kafka version : 0.10.2.1
2017-06-12 17:10:50 INFO  AppInfoParser:84 - Kafka commitId : e89bffd6b2eff799
2017-06-12 17:10:50 INFO  AbstractCoordinator:586 - Discovered coordinator tmon-jhkwon78-n.tmoncorp.com:9092 (id: 2147483647 rack: null) for group test.
2017-06-12 17:10:50 INFO  ConsumerCoordinator:397 - Revoking previously assigned partitions [] for group test
2017-06-12 17:10:50 INFO  AbstractCoordinator:420 - (Re-)joining group test
2017-06-12 17:10:50 INFO  AbstractCoordinator:388 - Successfully joined group test with generation 53
2017-06-12 17:10:50 INFO  ConsumerCoordinator:256 - Setting newly assigned partitions [WordsWithCountsTopic-0] for group test
2017-06-12 17:10:51 INFO  StreamThread:304 - stream-thread [StreamThread-1] Creating producer client
2017-06-12 17:10:51 INFO  ProducerConfig:180 - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	block.on.buffer.full = false
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = wordcount-lambda-example-23259ba9-dad1-4f58-91f0-1190774574c5-StreamThread-1-producer
	compression.type = none
	connections.max.idle.ms = 540000
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 100
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.fetch.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	timeout.ms = 30000
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2017-06-12 17:10:51 INFO  AppInfoParser:83 - Kafka version : 0.10.2.1
2017-06-12 17:10:51 INFO  AppInfoParser:84 - Kafka commitId : e89bffd6b2eff799
2017-06-12 17:10:51 INFO  StreamThread:306 - stream-thread [StreamThread-1] Creating consumer client
2017-06-12 17:10:51 INFO  ConsumerConfig:180 - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = wordcount-lambda-example-23259ba9-dad1-4f58-91f0-1190774574c5-StreamThread-1-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = wordcount-lambda-example
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 2147483647
	max.poll.records = 1000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamPartitionAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2017-06-12 17:10:51 INFO  AppInfoParser:83 - Kafka version : 0.10.2.1
2017-06-12 17:10:51 INFO  AppInfoParser:84 - Kafka commitId : e89bffd6b2eff799
2017-06-12 17:10:51 INFO  StreamThread:317 - stream-thread [StreamThread-1] Creating restore consumer client
2017-06-12 17:10:51 INFO  ConsumerConfig:180 - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = wordcount-lambda-example-23259ba9-dad1-4f58-91f0-1190774574c5-StreamThread-1-restore-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 2147483647
	max.poll.records = 1000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2017-06-12 17:10:51 INFO  AppInfoParser:83 - Kafka version : 0.10.2.1
2017-06-12 17:10:51 INFO  AppInfoParser:84 - Kafka commitId : e89bffd6b2eff799
2017-06-12 17:10:51 INFO  StreamThread:163 - stream-thread [StreamThread-1] State transition from NOT_RUNNING to RUNNING.
2017-06-12 17:10:51 INFO  KafkaStreams:224 - stream-client [wordcount-lambda-example-23259ba9-dad1-4f58-91f0-1190774574c5] State transition from CREATED to RUNNING.
2017-06-12 17:10:51 INFO  KafkaStreams:436 - stream-client [wordcount-lambda-example-23259ba9-dad1-4f58-91f0-1190774574c5] Started Kafka Stream process
2017-06-12 17:10:51 INFO  StreamThread:358 - stream-thread [StreamThread-1] Starting
2017-06-12 17:10:51 INFO  AbstractCoordinator:586 - Discovered coordinator tmon-jhkwon78-n.tmoncorp.com:9092 (id: 2147483647 rack: null) for group wordcount-lambda-example.
2017-06-12 17:10:51 INFO  ConsumerCoordinator:397 - Revoking previously assigned partitions [] for group wordcount-lambda-example
2017-06-12 17:10:51 INFO  StreamThread:248 - stream-thread [StreamThread-1] at state RUNNING: partitions [] revoked at the beginning of consumer rebalance.
2017-06-12 17:10:51 INFO  StreamThread:163 - stream-thread [StreamThread-1] State transition from RUNNING to PARTITIONS_REVOKED.
2017-06-12 17:10:51 INFO  KafkaStreams:224 - stream-client [wordcount-lambda-example-23259ba9-dad1-4f58-91f0-1190774574c5] State transition from RUNNING to REBALANCING.
2017-06-12 17:10:51 INFO  StreamThread:1042 - stream-thread [StreamThread-1] Updating suspended tasks to contain active tasks []
2017-06-12 17:10:51 INFO  StreamThread:1049 - stream-thread [StreamThread-1] Removing all active tasks []
2017-06-12 17:10:51 INFO  StreamThread:1064 - stream-thread [StreamThread-1] Removing all standby tasks []
2017-06-12 17:10:51 INFO  AbstractCoordinator:420 - (Re-)joining group wordcount-lambda-example
2017-06-12 17:10:51 INFO  StreamPartitionAssignor:300 - stream-thread [StreamThread-1] Constructed client metadata {23259ba9-dad1-4f58-91f0-1190774574c5=ClientMetadata{hostInfo=null, consumers=[wordcount-lambda-example-23259ba9-dad1-4f58-91f0-1190774574c5-StreamThread-1-consumer-a7443fa9-bb40-4b06-bb9e-b29bb8845584], state=[activeTasks: ([]) assignedTasks: ([]) prevActiveTasks: ([]) prevAssignedTasks: ([]) capacity: 1.0 cost: 0.0]}} from the member subscriptions.
2017-06-12 17:10:51 INFO  StreamPartitionAssignor:640 - stream-thread [StreamThread-1] Completed validating internal topics in partition assignor
2017-06-12 17:10:52 INFO  StreamPartitionAssignor:640 - stream-thread [StreamThread-1] Completed validating internal topics in partition assignor
2017-06-12 17:10:52 INFO  StreamPartitionAssignor:476 - stream-thread [StreamThread-1] Assigned tasks to clients as {23259ba9-dad1-4f58-91f0-1190774574c5=[activeTasks: ([0_0, 1_0]) assignedTasks: ([0_0, 1_0]) prevActiveTasks: ([]) prevAssignedTasks: ([]) capacity: 1.0 cost: 1.0]}.
2017-06-12 17:10:52 INFO  AbstractCoordinator:388 - Successfully joined group wordcount-lambda-example with generation 43
2017-06-12 17:10:52 INFO  ConsumerCoordinator:256 - Setting newly assigned partitions [TextLinesTopic-0, wordcount-lambda-example-Counts-repartition-0] for group wordcount-lambda-example
2017-06-12 17:10:52 INFO  StreamThread:228 - stream-thread [StreamThread-1] at state PARTITIONS_REVOKED: new partitions [TextLinesTopic-0, wordcount-lambda-example-Counts-repartition-0] assigned at the end of consumer rebalance.
2017-06-12 17:10:52 INFO  StreamThread:163 - stream-thread [StreamThread-1] State transition from PARTITIONS_REVOKED to ASSIGNING_PARTITIONS.
2017-06-12 17:10:52 INFO  KafkaStreams:224 - stream-client [wordcount-lambda-example-23259ba9-dad1-4f58-91f0-1190774574c5] State transition from REBALANCING to REBALANCING.
2017-06-12 17:10:52 INFO  StreamThread:858 - stream-thread [StreamThread-1] Creating active task 0_0 with assigned partitions [TextLinesTopic-0]
2017-06-12 17:10:52 INFO  StreamTask:140 - task [0_0] Initializing state stores
2017-06-12 17:10:52 INFO  StreamTask:333 - task [0_0] Initializing processor nodes of the topology
2017-06-12 17:10:52 INFO  StreamThread:858 - stream-thread [StreamThread-1] Creating active task 1_0 with assigned partitions [wordcount-lambda-example-Counts-repartition-0]
2017-06-12 17:10:52 INFO  StreamTask:140 - task [1_0] Initializing state stores
2017-06-12 17:10:52 INFO  StreamTask:333 - task [1_0] Initializing processor nodes of the topology
2017-06-12 17:10:52 INFO  StreamThread:163 - stream-thread [StreamThread-1] State transition from ASSIGNING_PARTITIONS to RUNNING.
2017-06-12 17:10:52 INFO  KafkaStreams:224 - stream-client [wordcount-lambda-example-23259ba9-dad1-4f58-91f0-1190774574c5] State transition from REBALANCING to RUNNING.
2017-06-12 17:11:21 INFO  StreamThread:767 - stream-thread [StreamThread-1] Committing all tasks because the commit interval 30000ms has elapsed
2017-06-12 17:11:21 INFO  StreamThread:805 - stream-thread [StreamThread-1] Committing task StreamTask 0_0
2017-06-12 17:11:21 INFO  StreamThread:805 - stream-thread [StreamThread-1] Committing task StreamTask 1_0
2017-06-12 17:11:48 INFO  StreamsConfig:180 - StreamsConfig values: 
	application.id = wordcount-lambda-example
	application.server = 
	bootstrap.servers = [localhost:9092]
	buffered.records.per.partition = 1000
	cache.max.bytes.buffering = 10485760
	client.id = 
	commit.interval.ms = 30000
	connections.max.idle.ms = 540000
	key.serde = class org.apache.kafka.common.serialization.Serdes$StringSerde
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	num.standby.replicas = 0
	num.stream.threads = 1
	partition.grouper = class org.apache.kafka.streams.processor.DefaultPartitionGrouper
	poll.ms = 100
	receive.buffer.bytes = 32768
	reconnect.backoff.ms = 50
	replication.factor = 1
	request.timeout.ms = 40000
	retry.backoff.ms = 100
	rocksdb.config.setter = null
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	state.cleanup.delay.ms = 60000
	state.dir = /tmp/kafka-streams
	timestamp.extractor = class org.apache.kafka.streams.processor.FailOnInvalidTimestamp
	value.serde = class org.apache.kafka.common.serialization.Serdes$StringSerde
	windowstore.changelog.additional.retention.ms = 86400000
	zookeeper.connect = 

2017-06-12 17:11:50 INFO  ConsumerConfig:180 - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.LongDeserializer

2017-06-12 17:11:50 INFO  AppInfoParser:83 - Kafka version : 0.10.2.1
2017-06-12 17:11:50 INFO  AppInfoParser:84 - Kafka commitId : e89bffd6b2eff799
2017-06-12 17:11:51 INFO  AbstractCoordinator:586 - Discovered coordinator tmon-jhkwon78-n.tmoncorp.com:9092 (id: 2147483647 rack: null) for group test.
2017-06-12 17:11:51 INFO  ConsumerCoordinator:397 - Revoking previously assigned partitions [] for group test
2017-06-12 17:11:51 INFO  AbstractCoordinator:420 - (Re-)joining group test
2017-06-12 17:11:51 INFO  AbstractCoordinator:388 - Successfully joined group test with generation 55
2017-06-12 17:11:51 INFO  ConsumerCoordinator:256 - Setting newly assigned partitions [WordsWithCountsTopic-0] for group test
2017-06-12 17:11:51 INFO  StreamThread:304 - stream-thread [StreamThread-1] Creating producer client
2017-06-12 17:11:51 INFO  ProducerConfig:180 - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	block.on.buffer.full = false
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = wordcount-lambda-example-a495f187-0be0-40e9-9c9f-ab07a6a84996-StreamThread-1-producer
	compression.type = none
	connections.max.idle.ms = 540000
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 100
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.fetch.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	timeout.ms = 30000
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2017-06-12 17:11:51 INFO  AppInfoParser:83 - Kafka version : 0.10.2.1
2017-06-12 17:11:51 INFO  AppInfoParser:84 - Kafka commitId : e89bffd6b2eff799
2017-06-12 17:11:51 INFO  StreamThread:306 - stream-thread [StreamThread-1] Creating consumer client
2017-06-12 17:11:51 INFO  ConsumerConfig:180 - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = wordcount-lambda-example-a495f187-0be0-40e9-9c9f-ab07a6a84996-StreamThread-1-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = wordcount-lambda-example
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 2147483647
	max.poll.records = 1000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamPartitionAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2017-06-12 17:11:51 INFO  AppInfoParser:83 - Kafka version : 0.10.2.1
2017-06-12 17:11:51 INFO  AppInfoParser:84 - Kafka commitId : e89bffd6b2eff799
2017-06-12 17:11:51 INFO  StreamThread:317 - stream-thread [StreamThread-1] Creating restore consumer client
2017-06-12 17:11:51 INFO  ConsumerConfig:180 - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = wordcount-lambda-example-a495f187-0be0-40e9-9c9f-ab07a6a84996-StreamThread-1-restore-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 2147483647
	max.poll.records = 1000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2017-06-12 17:11:51 INFO  AppInfoParser:83 - Kafka version : 0.10.2.1
2017-06-12 17:11:51 INFO  AppInfoParser:84 - Kafka commitId : e89bffd6b2eff799
2017-06-12 17:11:51 INFO  StreamThread:163 - stream-thread [StreamThread-1] State transition from NOT_RUNNING to RUNNING.
2017-06-12 17:11:51 INFO  KafkaStreams:224 - stream-client [wordcount-lambda-example-a495f187-0be0-40e9-9c9f-ab07a6a84996] State transition from CREATED to RUNNING.
2017-06-12 17:11:51 INFO  KafkaStreams:436 - stream-client [wordcount-lambda-example-a495f187-0be0-40e9-9c9f-ab07a6a84996] Started Kafka Stream process
2017-06-12 17:11:51 INFO  StreamThread:358 - stream-thread [StreamThread-1] Starting
2017-06-12 17:11:52 INFO  AbstractCoordinator:586 - Discovered coordinator tmon-jhkwon78-n.tmoncorp.com:9092 (id: 2147483647 rack: null) for group wordcount-lambda-example.
2017-06-12 17:11:52 INFO  ConsumerCoordinator:397 - Revoking previously assigned partitions [] for group wordcount-lambda-example
2017-06-12 17:11:52 INFO  StreamThread:248 - stream-thread [StreamThread-1] at state RUNNING: partitions [] revoked at the beginning of consumer rebalance.
2017-06-12 17:11:52 INFO  StreamThread:163 - stream-thread [StreamThread-1] State transition from RUNNING to PARTITIONS_REVOKED.
2017-06-12 17:11:52 INFO  KafkaStreams:224 - stream-client [wordcount-lambda-example-a495f187-0be0-40e9-9c9f-ab07a6a84996] State transition from RUNNING to REBALANCING.
2017-06-12 17:11:52 INFO  StreamThread:1042 - stream-thread [StreamThread-1] Updating suspended tasks to contain active tasks []
2017-06-12 17:11:52 INFO  StreamThread:1049 - stream-thread [StreamThread-1] Removing all active tasks []
2017-06-12 17:11:52 INFO  StreamThread:1064 - stream-thread [StreamThread-1] Removing all standby tasks []
2017-06-12 17:11:52 INFO  AbstractCoordinator:420 - (Re-)joining group wordcount-lambda-example
2017-06-12 17:11:52 INFO  StreamPartitionAssignor:300 - stream-thread [StreamThread-1] Constructed client metadata {a495f187-0be0-40e9-9c9f-ab07a6a84996=ClientMetadata{hostInfo=null, consumers=[wordcount-lambda-example-a495f187-0be0-40e9-9c9f-ab07a6a84996-StreamThread-1-consumer-213e40af-a9c9-49b7-8a00-013b1cfa4045], state=[activeTasks: ([]) assignedTasks: ([]) prevActiveTasks: ([]) prevAssignedTasks: ([]) capacity: 1.0 cost: 0.0]}} from the member subscriptions.
2017-06-12 17:11:52 INFO  StreamPartitionAssignor:640 - stream-thread [StreamThread-1] Completed validating internal topics in partition assignor
2017-06-12 17:11:52 INFO  StreamPartitionAssignor:640 - stream-thread [StreamThread-1] Completed validating internal topics in partition assignor
2017-06-12 17:11:52 INFO  StreamPartitionAssignor:476 - stream-thread [StreamThread-1] Assigned tasks to clients as {a495f187-0be0-40e9-9c9f-ab07a6a84996=[activeTasks: ([0_0, 1_0]) assignedTasks: ([0_0, 1_0]) prevActiveTasks: ([]) prevAssignedTasks: ([]) capacity: 1.0 cost: 1.0]}.
2017-06-12 17:11:52 INFO  AbstractCoordinator:388 - Successfully joined group wordcount-lambda-example with generation 45
2017-06-12 17:11:52 INFO  ConsumerCoordinator:256 - Setting newly assigned partitions [TextLinesTopic-0, wordcount-lambda-example-Counts-repartition-0] for group wordcount-lambda-example
2017-06-12 17:11:52 INFO  StreamThread:228 - stream-thread [StreamThread-1] at state PARTITIONS_REVOKED: new partitions [TextLinesTopic-0, wordcount-lambda-example-Counts-repartition-0] assigned at the end of consumer rebalance.
2017-06-12 17:11:52 INFO  StreamThread:163 - stream-thread [StreamThread-1] State transition from PARTITIONS_REVOKED to ASSIGNING_PARTITIONS.
2017-06-12 17:11:52 INFO  KafkaStreams:224 - stream-client [wordcount-lambda-example-a495f187-0be0-40e9-9c9f-ab07a6a84996] State transition from REBALANCING to REBALANCING.
2017-06-12 17:11:52 INFO  StreamThread:858 - stream-thread [StreamThread-1] Creating active task 0_0 with assigned partitions [TextLinesTopic-0]
2017-06-12 17:11:52 INFO  StreamTask:140 - task [0_0] Initializing state stores
2017-06-12 17:11:52 INFO  StreamTask:333 - task [0_0] Initializing processor nodes of the topology
2017-06-12 17:11:52 INFO  StreamThread:858 - stream-thread [StreamThread-1] Creating active task 1_0 with assigned partitions [wordcount-lambda-example-Counts-repartition-0]
2017-06-12 17:11:52 INFO  StreamTask:140 - task [1_0] Initializing state stores
2017-06-12 17:11:53 INFO  StreamTask:333 - task [1_0] Initializing processor nodes of the topology
2017-06-12 17:11:53 INFO  StreamThread:163 - stream-thread [StreamThread-1] State transition from ASSIGNING_PARTITIONS to RUNNING.
2017-06-12 17:11:53 INFO  KafkaStreams:224 - stream-client [wordcount-lambda-example-a495f187-0be0-40e9-9c9f-ab07a6a84996] State transition from REBALANCING to RUNNING.
2017-06-12 17:11:57 INFO  App:67 - close kafka stream
2017-06-12 17:11:57 INFO  KafkaStreams:224 - stream-client [wordcount-lambda-example-a495f187-0be0-40e9-9c9f-ab07a6a84996] State transition from RUNNING to PENDING_SHUTDOWN.
2017-06-12 17:11:57 INFO  StreamThread:380 - stream-thread [StreamThread-1] Informed thread to shut down
2017-06-12 17:11:57 INFO  StreamThread:163 - stream-thread [StreamThread-1] State transition from RUNNING to PENDING_SHUTDOWN.
2017-06-12 17:11:57 INFO  StreamThread:652 - stream-thread [StreamThread-1] Shutting down at user request
2017-06-12 17:11:57 INFO  StreamThread:390 - stream-thread [StreamThread-1] Shutting down
2017-06-12 17:11:57 INFO  StreamThread:1075 - stream-thread [StreamThread-1] Closing task 0_0
2017-06-12 17:11:57 INFO  StreamThread:1075 - stream-thread [StreamThread-1] Closing task 1_0
2017-06-12 17:11:57 INFO  StreamThread:554 - stream-thread [StreamThread-1] Flushing state stores of task 0_0
2017-06-12 17:11:57 INFO  StreamThread:554 - stream-thread [StreamThread-1] Flushing state stores of task 1_0
2017-06-12 17:11:57 INFO  StreamThread:533 - stream-thread [StreamThread-1] Closing the state manager of task 0_0
2017-06-12 17:11:57 INFO  StreamThread:533 - stream-thread [StreamThread-1] Closing the state manager of task 1_0
2017-06-12 17:11:57 INFO  StreamThread:544 - stream-thread [StreamThread-1] Committing consumer offsets of task 0_0
2017-06-12 17:11:57 INFO  StreamThread:544 - stream-thread [StreamThread-1] Committing consumer offsets of task 1_0
2017-06-12 17:11:57 INFO  KafkaProducer:689 - Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2017-06-12 17:11:57 INFO  StreamThread:1049 - stream-thread [StreamThread-1] Removing all active tasks [0_0, 1_0]
2017-06-12 17:11:57 INFO  StreamThread:1064 - stream-thread [StreamThread-1] Removing all standby tasks []
2017-06-12 17:11:57 INFO  StreamThread:420 - stream-thread [StreamThread-1] Stream thread shutdown complete
2017-06-12 17:11:57 INFO  StreamThread:163 - stream-thread [StreamThread-1] State transition from PENDING_SHUTDOWN to NOT_RUNNING.
2017-06-12 17:11:57 INFO  KafkaStreams:497 - stream-client [wordcount-lambda-example-a495f187-0be0-40e9-9c9f-ab07a6a84996] Stopped Kafka Streams process.
2017-06-12 17:11:57 INFO  KafkaStreams:224 - stream-client [wordcount-lambda-example-a495f187-0be0-40e9-9c9f-ab07a6a84996] State transition from PENDING_SHUTDOWN to NOT_RUNNING.
2017-06-12 17:28:57 INFO  StreamsConfig:180 - StreamsConfig values: 
	application.id = wordcount-lambda-example
	application.server = 
	bootstrap.servers = [localhost:9092]
	buffered.records.per.partition = 1000
	cache.max.bytes.buffering = 10485760
	client.id = 
	commit.interval.ms = 30000
	connections.max.idle.ms = 540000
	key.serde = class org.apache.kafka.common.serialization.Serdes$StringSerde
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	num.standby.replicas = 0
	num.stream.threads = 1
	partition.grouper = class org.apache.kafka.streams.processor.DefaultPartitionGrouper
	poll.ms = 100
	receive.buffer.bytes = 32768
	reconnect.backoff.ms = 50
	replication.factor = 1
	request.timeout.ms = 40000
	retry.backoff.ms = 100
	rocksdb.config.setter = null
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	state.cleanup.delay.ms = 60000
	state.dir = /tmp/kafka-streams
	timestamp.extractor = class org.apache.kafka.streams.processor.FailOnInvalidTimestamp
	value.serde = class org.apache.kafka.common.serialization.Serdes$StringSerde
	windowstore.changelog.additional.retention.ms = 86400000
	zookeeper.connect = 

2017-06-12 17:28:59 INFO  ConsumerConfig:180 - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.LongDeserializer

2017-06-12 17:28:59 INFO  AppInfoParser:83 - Kafka version : 0.10.2.1
2017-06-12 17:28:59 INFO  AppInfoParser:84 - Kafka commitId : e89bffd6b2eff799
2017-06-12 17:29:00 INFO  AbstractCoordinator:586 - Discovered coordinator tmon-jhkwon78-n.tmoncorp.com:9092 (id: 2147483647 rack: null) for group test.
2017-06-12 17:29:00 INFO  ConsumerCoordinator:397 - Revoking previously assigned partitions [] for group test
2017-06-12 17:29:00 INFO  AbstractCoordinator:420 - (Re-)joining group test
2017-06-12 17:29:00 INFO  AbstractCoordinator:388 - Successfully joined group test with generation 57
2017-06-12 17:29:00 INFO  ConsumerCoordinator:256 - Setting newly assigned partitions [WordsWithCountsTopic-0] for group test
2017-06-12 17:29:01 INFO  StreamThread:304 - stream-thread [StreamThread-1] Creating producer client
2017-06-12 17:29:01 INFO  ProducerConfig:180 - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	block.on.buffer.full = false
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = wordcount-lambda-example-39f47dd8-b5d5-43a8-aa15-0c075634087f-StreamThread-1-producer
	compression.type = none
	connections.max.idle.ms = 540000
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 100
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.fetch.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	timeout.ms = 30000
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2017-06-12 17:29:01 INFO  AppInfoParser:83 - Kafka version : 0.10.2.1
2017-06-12 17:29:01 INFO  AppInfoParser:84 - Kafka commitId : e89bffd6b2eff799
2017-06-12 17:29:01 INFO  StreamThread:306 - stream-thread [StreamThread-1] Creating consumer client
2017-06-12 17:29:02 INFO  ConsumerConfig:180 - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = wordcount-lambda-example-39f47dd8-b5d5-43a8-aa15-0c075634087f-StreamThread-1-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = wordcount-lambda-example
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 2147483647
	max.poll.records = 1000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamPartitionAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2017-06-12 17:29:02 INFO  AppInfoParser:83 - Kafka version : 0.10.2.1
2017-06-12 17:29:02 INFO  AppInfoParser:84 - Kafka commitId : e89bffd6b2eff799
2017-06-12 17:29:02 INFO  StreamThread:317 - stream-thread [StreamThread-1] Creating restore consumer client
2017-06-12 17:29:02 INFO  ConsumerConfig:180 - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = wordcount-lambda-example-39f47dd8-b5d5-43a8-aa15-0c075634087f-StreamThread-1-restore-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 2147483647
	max.poll.records = 1000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2017-06-12 17:29:02 INFO  AppInfoParser:83 - Kafka version : 0.10.2.1
2017-06-12 17:29:02 INFO  AppInfoParser:84 - Kafka commitId : e89bffd6b2eff799
2017-06-12 17:29:02 INFO  StreamThread:163 - stream-thread [StreamThread-1] State transition from NOT_RUNNING to RUNNING.
2017-06-12 17:29:02 INFO  KafkaStreams:224 - stream-client [wordcount-lambda-example-39f47dd8-b5d5-43a8-aa15-0c075634087f] State transition from CREATED to RUNNING.
2017-06-12 17:29:02 INFO  KafkaStreams:436 - stream-client [wordcount-lambda-example-39f47dd8-b5d5-43a8-aa15-0c075634087f] Started Kafka Stream process
2017-06-12 17:29:02 INFO  StreamThread:358 - stream-thread [StreamThread-1] Starting
2017-06-12 17:29:02 INFO  AbstractCoordinator:586 - Discovered coordinator tmon-jhkwon78-n.tmoncorp.com:9092 (id: 2147483647 rack: null) for group wordcount-lambda-example.
2017-06-12 17:29:02 INFO  ConsumerCoordinator:397 - Revoking previously assigned partitions [] for group wordcount-lambda-example
2017-06-12 17:29:02 INFO  StreamThread:248 - stream-thread [StreamThread-1] at state RUNNING: partitions [] revoked at the beginning of consumer rebalance.
2017-06-12 17:29:02 INFO  StreamThread:163 - stream-thread [StreamThread-1] State transition from RUNNING to PARTITIONS_REVOKED.
2017-06-12 17:29:02 INFO  KafkaStreams:224 - stream-client [wordcount-lambda-example-39f47dd8-b5d5-43a8-aa15-0c075634087f] State transition from RUNNING to REBALANCING.
2017-06-12 17:29:02 INFO  StreamThread:1042 - stream-thread [StreamThread-1] Updating suspended tasks to contain active tasks []
2017-06-12 17:29:02 INFO  StreamThread:1049 - stream-thread [StreamThread-1] Removing all active tasks []
2017-06-12 17:29:02 INFO  StreamThread:1064 - stream-thread [StreamThread-1] Removing all standby tasks []
2017-06-12 17:29:02 INFO  AbstractCoordinator:420 - (Re-)joining group wordcount-lambda-example
2017-06-12 17:29:02 INFO  StreamPartitionAssignor:300 - stream-thread [StreamThread-1] Constructed client metadata {39f47dd8-b5d5-43a8-aa15-0c075634087f=ClientMetadata{hostInfo=null, consumers=[wordcount-lambda-example-39f47dd8-b5d5-43a8-aa15-0c075634087f-StreamThread-1-consumer-dec17e28-27c4-4bf1-8be8-f4add828dcd2], state=[activeTasks: ([]) assignedTasks: ([]) prevActiveTasks: ([]) prevAssignedTasks: ([1_0]) capacity: 1.0 cost: 0.0]}} from the member subscriptions.
2017-06-12 17:29:02 INFO  StreamPartitionAssignor:640 - stream-thread [StreamThread-1] Completed validating internal topics in partition assignor
2017-06-12 17:29:03 INFO  StreamPartitionAssignor:640 - stream-thread [StreamThread-1] Completed validating internal topics in partition assignor
2017-06-12 17:29:03 INFO  StreamPartitionAssignor:476 - stream-thread [StreamThread-1] Assigned tasks to clients as {39f47dd8-b5d5-43a8-aa15-0c075634087f=[activeTasks: ([0_0, 1_0]) assignedTasks: ([0_0, 1_0]) prevActiveTasks: ([]) prevAssignedTasks: ([]) capacity: 1.0 cost: 0.7]}.
2017-06-12 17:29:03 INFO  AbstractCoordinator:388 - Successfully joined group wordcount-lambda-example with generation 47
2017-06-12 17:29:03 INFO  ConsumerCoordinator:256 - Setting newly assigned partitions [TextLinesTopic-0, wordcount-lambda-example-Counts-repartition-0] for group wordcount-lambda-example
2017-06-12 17:29:03 INFO  StreamThread:228 - stream-thread [StreamThread-1] at state PARTITIONS_REVOKED: new partitions [TextLinesTopic-0, wordcount-lambda-example-Counts-repartition-0] assigned at the end of consumer rebalance.
2017-06-12 17:29:03 INFO  StreamThread:163 - stream-thread [StreamThread-1] State transition from PARTITIONS_REVOKED to ASSIGNING_PARTITIONS.
2017-06-12 17:29:03 INFO  KafkaStreams:224 - stream-client [wordcount-lambda-example-39f47dd8-b5d5-43a8-aa15-0c075634087f] State transition from REBALANCING to REBALANCING.
2017-06-12 17:29:03 INFO  StreamThread:858 - stream-thread [StreamThread-1] Creating active task 0_0 with assigned partitions [TextLinesTopic-0]
2017-06-12 17:29:03 INFO  StreamTask:140 - task [0_0] Initializing state stores
2017-06-12 17:29:03 INFO  StreamTask:333 - task [0_0] Initializing processor nodes of the topology
2017-06-12 17:29:03 INFO  StreamThread:858 - stream-thread [StreamThread-1] Creating active task 1_0 with assigned partitions [wordcount-lambda-example-Counts-repartition-0]
2017-06-12 17:29:03 INFO  StreamTask:140 - task [1_0] Initializing state stores
2017-06-12 17:29:03 INFO  StreamTask:333 - task [1_0] Initializing processor nodes of the topology
2017-06-12 17:29:03 INFO  StreamThread:163 - stream-thread [StreamThread-1] State transition from ASSIGNING_PARTITIONS to RUNNING.
2017-06-12 17:29:03 INFO  KafkaStreams:224 - stream-client [wordcount-lambda-example-39f47dd8-b5d5-43a8-aa15-0c075634087f] State transition from REBALANCING to RUNNING.
2017-06-12 17:29:32 INFO  StreamThread:767 - stream-thread [StreamThread-1] Committing all tasks because the commit interval 30000ms has elapsed
2017-06-12 17:29:32 INFO  StreamThread:805 - stream-thread [StreamThread-1] Committing task StreamTask 0_0
2017-06-12 17:29:32 INFO  StreamThread:805 - stream-thread [StreamThread-1] Committing task StreamTask 1_0
2017-06-12 17:29:50 INFO  App:45 - value sbs
2017-06-12 17:29:50 INFO  App:56 - groupBy sbs
2017-06-12 17:29:52 INFO  App:45 - value ebs
2017-06-12 17:29:52 INFO  App:56 - groupBy ebs
2017-06-12 17:30:01 INFO  App:45 - value hello
2017-06-12 17:30:01 INFO  App:56 - groupBy hello
2017-06-12 17:30:02 INFO  StreamThread:767 - stream-thread [StreamThread-1] Committing all tasks because the commit interval 30000ms has elapsed
2017-06-12 17:30:02 INFO  StreamThread:805 - stream-thread [StreamThread-1] Committing task StreamTask 0_0
2017-06-12 17:30:02 INFO  StreamThread:805 - stream-thread [StreamThread-1] Committing task StreamTask 1_0
2017-06-12 17:30:02 INFO  App:98 - offset 53 key sbs value 3
2017-06-12 17:30:02 INFO  App:98 - offset 54 key ebs value 4
2017-06-12 17:30:02 INFO  App:98 - offset 55 key hello value 21
2017-06-14 11:50:47 INFO  StreamsConfig:180 - StreamsConfig values: 
	application.id = mapapp-example
	application.server = 
	bootstrap.servers = [localhost:9092]
	buffered.records.per.partition = 1000
	cache.max.bytes.buffering = 10485760
	client.id = 
	commit.interval.ms = 30000
	connections.max.idle.ms = 540000
	key.serde = class org.apache.kafka.common.serialization.Serdes$StringSerde
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	num.standby.replicas = 0
	num.stream.threads = 1
	partition.grouper = class org.apache.kafka.streams.processor.DefaultPartitionGrouper
	poll.ms = 100
	receive.buffer.bytes = 32768
	reconnect.backoff.ms = 50
	replication.factor = 1
	request.timeout.ms = 40000
	retry.backoff.ms = 100
	rocksdb.config.setter = null
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	state.cleanup.delay.ms = 60000
	state.dir = /tmp/kafka-streams
	timestamp.extractor = class org.apache.kafka.streams.processor.FailOnInvalidTimestamp
	value.serde = class org.apache.kafka.common.serialization.Serdes$StringSerde
	windowstore.changelog.additional.retention.ms = 86400000
	zookeeper.connect = 

2017-06-14 11:50:49 INFO  ConsumerConfig:180 - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2017-06-14 11:50:49 INFO  AppInfoParser:83 - Kafka version : 0.10.2.1
2017-06-14 11:50:49 INFO  AppInfoParser:84 - Kafka commitId : e89bffd6b2eff799
2017-06-14 11:50:49 INFO  AbstractCoordinator:586 - Discovered coordinator tmon-jhkwon78-n.tmoncorp.com:9092 (id: 2147483647 rack: null) for group test.
2017-06-14 11:50:49 INFO  ConsumerCoordinator:397 - Revoking previously assigned partitions [] for group test
2017-06-14 11:50:49 INFO  AbstractCoordinator:420 - (Re-)joining group test
2017-06-14 11:50:49 INFO  AbstractCoordinator:388 - Successfully joined group test with generation 1
2017-06-14 11:50:49 INFO  ConsumerCoordinator:256 - Setting newly assigned partitions [UpperMapAppTopic-0] for group test
2017-06-14 11:50:54 INFO  StreamThread:304 - stream-thread [StreamThread-1] Creating producer client
2017-06-14 11:50:54 INFO  ProducerConfig:180 - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	block.on.buffer.full = false
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = mapapp-example-63a0cb89-28bc-4b5f-bbc6-d7d0bf17d756-StreamThread-1-producer
	compression.type = none
	connections.max.idle.ms = 540000
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 100
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.fetch.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	timeout.ms = 30000
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2017-06-14 11:50:54 INFO  AppInfoParser:83 - Kafka version : 0.10.2.1
2017-06-14 11:50:54 INFO  AppInfoParser:84 - Kafka commitId : e89bffd6b2eff799
2017-06-14 11:50:54 INFO  StreamThread:306 - stream-thread [StreamThread-1] Creating consumer client
2017-06-14 11:50:54 INFO  ConsumerConfig:180 - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = mapapp-example-63a0cb89-28bc-4b5f-bbc6-d7d0bf17d756-StreamThread-1-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = mapapp-example
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 2147483647
	max.poll.records = 1000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamPartitionAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2017-06-14 11:50:54 INFO  AppInfoParser:83 - Kafka version : 0.10.2.1
2017-06-14 11:50:54 INFO  AppInfoParser:84 - Kafka commitId : e89bffd6b2eff799
2017-06-14 11:50:54 INFO  StreamThread:317 - stream-thread [StreamThread-1] Creating restore consumer client
2017-06-14 11:50:54 INFO  ConsumerConfig:180 - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = mapapp-example-63a0cb89-28bc-4b5f-bbc6-d7d0bf17d756-StreamThread-1-restore-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 2147483647
	max.poll.records = 1000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2017-06-14 11:50:54 INFO  AppInfoParser:83 - Kafka version : 0.10.2.1
2017-06-14 11:50:54 INFO  AppInfoParser:84 - Kafka commitId : e89bffd6b2eff799
2017-06-14 11:50:54 INFO  StreamThread:163 - stream-thread [StreamThread-1] State transition from NOT_RUNNING to RUNNING.
2017-06-14 11:50:54 INFO  KafkaStreams:224 - stream-client [mapapp-example-63a0cb89-28bc-4b5f-bbc6-d7d0bf17d756] State transition from CREATED to RUNNING.
2017-06-14 11:50:54 INFO  KafkaStreams:436 - stream-client [mapapp-example-63a0cb89-28bc-4b5f-bbc6-d7d0bf17d756] Started Kafka Stream process
2017-06-14 11:50:54 INFO  StreamThread:358 - stream-thread [StreamThread-1] Starting
2017-06-14 11:50:55 INFO  AbstractCoordinator:586 - Discovered coordinator tmon-jhkwon78-n.tmoncorp.com:9092 (id: 2147483647 rack: null) for group mapapp-example.
2017-06-14 11:50:55 INFO  ConsumerCoordinator:397 - Revoking previously assigned partitions [] for group mapapp-example
2017-06-14 11:50:55 INFO  StreamThread:248 - stream-thread [StreamThread-1] at state RUNNING: partitions [] revoked at the beginning of consumer rebalance.
2017-06-14 11:50:55 INFO  StreamThread:163 - stream-thread [StreamThread-1] State transition from RUNNING to PARTITIONS_REVOKED.
2017-06-14 11:50:55 INFO  KafkaStreams:224 - stream-client [mapapp-example-63a0cb89-28bc-4b5f-bbc6-d7d0bf17d756] State transition from RUNNING to REBALANCING.
2017-06-14 11:50:55 INFO  StreamThread:1042 - stream-thread [StreamThread-1] Updating suspended tasks to contain active tasks []
2017-06-14 11:50:55 INFO  StreamThread:1049 - stream-thread [StreamThread-1] Removing all active tasks []
2017-06-14 11:50:55 INFO  StreamThread:1064 - stream-thread [StreamThread-1] Removing all standby tasks []
2017-06-14 11:50:55 INFO  AbstractCoordinator:420 - (Re-)joining group mapapp-example
2017-06-14 11:50:55 INFO  StreamPartitionAssignor:300 - stream-thread [StreamThread-1] Constructed client metadata {63a0cb89-28bc-4b5f-bbc6-d7d0bf17d756=ClientMetadata{hostInfo=null, consumers=[mapapp-example-63a0cb89-28bc-4b5f-bbc6-d7d0bf17d756-StreamThread-1-consumer-0b109991-9e39-4153-9228-74cc8de32f61], state=[activeTasks: ([]) assignedTasks: ([]) prevActiveTasks: ([]) prevAssignedTasks: ([]) capacity: 1.0 cost: 0.0]}} from the member subscriptions.
2017-06-14 11:50:55 INFO  StreamPartitionAssignor:640 - stream-thread [StreamThread-1] Completed validating internal topics in partition assignor
2017-06-14 11:50:55 INFO  StreamPartitionAssignor:640 - stream-thread [StreamThread-1] Completed validating internal topics in partition assignor
2017-06-14 11:50:55 INFO  StreamPartitionAssignor:476 - stream-thread [StreamThread-1] Assigned tasks to clients as {63a0cb89-28bc-4b5f-bbc6-d7d0bf17d756=[activeTasks: ([0_0]) assignedTasks: ([0_0]) prevActiveTasks: ([]) prevAssignedTasks: ([]) capacity: 1.0 cost: 0.5]}.
2017-06-14 11:50:55 INFO  AbstractCoordinator:388 - Successfully joined group mapapp-example with generation 1
2017-06-14 11:50:55 INFO  ConsumerCoordinator:256 - Setting newly assigned partitions [MapAppTopic-0] for group mapapp-example
2017-06-14 11:50:55 INFO  StreamThread:228 - stream-thread [StreamThread-1] at state PARTITIONS_REVOKED: new partitions [MapAppTopic-0] assigned at the end of consumer rebalance.
2017-06-14 11:50:55 INFO  StreamThread:163 - stream-thread [StreamThread-1] State transition from PARTITIONS_REVOKED to ASSIGNING_PARTITIONS.
2017-06-14 11:50:55 INFO  KafkaStreams:224 - stream-client [mapapp-example-63a0cb89-28bc-4b5f-bbc6-d7d0bf17d756] State transition from REBALANCING to REBALANCING.
2017-06-14 11:50:55 INFO  StreamThread:858 - stream-thread [StreamThread-1] Creating active task 0_0 with assigned partitions [MapAppTopic-0]
2017-06-14 11:50:55 INFO  StreamTask:140 - task [0_0] Initializing state stores
2017-06-14 11:50:55 INFO  StreamTask:333 - task [0_0] Initializing processor nodes of the topology
2017-06-14 11:50:55 INFO  StreamThread:163 - stream-thread [StreamThread-1] State transition from ASSIGNING_PARTITIONS to RUNNING.
2017-06-14 11:50:55 INFO  KafkaStreams:224 - stream-client [mapapp-example-63a0cb89-28bc-4b5f-bbc6-d7d0bf17d756] State transition from REBALANCING to RUNNING.
2017-06-14 11:51:24 INFO  StreamThread:767 - stream-thread [StreamThread-1] Committing all tasks because the commit interval 30000ms has elapsed
2017-06-14 11:51:24 INFO  StreamThread:805 - stream-thread [StreamThread-1] Committing task StreamTask 0_0
2017-06-14 11:51:25 INFO  MapApp:84 - offset 0 key null value 111
2017-06-14 11:51:30 INFO  MapApp:84 - offset 1 key null value ABC
2017-06-14 11:51:35 INFO  MapApp:84 - offset 2 key null value GREAT
2017-06-14 11:51:40 INFO  MapApp:84 - offset 3 key null value HELLO WORLD
2017-06-14 11:51:54 INFO  StreamThread:767 - stream-thread [StreamThread-1] Committing all tasks because the commit interval 30000ms has elapsed
2017-06-14 11:51:54 INFO  StreamThread:805 - stream-thread [StreamThread-1] Committing task StreamTask 0_0
2017-06-14 11:52:24 INFO  StreamThread:767 - stream-thread [StreamThread-1] Committing all tasks because the commit interval 30000ms has elapsed
2017-06-14 11:52:24 INFO  StreamThread:805 - stream-thread [StreamThread-1] Committing task StreamTask 0_0
2017-06-14 11:52:54 INFO  StreamThread:767 - stream-thread [StreamThread-1] Committing all tasks because the commit interval 30000ms has elapsed
2017-06-14 11:52:54 INFO  StreamThread:805 - stream-thread [StreamThread-1] Committing task StreamTask 0_0
2017-06-14 11:53:24 INFO  StreamThread:767 - stream-thread [StreamThread-1] Committing all tasks because the commit interval 30000ms has elapsed
2017-06-14 11:53:24 INFO  StreamThread:805 - stream-thread [StreamThread-1] Committing task StreamTask 0_0
2017-06-14 11:53:54 INFO  StreamThread:767 - stream-thread [StreamThread-1] Committing all tasks because the commit interval 30000ms has elapsed
2017-06-14 11:53:54 INFO  StreamThread:805 - stream-thread [StreamThread-1] Committing task StreamTask 0_0
2017-06-14 11:54:24 INFO  StreamThread:767 - stream-thread [StreamThread-1] Committing all tasks because the commit interval 30000ms has elapsed
2017-06-14 11:54:24 INFO  StreamThread:805 - stream-thread [StreamThread-1] Committing task StreamTask 0_0
2017-06-14 11:54:54 INFO  StreamThread:767 - stream-thread [StreamThread-1] Committing all tasks because the commit interval 30000ms has elapsed
2017-06-14 11:54:54 INFO  StreamThread:805 - stream-thread [StreamThread-1] Committing task StreamTask 0_0
2017-06-14 11:55:24 INFO  StreamThread:767 - stream-thread [StreamThread-1] Committing all tasks because the commit interval 30000ms has elapsed
2017-06-14 11:55:24 INFO  StreamThread:805 - stream-thread [StreamThread-1] Committing task StreamTask 0_0
2017-06-14 11:55:55 INFO  StreamThread:767 - stream-thread [StreamThread-1] Committing all tasks because the commit interval 30000ms has elapsed
2017-06-14 11:55:55 INFO  StreamThread:805 - stream-thread [StreamThread-1] Committing task StreamTask 0_0
2017-06-14 11:56:25 INFO  StreamThread:767 - stream-thread [StreamThread-1] Committing all tasks because the commit interval 30000ms has elapsed
2017-06-14 11:56:25 INFO  StreamThread:805 - stream-thread [StreamThread-1] Committing task StreamTask 0_0
2017-06-14 11:56:55 INFO  StreamThread:767 - stream-thread [StreamThread-1] Committing all tasks because the commit interval 30000ms has elapsed
2017-06-14 11:56:55 INFO  StreamThread:805 - stream-thread [StreamThread-1] Committing task StreamTask 0_0
2017-06-14 11:57:25 INFO  StreamThread:767 - stream-thread [StreamThread-1] Committing all tasks because the commit interval 30000ms has elapsed
2017-06-14 11:57:25 INFO  StreamThread:805 - stream-thread [StreamThread-1] Committing task StreamTask 0_0
2017-06-14 11:57:55 INFO  StreamThread:767 - stream-thread [StreamThread-1] Committing all tasks because the commit interval 30000ms has elapsed
2017-06-14 11:57:55 INFO  StreamThread:805 - stream-thread [StreamThread-1] Committing task StreamTask 0_0
2017-06-14 11:58:25 INFO  StreamThread:767 - stream-thread [StreamThread-1] Committing all tasks because the commit interval 30000ms has elapsed
2017-06-14 11:58:25 INFO  StreamThread:805 - stream-thread [StreamThread-1] Committing task StreamTask 0_0
2017-06-14 11:58:55 INFO  StreamThread:767 - stream-thread [StreamThread-1] Committing all tasks because the commit interval 30000ms has elapsed
2017-06-14 11:58:55 INFO  StreamThread:805 - stream-thread [StreamThread-1] Committing task StreamTask 0_0
2017-06-14 11:59:25 INFO  StreamThread:767 - stream-thread [StreamThread-1] Committing all tasks because the commit interval 30000ms has elapsed
2017-06-14 11:59:25 INFO  StreamThread:805 - stream-thread [StreamThread-1] Committing task StreamTask 0_0
2017-06-14 11:59:55 INFO  StreamThread:767 - stream-thread [StreamThread-1] Committing all tasks because the commit interval 30000ms has elapsed
2017-06-14 11:59:55 INFO  StreamThread:805 - stream-thread [StreamThread-1] Committing task StreamTask 0_0
2017-06-14 12:00:25 INFO  StreamThread:767 - stream-thread [StreamThread-1] Committing all tasks because the commit interval 30000ms has elapsed
2017-06-14 12:00:25 INFO  StreamThread:805 - stream-thread [StreamThread-1] Committing task StreamTask 0_0
2017-06-14 12:00:55 INFO  StreamThread:767 - stream-thread [StreamThread-1] Committing all tasks because the commit interval 30000ms has elapsed
2017-06-14 12:00:55 INFO  StreamThread:805 - stream-thread [StreamThread-1] Committing task StreamTask 0_0
2017-06-14 12:01:25 INFO  StreamThread:767 - stream-thread [StreamThread-1] Committing all tasks because the commit interval 30000ms has elapsed
2017-06-14 12:01:25 INFO  StreamThread:805 - stream-thread [StreamThread-1] Committing task StreamTask 0_0
2017-06-14 12:01:55 INFO  StreamThread:767 - stream-thread [StreamThread-1] Committing all tasks because the commit interval 30000ms has elapsed
2017-06-14 12:01:55 INFO  StreamThread:805 - stream-thread [StreamThread-1] Committing task StreamTask 0_0
2017-06-14 12:02:25 INFO  StreamThread:767 - stream-thread [StreamThread-1] Committing all tasks because the commit interval 30000ms has elapsed
2017-06-14 12:02:25 INFO  StreamThread:805 - stream-thread [StreamThread-1] Committing task StreamTask 0_0
2017-06-14 12:02:55 INFO  StreamThread:767 - stream-thread [StreamThread-1] Committing all tasks because the commit interval 30000ms has elapsed
2017-06-14 12:02:55 INFO  StreamThread:805 - stream-thread [StreamThread-1] Committing task StreamTask 0_0
2017-06-14 12:03:25 INFO  StreamThread:767 - stream-thread [StreamThread-1] Committing all tasks because the commit interval 30000ms has elapsed
2017-06-14 12:03:25 INFO  StreamThread:805 - stream-thread [StreamThread-1] Committing task StreamTask 0_0
2017-06-14 12:03:55 INFO  StreamThread:767 - stream-thread [StreamThread-1] Committing all tasks because the commit interval 30000ms has elapsed
2017-06-14 12:03:55 INFO  StreamThread:805 - stream-thread [StreamThread-1] Committing task StreamTask 0_0
2017-06-14 12:04:25 INFO  StreamThread:767 - stream-thread [StreamThread-1] Committing all tasks because the commit interval 30000ms has elapsed
2017-06-14 12:04:25 INFO  StreamThread:805 - stream-thread [StreamThread-1] Committing task StreamTask 0_0
2017-06-14 12:04:55 INFO  StreamThread:767 - stream-thread [StreamThread-1] Committing all tasks because the commit interval 30000ms has elapsed
2017-06-14 12:04:55 INFO  StreamThread:805 - stream-thread [StreamThread-1] Committing task StreamTask 0_0
2017-06-14 12:05:25 INFO  StreamThread:767 - stream-thread [StreamThread-1] Committing all tasks because the commit interval 30000ms has elapsed
2017-06-14 12:05:25 INFO  StreamThread:805 - stream-thread [StreamThread-1] Committing task StreamTask 0_0
2017-06-14 12:05:56 INFO  StreamThread:767 - stream-thread [StreamThread-1] Committing all tasks because the commit interval 30000ms has elapsed
2017-06-14 12:05:56 INFO  StreamThread:805 - stream-thread [StreamThread-1] Committing task StreamTask 0_0
2017-06-14 12:06:26 INFO  StreamThread:767 - stream-thread [StreamThread-1] Committing all tasks because the commit interval 30000ms has elapsed
2017-06-14 12:06:26 INFO  StreamThread:805 - stream-thread [StreamThread-1] Committing task StreamTask 0_0
2017-06-14 12:06:56 INFO  StreamThread:767 - stream-thread [StreamThread-1] Committing all tasks because the commit interval 30000ms has elapsed
2017-06-14 12:06:56 INFO  StreamThread:805 - stream-thread [StreamThread-1] Committing task StreamTask 0_0
2017-06-14 12:07:26 INFO  StreamThread:767 - stream-thread [StreamThread-1] Committing all tasks because the commit interval 30000ms has elapsed
2017-06-14 12:07:26 INFO  StreamThread:805 - stream-thread [StreamThread-1] Committing task StreamTask 0_0
2017-06-14 12:07:56 INFO  StreamThread:767 - stream-thread [StreamThread-1] Committing all tasks because the commit interval 30000ms has elapsed
2017-06-14 12:07:56 INFO  StreamThread:805 - stream-thread [StreamThread-1] Committing task StreamTask 0_0
2017-06-14 12:08:09 INFO  MapApp:84 - offset 4 key null value MBC
2017-06-14 12:08:12 INFO  MapApp:84 - offset 5 key null value MBC
2017-06-14 12:08:14 INFO  MapApp:84 - offset 6 key null value TEST
2017-06-14 12:08:26 INFO  StreamThread:767 - stream-thread [StreamThread-1] Committing all tasks because the commit interval 30000ms has elapsed
2017-06-14 12:08:26 INFO  StreamThread:805 - stream-thread [StreamThread-1] Committing task StreamTask 0_0
